{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder_Decoder_Machine_Transliteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNS8Dh1lExXxR/1PU1gyaN6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1792a028fd3f46d6b4ac14ea00fc58ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1773227a36984c7abeb5a04956ce29cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7e596b26d76145aabf8d02c38030f161",
              "IPY_MODEL_765da8347d72407ca6d8711869f158b8"
            ]
          }
        },
        "1773227a36984c7abeb5a04956ce29cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e596b26d76145aabf8d02c38030f161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_52c2d1db9c604bbdb035db14f6dfa61e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4cdc161be931416e972671d084a5f604"
          }
        },
        "765da8347d72407ca6d8711869f158b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74c0317e018d403dbddc1051d693de6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [01:55&lt;00:00, 11.54s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5974a930b9f04783b2b7739da8af98cb"
          }
        },
        "52c2d1db9c604bbdb035db14f6dfa61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4cdc161be931416e972671d084a5f604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74c0317e018d403dbddc1051d693de6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5974a930b9f04783b2b7739da8af98cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgohrani/Recurrent_Neural_Networks/blob/main/Project%3A%20Encoder_Decoder_Machine_Transliteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCFAnbR7fMGL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score , mean_squared_error\n",
        "import matplotlib.colors\n",
        "import math\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import time\n",
        "sns.set()\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "#importing essential libraries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EcRJyAcfeIl"
      },
      "source": [
        "## English and hindi dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDUL6yKlfOXT",
        "outputId": "5839e358-219e-4069-a597-f734fcdf63f7"
      },
      "source": [
        "char_eng = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_eng = '<pad>'\n",
        "eng_dict = {}\n",
        "eng_dict[pad_eng] = 0\n",
        "\n",
        "for i,letter in enumerate(char_eng):\n",
        "  eng_dict[letter] = i+1\n",
        "\n",
        "print(eng_dict)\n",
        "\n",
        "#Creating a dictionary for english and hindi letters which will be used for encoding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSlom9qBfRzB",
        "outputId": "31d166a1-6ded-41a6-d467-b5ffe8a2e67b"
      },
      "source": [
        "hindi_dict = {}\n",
        "hindi_dict['<pad>'] = 0\n",
        "\n",
        "char_hindi = ''\n",
        "for i in range(2304, 2432):\n",
        "  char_hindi += chr(i)\n",
        "\n",
        "for i, letter in enumerate(char_hindi):\n",
        "  hindi_dict[letter] = i+1\n",
        "\n",
        "print(hindi_dict)\n",
        "\n",
        "#Hindi Dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHXLjbSAfhJQ"
      },
      "source": [
        "## Text processing helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euqPztimfUga"
      },
      "source": [
        "def split_hindi_words(hindi_list):\n",
        "  new_hindi_list = []\n",
        "  for hind_name in hindi_list:\n",
        "    hind_name = hind_name.replace(',',' ').replace('_',' ').replace('.',' ').replace(\"'\",' ').replace('-',' ').replace('/',' ').replace('\\u200d',' ').replace('(',' ').replace(')',' ').replace('?',' ')\n",
        "    hind_name = hind_name.split()\n",
        "    new_hindi_list.append(hind_name)\n",
        "  return new_hindi_list\n",
        "\n",
        "def split_english_words(english_list):\n",
        "  new_english_list = []\n",
        "  regex = re.compile('[^a-zA-Z]')\n",
        "  for eng_name in english_list:\n",
        "    eng_name = eng_name.upper()\n",
        "    eng_name = eng_name.replace(\"'\",'').replace('/',' ')\n",
        "    eng_name = regex.sub(' ', eng_name)\n",
        "    eng_name = eng_name.split()\n",
        "    new_english_list.append(eng_name)\n",
        "  return new_english_list\n",
        "\n",
        "def clean_english_list(eng_list):\n",
        "  regex = re.compile('[^a-zA-Z]')\n",
        "  new_english_names = []\n",
        "  for word in eng_list:\n",
        "    new_english_names.append(regex.sub('', word))\n",
        "  return new_english_names\n",
        "\n",
        "def clean_hindi_list(hindi_list):\n",
        "  new_hindi_names = []\n",
        "  for word in hindi_list:\n",
        "    word = word.replace(',','').replace('_','').replace('.','').replace(\"'\",'')\n",
        "    new_hindi_names.append(word)\n",
        "  return new_hindi_names\n",
        "\n",
        "#Some helper functions to help preprocess the text we have \n",
        "#First functions splits the hindi strings into single words and returns a list of the same, same for english\n",
        "#The next two functions removes unnecessary characters from english and hindi words\n",
        "#These functions will be used later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAYb6C9XfnAS"
      },
      "source": [
        "## Encoding functions, only indices returned not one-hot-encoded vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgKHiOoTfW2g"
      },
      "source": [
        "def convert_eng_to_encoded(X_train):\n",
        "  list_to_return = []\n",
        "  for word in X_train:\n",
        "    onehotstart = torch.zeros([len(word)+1,1])\n",
        "    onehotstart[len(word)][0] = 1\n",
        "    for i,letter in enumerate(word):\n",
        "      index = eng_dict[letter]\n",
        "      onehotstart[i][0] = index\n",
        "    list_to_return.append(onehotstart)\n",
        "  \n",
        "  return list_to_return \n",
        "\n",
        "def convert_hindi_to_encoded(Y_train):\n",
        "  list_to_return = []\n",
        "  for word in Y_train:\n",
        "    onehotstart = torch.zeros([len(word)+1,1])\n",
        "    onehotstart[len(word)][0] = 0\n",
        "    for i,letter in enumerate(word):\n",
        "      index = hindi_dict[letter]\n",
        "      onehotstart[i][0] = index\n",
        "    list_to_return.append(onehotstart)\n",
        "  \n",
        "  return list_to_return\n",
        "\n",
        "#Functions for encoding but not one hot encoding, the index value is directly used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cZ2Pbr-fsK7"
      },
      "source": [
        "## Class to process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKI9G-Z8fYQw"
      },
      "source": [
        "class EncoderDecoderData():\n",
        "\n",
        "  def __init__(self,filename):\n",
        "    self.final_eng_list, self.final_hindi_list = self.create_data_from_XML(filename)\n",
        "\n",
        " \n",
        "  def create_data_from_XML(self,filename):\n",
        "    tree = ET.parse(filename)\n",
        "    root = tree.getroot()\n",
        "    english_names = []\n",
        "    hindi_names = []\n",
        "    for elem in root.iter():\n",
        "      if elem.tag == 'SourceName':\n",
        "        english_names.append(elem.text)\n",
        "      if elem.tag == 'TargetName' and elem.attrib['ID'] == '1':\n",
        "        hindi_names.append(elem.text)\n",
        "    \n",
        "    new_english_names = split_english_words(english_names)\n",
        "    new_hindi_names = split_hindi_words(hindi_names)\n",
        "\n",
        "    final_hindi_data = []\n",
        "    final_english_data = []\n",
        "    \n",
        "    for eng_word, hindi_word in zip(new_english_names, new_hindi_names):\n",
        "      if len(eng_word) != len(hindi_word):\n",
        "        print('Skipping:', eng_word, '-', hindi_word)\n",
        "      else:\n",
        "        for eng_word_part, hindi_word_part in zip(eng_word, hindi_word):\n",
        "          final_hindi_data.append(hindi_word_part)\n",
        "          final_english_data.append(eng_word_part)\n",
        "    \n",
        "    final_hindi_data = clean_hindi_list(final_hindi_data)\n",
        "    final_english_data = clean_english_list(final_english_data)\n",
        "    self.final_eng_list = final_english_data\n",
        "    self.final_hindi_list = final_hindi_data\n",
        "    return self.final_eng_list, self.final_hindi_list\n",
        "\n",
        "\n",
        "  def generate_random_sample(self):\n",
        "    index = np.random.randint(len(self.final_eng_list))\n",
        "    return self.final_eng_list[index], self.final_hindi_list[index]\n",
        "  \n",
        "  def generate_random_batch(self,batch_size):\n",
        "    index = np.random.randint(len(self.final_eng_list))\n",
        "    batch_list_english = []\n",
        "    batch_list_hindi = []\n",
        "    for i in range(index,index+batch_size,1):\n",
        "      if i >= len(self.final_eng_list):\n",
        "        batch_list_english.append(self.final_eng_list[i-len(self.final_eng_list)])\n",
        "        batch_list_hindi.append(self.final_hindi_list[i-len(self.final_eng_list)])\n",
        "      else:\n",
        "        batch_list_english.append(self.final_eng_list[i])\n",
        "        batch_list_hindi.append(self.final_hindi_list[i])\n",
        "    return batch_list_english, batch_list_hindi\n",
        "\n",
        "\n",
        "#This is a class we will call on the xml data we have for english and hindi\n",
        "#In the init function, a create_data_from_xml is called which is defined below\n",
        "#In create_data_from XML, the file is parsed and its root is iterated over to get the hindi strings and their corresponding english strings\n",
        "#ID = 1 is used since one string might have multiple others\n",
        "#We now have equivalent strings in the form of a nested list. Our next step is to remove strings which have unequal number of words, so we compare and skip\n",
        "#Finally individual words are added into a new list, and the cleaning functions are called as usual\n",
        "#Two functions to create data for us are used, one which returns a sample of a hindi and english word and the other which returns a consecutive batch of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EQyLn4_jOD5"
      },
      "source": [
        "# Testing our processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhMh8dhqjP0S",
        "outputId": "867a734f-cf3b-4e1a-be5c-6e05b125a255"
      },
      "source": [
        "data_store = EncoderDecoderData('training.xml')\n",
        "\n",
        "#Calling this preprocessing class on our data, the words which are not present are skipped"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping: ['MAHARANI', 'PADMINI'] - ['महारानी', 'पद्', 'मिनी']\n",
            "Skipping: ['STATE', 'MUSEUM', 'OF', 'THE', 'VERMONT', 'HISTORICAL', 'SOCIETY'] - ['स्टेट', 'म्युज़ियम', 'ऑफ', 'द', 'वरमाउंट', 'हिस्टॉरिकल', 'सोसायट', 'ी']\n",
            "Skipping: ['I', 'DUKAANT'] - ['इंदुकांत']\n",
            "Skipping: ['EFFIE', 'AWARDS'] - ['एफी', 'अवार्ड्', 'स']\n",
            "Skipping: ['LAURENCE', 'OLIVIER', 'AWARDS'] - ['लॉरेंस', 'ओलिवर', 'अवार्ड्', 'स']\n",
            "Skipping: ['ETTA'] - ['एट्', 'टा']\n",
            "Skipping: ['COLLEGE', 'FOOTBALL', 'AWARDS'] - ['कॉलेज', 'फुटबॉल', 'अवार्ड्', 'स']\n",
            "Skipping: ['STEVE', 'RHODES'] - ['स्टीव', 'रोड्', 'स']\n",
            "Skipping: ['WINDHAM', 'COUNTY', 'HISTORICAL', 'MUSEUM'] - ['व', 'िंडहैम', 'काउंट', 'ी', 'ह', 'िस्टॉर', 'िकल', 'म्युज़ियम']\n",
            "Skipping: ['PLAZA'] - ['प्लाज़ा', '66']\n",
            "Skipping: ['ADVAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['BHAALACHAN', 'DR'] - ['भालचन्द्र']\n",
            "Skipping: ['BARHARWA', 'JUNCTION'] - ['बरहरवा']\n",
            "Skipping: ['STATE', 'BNK', 'TR'] - ['स्टेट', 'बैंक', 'ऑफ', 'त्रावणकोर']\n",
            "Skipping: ['IN', 'DRAJEET'] - ['इन्द्रजीत']\n",
            "Skipping: ['WOODSTOCK', 'HISTORICAL', 'SOCIETY'] - ['वुडस्टॉक', 'ह', 'िस्टॉर', 'िकल', 'सोसायटी']\n",
            "Skipping: ['BROOKLYN', 'MUSEUM'] - ['ब्रुकल', 'िन', 'म्युज़ियम']\n",
            "Skipping: ['SAINT', 'FRANCIS', 'DASSISI', 'HIGH', 'SCHOOL'] - ['सेंट', 'फ्रांसिस', 'ड', 'िअस', 'ीसी', 'हाई', 'स्कूल']\n",
            "Skipping: ['PADMA', 'VIBHUSHAN'] - ['पद्', 'म', 'विभूषण']\n",
            "Skipping: ['GROUPE', 'CAISSE', 'D', 'PARGME'] - ['ग्रुप', 'कैस', 'डेपॉर्मे']\n",
            "Skipping: ['BRITISH', 'BOOK', 'AWARDS'] - ['ब्रिटिश', 'बुक', 'अवार्ड्', 'स']\n",
            "Skipping: ['SOUTH', 'ARLINGTON', 'CHURCH', 'OF', 'CHRIST'] - ['साउथ', 'अर्लिंग्टन']\n",
            "Skipping: ['JACK', 'RICHARDS'] - ['जैक', 'रिचर्ड्', 'स']\n",
            "Skipping: ['I', 'DIYAA'] - ['इंडिया']\n",
            "Skipping: ['DAINIK', 'JUGASANKHA'] - ['दैन', 'िक', 'जुगसंखा']\n",
            "Skipping: ['EISNER', 'AWARDS'] - ['ईस्नर', 'अवार्ड्', 'स']\n",
            "Skipping: ['THE', 'PIONEER', 'DAILY'] - ['द', 'पायोन', 'ियर', 'डेली']\n",
            "Skipping: ['SIYASAT'] - ['स', 'ियासत']\n",
            "Skipping: ['SCOTTISH', 'CHURCH', 'COLLEGE', 'CALCUTTA'] - ['स्कॉट', 'िश', 'चर्च', 'कॉलेज', 'कैलकटा']\n",
            "Skipping: ['ISMAIL'] - ['इस्मा', 'ईल']\n",
            "Skipping: ['DESHONNATI'] - ['देशोन्नत', 'ि']\n",
            "Skipping: ['PEARSE', 'MUSEUM'] - ['प', 'ियर्स', 'म्युज़ियम']\n",
            "Skipping: ['TAMBURITZA'] - ['तम्बुरिट्', 'ज़ा']\n",
            "Skipping: ['ANAIKUTTAM', 'RESERVOIR'] - ['अनइकुट्', 'टम', 'रिज़रवायर']\n",
            "Skipping: ['CENTRAL', 'CHRONICLE'] - ['सेंट्रल', 'क्रॉन', 'िकल']\n",
            "Skipping: ['SOCI', 'T', 'G', 'N', 'RALE'] - ['सोसायटी', 'जनरल']\n",
            "Skipping: ['KING', 'EDWARD', 'VII'] - ['किंग', 'एडवर्ड']\n",
            "Skipping: ['ORDER', 'OF', 'SPORTS', 'MERIT'] - ['ऑडर', 'ऑफ', 'स्पोर्ट्', 'स', 'मेरिट']\n",
            "Skipping: ['SRI', 'RAMAKRISHNA', 'VIDYASHALA', 'MYSORE'] - ['श्री', 'रामकृष्ण', 'व', 'िद्याशाला', 'मैसूर']\n",
            "Skipping: ['BRAJEN', 'DR'] - ['ब्रजेन्द्र']\n",
            "Skipping: ['DIBANG', 'VALLEY'] - ['दिबंगवैली']\n",
            "Skipping: ['THATTHA'] - ['ठट्', 'ठा']\n",
            "Skipping: ['LULU'] - ['लु', 'लु']\n",
            "Skipping: ['WILHEM', 'BUSCH', 'MUSEUM'] - ['व', 'िल्हेम', 'बुश', 'म्युज़ियम']\n",
            "Skipping: ['CHITTAR', 'RESERVOIR'] - ['चित्तर', 'रिज़रवायर', '2']\n",
            "Skipping: ['FIELDS', 'MEDAL'] - ['फील्ड्', 'स', 'मेडल']\n",
            "Skipping: ['SQUADS'] - ['सक्वैड्', 'स']\n",
            "Skipping: ['CARMEL', 'CONVENT', 'SCHOOL', 'CHANAKYA', 'PURI'] - ['कारमेल', 'कॉन्', 'वेंट', 'स्कूल', 'चाणक्य', 'पुरी']\n",
            "Skipping: ['VITTHAL'] - ['विट्', 'ठल']\n",
            "Skipping: ['LOGIE', 'AWARD'] - ['लॉग', 'अवार्ड्', 'स']\n",
            "Skipping: ['LOLA', 'AWARDS'] - ['लोला', 'अवार्ड्', 'स']\n",
            "Skipping: ['NOTTINGHAM', 'EAST', 'MIDLANDS'] - ['नॉटिंगहॅम', 'ईस्ट', 'मिडलैंड्', 'स']\n",
            "Skipping: ['MAHINDRA', 'UNITED', 'WORLD', 'COLLEGE', 'OF', 'INDIA'] - ['मह', 'िंद्रा', 'यूनाइटेड', 'वर्ल्ड', 'कॉलेज', 'ऑफ', 'इंडिया']\n",
            "Skipping: ['ADWAAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['ADWAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['THE', 'ECONOMIC', 'TIMES'] - ['द', 'इकनॉम', 'िक', 'टाइम्स']\n",
            "Skipping: ['I', 'JEENIYAR'] - ['इंजीनियर']\n",
            "Skipping: ['JITEN', 'DR'] - ['जितेन्द्र']\n",
            "Skipping: ['PRECISION', 'VALLEY', 'CORVETTE', 'MUSEUM'] - ['प्रिस', 'िज़न', 'वैली', 'कॉरवेट', 'म्युज़ियम']\n",
            "Skipping: ['SCHOCK', 'PRIZE', 'IN', 'VISUAL', 'ARTS'] - ['श्कॉक', 'प्राइज़', 'इन', 'विज़्युल', 'आर्ट्', 'स']\n",
            "Skipping: ['NATUN', 'DIN'] - ['नतुन', 'द', 'िन']\n",
            "Skipping: ['LYCHNOSTATIS', 'OPEN', 'AIR', 'MUSEUM'] - ['ल', 'िक्नोस्टेट', 'िस', 'ओपन', 'एयर', 'म्युज़ियम']\n",
            "Skipping: ['ADVAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['THE', 'TYNE', 'TURRETS'] - ['द', 'टायने', 'टरेट्', 'स']\n",
            "Skipping: ['VERIA', 'FOLKLORE', 'MUSEUM'] - ['वेर', 'िया', 'फोकलोर', 'म्युज़ियम']\n",
            "Skipping: ['DUBLIN', 'WRITERS', 'MUSEUM'] - ['डब्ल', 'िन', 'राइटर्स', 'म्युज़ियम']\n",
            "Skipping: ['NAIDUNIYA'] - ['नईदुन', 'िया']\n",
            "Skipping: ['FIDEL', 'EDWARDS'] - ['फिडल', 'एडवर्ड्', 'स']\n",
            "Skipping: ['ROSTER'] - ['रोस्', 'टर']\n",
            "Skipping: ['ORDER', 'OF', 'VASA'] - ['ऑडर', 'ऑफ़', 'द', 'वासा']\n",
            "Skipping: ['RUBIN', 'MUSEUM', 'OF', 'ART'] - ['रुब', 'िन', 'म्युज़ियम', 'ऑफ', 'आर्ट']\n",
            "Skipping: ['DINAKARAN'] - ['द', 'िनाकरण']\n",
            "Skipping: ['HARVEY', 'AWARDS'] - ['हार्वे', 'अवार्ड्', 'स']\n",
            "Skipping: ['ARGENTINA', 'MUSEUM', 'OF', 'NATURAL', 'SCIENCES'] - ['अर्जेंट', 'ीना', 'म्युज़ियम', 'ऑफ', 'नैचरल', 'साइंसेस']\n",
            "Skipping: ['BIG', 'BROTHER', 'AWARDS'] - ['बिग', 'ब्रदर', 'अवार्ड्', 'स']\n",
            "Skipping: ['ANDROS', 'MARITIME', 'MUSEUM'] - ['एण्ड्रॉस', 'मैर', 'िटाइम', 'म्युज़ियम']\n",
            "Skipping: ['CALCIUM'] - ['कै', 'ल्शियम']\n",
            "Skipping: ['TAKHAT', 'SHRI', 'SACHKHAND', 'SAHIB', 'NANDED', 'MAHARASHTRA'] - ['तखत', 'श्री', 'सचखंड', 'साहिब', 'नांदेड़', 'महाराष्', 'ट्र']\n",
            "Skipping: ['AZAMNAGAR', 'ROAD'] - ['आज़मनगर']\n",
            "Skipping: ['COMMIE', 'AWARDS'] - ['कॉमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['VIDUTHALAI'] - ['व', 'िदुथलई']\n",
            "Skipping: ['AKTIESELSKABET', 'DAMPSKIBSSELSKABET', 'TORM'] - ['एक्टीसल्:कॅबेट', 'डॅम्प्सकीबेसेल्सकॅबेट', 'टॉर्म', '4']\n",
            "Skipping: ['HISTORICAL', 'MUSEUM', 'OF', 'CRETE'] - ['ह', 'िस्टॉर', 'िकल', 'म्युज़ियम', 'ऑफ', 'क्रेट']\n",
            "Skipping: ['FRANCE', 'T', 'L', 'COM'] - ['फ़्रांस', 'टेलीकॉम']\n",
            "Skipping: ['MEDZHYBIZH'] - ['मेझ्ही', 'बिज़']\n",
            "Skipping: ['BRIT', 'AWARDS'] - ['ब्रिट', 'अवार्ड्', 'स']\n",
            "Skipping: ['ASOMIYA', 'PRATIDIN'] - ['असोम', 'िया', 'प्रतिद', 'िन']\n",
            "Skipping: ['RUTGERS'] - ['रट्', 'जर्स']\n",
            "Skipping: ['DAINIK', 'JANAMBHUMI'] - ['दैन', 'िक', 'जन्मभूमि']\n",
            "Skipping: ['CAPE', 'TOWN'] - ['केपटाउन']\n",
            "Skipping: ['PETRONAS', 'TOWER'] - ['प्रेट्रोनॉस', 'टॉवर', '2']\n",
            "Skipping: ['CHATTAN', 'SINGH'] - ['चट्', 'टान', 'सिंह']\n",
            "Skipping: ['AUSTRALIAN', 'WAR', 'MEMORIAL'] - ['ऑस्ट्रेल', 'ियन', 'वार', 'मेमोर', 'ियल']\n",
            "Skipping: ['EADS'] - ['इएड्', 'स']\n",
            "Skipping: ['NEW', 'ZEALAND'] - ['न्यूज़ीलैंड']\n",
            "Skipping: ['MUSEUM', 'AND', 'STUDY', 'CENTRE', 'OF', 'THE', 'GREEK', 'THEATRE'] - ['म्युज़ियम', 'एण्ड', 'स्टडी', 'सेंटर', 'ऑफ', 'द', 'ग्रीक', 'थ', 'िएटर']\n",
            "Skipping: ['HISTORICAL', 'AND', 'FOLKLORE', 'MUSEUM', 'OF', 'CORINTH'] - ['ह', 'िस्टॉर', 'िकल', 'एण्ड', 'फोकलोर', 'म्युज़ियम', 'ऑफ', 'कॉर', 'िन्थ']\n",
            "Skipping: ['RAMCOIND'] - ['राम्को', 'इंड']\n",
            "Skipping: ['NAWAB', 'SIRAZUDDAULA'] - ['नवाब', 'सिराज़ुद्', 'दौला']\n",
            "Skipping: ['PAUL', 'VRELLIS', 'GREEK', 'HISTORY', 'MUSEUM'] - ['पॉल', 'व्रेलिस', 'ग्रीक', 'ह', 'िस्ट्री', 'म्युज़ियम']\n",
            "Skipping: ['MANEE', 'DR'] - ['मणींद्र']\n",
            "Skipping: ['BILLIARDS'] - ['बिलियर्ड्', 'स']\n",
            "Skipping: ['PETROBR', 'S'] - ['पेट्रोब्रस']\n",
            "Skipping: ['DHARITRI'] - ['धर', 'ित्री']\n",
            "Skipping: ['VISHVEN', 'DR'] - ['विश्वेन्द्र']\n",
            "Skipping: ['NANJING', 'MUSEUM'] - ['नंज', 'िंग', 'म्युज़ियम']\n",
            "Skipping: ['MUSEUM', 'OF', 'DIONYSIOS', 'SOLOMOS', 'AND', 'EMINENT', 'ZAKYNTHIANS'] - ['म्युज़ियम', 'ऑफ', 'डीयॉन', 'िस', 'ियॉस', 'सोलोमॉस', 'एण्ड', 'एम', 'िनेंट', 'झॅक', 'िन्थ', 'ियन्स']\n",
            "Skipping: ['NAHANNI'] - ['ना', 'हान्नी']\n",
            "Skipping: ['AUSTRALIAN', 'NATIONAL', 'UNIVERSITY'] - ['ऑस्ट्रेलियननेशनल', 'यूनिवर्सिटी']\n",
            "Skipping: ['GOKHALE', 'MEMORIAL', 'GIRLS', 'SCHOOL', 'CALCUTTA'] - ['गोखले', 'मेमोर', 'ियल', 'गर्ल्स', 'स्कूल', 'कैलकटा']\n",
            "Skipping: ['JAHAN', 'AARA'] - ['जहाँआरा']\n",
            "Skipping: ['NAVABHARAT', 'FERRO', 'ALLOYS'] - ['नव', 'भारत', 'फ़ैरो', 'अलॉय']\n",
            "Skipping: ['UNIVERSITY', 'OF', 'LEEDS'] - ['यूनिवर्सिटी', 'ऑफ', 'लीड्', 'स']\n",
            "Skipping: ['ADBUL', 'QAWI'] - ['अद्', 'बुल', 'कावी']\n",
            "Skipping: ['GONCI', 'RE', 'EURIS'] - ['गॉन्सियर', 'यूरिस']\n",
            "Skipping: ['JEEVAN', 'MRITYU'] - ['जीवन', 'मृ', 'त्यु']\n",
            "Skipping: ['DIVYA', 'BHASKAR'] - ['द', 'िव्य', 'भास्कर']\n",
            "Skipping: ['RAMA', 'LINGESHWARA'] - ['रामालिंगेश्वर']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'RHODES'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'रोड्स']\n",
            "Skipping: ['FAKHRUN', 'NISA'] - ['फखरुन्निसा']\n",
            "Skipping: ['DEVEN', 'DR'] - ['देवेन्द्र']\n",
            "Skipping: ['FORT', 'LAFAYETTE'] - ['फोर्ट', 'लैफायेट्', 'ट']\n",
            "Skipping: ['MAJOR', 'LEAGUE', 'LACROSSE', 'SPORTSMAN', 'OF', 'THE', 'YEAR', 'AWARD'] - ['मेजर', 'लीग', 'लैक्रोस', 'स्पोर्ट्', 'समैन', 'ऑफ', 'द', 'ईयर', 'अवार्ड']\n",
            "Skipping: ['DAINIK', 'BHASKAR'] - ['दैन', 'िक', 'भास्कर']\n",
            "Skipping: ['FOUR', 'CONTINENTS', 'FIGURE', 'SKATING', 'CHAMPIONS'] - ['फोर', 'कॉनटिनेंट्', 'स', 'फिगर', 'स्केटिंग', 'चैंपियन्स']\n",
            "Skipping: ['GALLANTS', 'BOWER'] - ['गैलंट्', 'स', 'बॉवर']\n",
            "Skipping: ['GUINESS', 'WORLD', 'OF', 'RECORD', 'MUSEUM'] - ['ग', 'िनीज', 'वर्ल्ड', 'ऑफ', 'रेकॉर्ड्स', 'म्युज़ियम']\n",
            "Skipping: ['REDIFF', 'COM', 'INDIA', 'LIMITED'] - ['रेडिफ़', 'डॉट', 'कॉम', 'इंडिया', 'लिमिटेड']\n",
            "Skipping: ['ORDER', 'OF', 'THE', 'NATIONAL', 'COAT', 'OF', 'ARMS'] - ['ऑडर', 'ऑफ', 'द', 'नेशनल', 'कोट', 'ऑफ', 'आर्म्', 'स']\n",
            "Skipping: ['NATIONAL', 'MUSEUM', 'OF', 'AMERICAN', 'INDIAN'] - ['नेशनल', 'म्युज़ियम', 'ऑफ', 'अमेर', 'िकन', 'इंड', 'ियन']\n",
            "Skipping: ['THE', 'HUMBER', 'FORTS'] - ['द', 'हंबर', 'फोर्ट्', 'स']\n",
            "Skipping: ['OMKARNATH', 'THAKUR'] - ['ओंकार', 'नाथ', 'ठाकुर']\n",
            "Skipping: ['NAOMI', 'AWARDS'] - ['नाओमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['AMERICAN', 'NUMISMATIC', 'SOCIETY', 'MUSEUM'] - ['अमेर', 'िकन', 'न्यूम', 'िस्मेट', 'िक', 'सोसायटी', 'म्युज़ियम']\n",
            "Skipping: ['RASTRIYA', 'SAHARA'] - ['राष्', 'ट्रीय', 'सहारा']\n",
            "Skipping: ['GARY', 'ROBERTSON'] - ['गेरी', 'रॉबर्ट्', 'सन']\n",
            "Skipping: ['ANDREW', 'SYMONDS'] - ['एण्ड्रयू', 'सिमंड्', 'स']\n",
            "Skipping: ['NEW', 'YORK', 'HISTORICAL', 'SOCIETY'] - ['न्यू', 'यॉर्क', 'ह', 'िस्टॉर', 'िकल', 'सोसायटी']\n",
            "Skipping: ['IDJWI'] - ['इड्', 'ज्वी']\n",
            "Skipping: ['OPENTV'] - ['ओपन', 'टीवी']\n",
            "Skipping: ['BRITISH', 'COMEDY', 'AWARDS'] - ['ब्रिटिश', 'कॉमेडी', 'अवार्ड्', 'स']\n",
            "Skipping: ['SHAILEN', 'DR'] - ['शैलेन्द्र']\n",
            "Skipping: ['FORT', 'RONDUIT'] - ['फोर्ट', 'रॉनड्', 'यूट']\n",
            "Skipping: ['GERMAN', 'HISTORICAL', 'MUSEUM'] - ['जर्मन', 'ह', 'िस्टॉर', 'िकल', 'म्युज़ियम']\n",
            "Skipping: ['STUDIO', 'MUSEUM', 'IN', 'HARLEN'] - ['स्टुड', 'ियो', 'म्युज़ियम', 'इन', 'हार्लेन']\n",
            "Skipping: ['UN', 'MANAA'] - ['उन्मना']\n",
            "Skipping: ['ENVOY', 'COMMUNICATIONS', 'GROUP'] - ['एन्वॉय', 'कम्युनिकेशंस']\n",
            "Skipping: ['ARISTOTLE', 'ONASSIS'] - ['एरीस्टोटल', 'ओनासि', 'स']\n",
            "Skipping: ['DAINIK', 'AGRADOOT'] - ['दैन', 'िक', 'अग्रदूत']\n",
            "Skipping: ['WAR', 'OF', 'THE', 'HOLY', 'LEAGUE'] - ['वार', 'ऑफ', 'होली', 'लीग']\n",
            "Skipping: ['PHIL', 'EDMONDS'] - ['फिल', 'एडमंड्', 'स']\n",
            "Skipping: ['UNITED', 'STATES', 'MILITARY', 'ACADEMY'] - ['यूनाइटेड', 'स्टेट्', 'स', 'मिलिट्री', 'अकेडमी']\n",
            "Skipping: ['HINDI', 'MILAP'] - ['ह', 'िन्दी', 'म', 'िलाप']\n",
            "Skipping: ['THE', 'GOULANDRIS', 'MUSEUM', 'OF', 'NATURAL', 'HISTORY'] - ['द', 'गॉलैंड्रीस', 'म्युज़ियम', 'ऑफ', 'नैचरल', 'ह', 'िस्टरी']\n",
            "Skipping: ['PRICKETTS', 'FORT'] - ['प्रिकेट्', 'स', 'फोर्ट']\n",
            "Skipping: ['ROBERT', 'BRIDGES'] - ['रॉबर्ट', 'ब्रिजि', 'ज']\n",
            "Skipping: ['MUSEUM', 'OF', 'ENGRAVINGS', 'AND', 'GRAPHIC', 'ARTS'] - ['म्युज़ियम', 'ऑफ', 'एन्ग्रेव', 'िंग्स', 'एण्ड', 'ग्राफ', 'िक', 'आर्ट्स']\n",
            "Skipping: ['WHITNEY', 'MUSEUM', 'OF', 'AMERICAN', 'ART'] - ['व्ह', 'िटनी', 'म्युज़ियम', 'ऑफ', 'अमेर', 'िकन', 'आर्ट']\n",
            "Skipping: ['SAPTAHIK', 'HINDUSTAN'] - ['साप्ताह', 'िक', 'ह', 'िन्दुस्तान']\n",
            "Skipping: ['VIJAYA', 'KARNATAKA'] - ['व', 'िजय', 'कर्नाटका']\n",
            "Skipping: ['CORNISH', 'COLONY', 'MUSEUM'] - ['कॉर्न', 'िश', 'कॉलोनी', 'म्युज़ियम']\n",
            "Skipping: ['MUSEUM', 'OF', 'ISLAMIC', 'CERAMICS'] - ['म्युज़ियम', 'ऑफ', 'इस्लाम', 'िक', 'स', 'िरेम', 'िक्स']\n",
            "Skipping: ['SUREN', 'DAR'] - ['सुरेन्दर']\n",
            "Skipping: ['SAINT', 'JOHNS', 'HIGH', 'SCHOOL', 'CHANDIGARH'] - ['सेंट', 'जॉन्', 'स', 'हाई', 'स्कूल', 'चंडीगढ़']\n",
            "Skipping: ['DENIRO'] - ['डी', 'निरो']\n",
            "Skipping: ['LOKAMAAN', 'Y'] - ['लोकमान्य']\n",
            "Skipping: ['VAPARAISO', 'CHURCH', 'OF', 'CHRIST'] - ['व्हापरासिओ']\n",
            "Skipping: ['AJI', 'ASSAMESE', 'DAILY'] - ['अज', 'ि', 'असमीज', 'डेल्ही']\n",
            "Skipping: ['PHILATELIC', 'MUSEUM'] - ['फ', 'िलेटेलिक', 'म्युज़ियम']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'PIRAEUS'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'पायरस']\n",
            "Skipping: ['SAUJAN', 'Y'] - ['सौजन्य']\n",
            "Skipping: ['PARIS', 'CHARLES', 'DE', 'GAULLE'] - ['पेरिस', 'रॉसे', 'चार्ल्स', 'डे', 'ग्यूले']\n",
            "Skipping: ['PARKWAY', 'APOSTOLIC'] - ['पार्क', 'वे', 'अपोस्टोलिक']\n",
            "Skipping: ['PRATIDIN'] - ['प्रत', 'िद', 'िन']\n",
            "Skipping: ['RADIO', 'ACADEMY', 'AWARDS'] - ['रेडियो', 'अकेडमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['KAPEE', 'DR'] - ['कपींद्र']\n",
            "Skipping: ['NETHERLANDS'] - ['नीदरलैंड्', 'स']\n",
            "Skipping: ['CHAITANYA', 'ENGLISH', 'MEDIUM', 'HIGH', 'SCHOOL', 'TIRUPATHI'] - ['चैतन्य', 'इंग्ल', 'िश', 'म', 'ीड', 'ियम', 'हाई', 'स्कलू', 'त', 'िरुपती']\n",
            "Skipping: ['SORRENTO'] - ['सॉरेन्टो', '1']\n",
            "Skipping: ['PRIX', 'DES', 'DEUX', 'MAGOTS'] - ['प्रिक्स', 'डेस', 'ड्', 'यूक्स', 'मैगट्', 'स']\n",
            "Skipping: ['MAUNA', 'LOA'] - ['मौनालोआ']\n",
            "Skipping: ['HANSRAJ', 'MORARJI', 'PUBLIC', 'SCHOOL'] - ['हंसराज', 'मोरारजी', 'पब्ल', 'िक', 'स्कूल']\n",
            "Skipping: ['BUDNASEEB'] - ['बद', 'नसीब']\n",
            "Skipping: ['SQUIDDY', 'AWARDS'] - ['स्क्विडी', 'अवार्ड्', 'स']\n",
            "Skipping: ['MASS', 'MUTUAL', 'LIFE'] - ['मास', 'म्युच्युअल', 'लाइफ़', 'इंश्योरेंस']\n",
            "Skipping: ['STATS', 'CHIPPAC'] - ['स्टेट्सचिपपैक']\n",
            "Skipping: ['KAREN', 'BLIXEN', 'MUSEUM'] - ['कैरेन', 'ब्ल', 'िक्सन', 'म्युज़ियम']\n",
            "Skipping: ['YESHIVA', 'UNIVERSITY', 'MUSEUM'] - ['येश', 'िवा', 'युन', 'िवर्स', 'िटी', 'म्युज़ियम']\n",
            "Skipping: ['KAVIGNAR', 'INKULAB'] - ['कव', 'िग्नर', 'इंकलाब']\n",
            "Skipping: ['DRAFTS'] - ['ड्राफ्ट्', 'स']\n",
            "Skipping: ['SATISH', 'GUJRAL'] - ['स', 'तीश', 'गुजराल']\n",
            "Skipping: ['BHATTU'] - ['भट्', 'टु']\n",
            "Skipping: ['DAVIS', 'MUSEUM', 'AND', 'CULTURAL', 'ART'] - ['डेव', 'िस', 'म्युज़ियम', 'एण्ड', 'कल्चरल', 'आर्ट']\n",
            "Skipping: ['AMERICAN', 'ACADEMY', 'OF', 'ARTS', 'AND', 'LETTERS', 'FICTION', 'AWARD'] - ['अमेरिकन', 'अकेडमी', 'ऑफ', 'आर्ट्', 'स', 'एण्ड', 'लेटर्स', 'फिक्शन', 'अवार्ड']\n",
            "Skipping: ['NEWFOUNDLAND'] - ['न्यू', 'फाउंडलैंड']\n",
            "Skipping: ['FLORINA', 'MUSEUM', 'OF', 'MODERN', 'ART'] - ['फ्लोर', 'िना', 'म्युज़ियम', 'ऑफ', 'मॉडर्न', 'आर्ट']\n",
            "Skipping: ['LONDONHEATHROW'] - ['लंदन', 'हीथ्रो']\n",
            "Skipping: ['RYAN', 'HINDS'] - ['रियान', 'हिन्ड्', 'स']\n",
            "Skipping: ['MUSEUM', 'OF', 'THE', 'CITY', 'OF', 'NEW', 'YORK'] - ['म्युज़ियम', 'ऑफ', 'द', 'स', 'िट', 'ी', 'ऑफ', 'न्यू', 'यॉर्क']\n",
            "Skipping: ['GAJEN', 'DR'] - ['गजेन्द्र']\n",
            "Skipping: ['NAREN', 'DAR'] - ['नरेन्दर']\n",
            "Skipping: ['KOSTAS', 'FRONTZOS', 'MUSEUM', 'OF', 'EPIRUS', 'FOLK', 'ART'] - ['कोस्टस', 'फ्रंटज़ॉस', 'म्युज़ियम', 'ऑफ', 'एप', 'िरस', 'फोक', 'आर्ट']\n",
            "Skipping: ['THE', 'INTERNATIONAL', 'ANDY', 'AWARDS'] - ['द', 'इंटरनेशनल', 'एण्डी', 'अवार्ड्', 'स']\n",
            "Skipping: ['DELHI', 'PUBLIC', 'SCHOOL', 'INDIRAPURAM'] - ['डेल्ही', 'पब्ल', 'िक', 'स्कूल', 'इंदिरापुरम']\n",
            "Skipping: ['HUKUMACHAN', 'D'] - ['हुकुमचन्द']\n",
            "Skipping: ['ADWAAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['DAYTON', 'ART', 'INSTITUTE'] - ['डेटन', 'आर्ट', 'इंस्टीट्', 'यूट']\n",
            "Skipping: ['NATIONAL', 'TELEVISION', 'AWARDS'] - ['नेशनल', 'टेलीविज़न', 'अवार्ड्', 'स']\n",
            "Skipping: ['VANAKAN', 'YAA'] - ['वनकन्या']\n",
            "Skipping: ['COBH', 'HERITAGE', 'CENTER'] - ['कोब', 'हेर', 'िटेज', 'सेंटर']\n",
            "Skipping: ['RETALIX'] - ['रेटालिक्स', 'लि']\n",
            "Skipping: ['CARMEL', 'CONVENT', 'SCHOOL', 'CHANDIGARH'] - ['कारमेल', 'कॉन्', 'वेंट', 'स्कूल', 'चंडीगढ़']\n",
            "Skipping: ['MUSEUM', 'OF', 'WORKS', 'BY', 'THEOPHILOS'] - ['म्युज़ियम', 'ऑफ', 'वर्क्स', 'बाय', 'थ', 'ियोफ', 'िलस']\n",
            "Skipping: ['CHENGALPATTU'] - ['चेंगलपट्', 'टु']\n",
            "Skipping: ['VIV', 'RICHARDS'] - ['विव', 'रिचर्ड्', 'स']\n",
            "Skipping: ['LAUREUS', 'WORLD', 'SPORTS', 'AWARDS'] - ['लॉरेस', 'वर्ल्ड', 'स्पोर्ट्', 'स', 'अवार्ड्', 'स']\n",
            "Skipping: ['WILSON', 'CASTLE'] - ['व', 'िल्सन', 'कॅसल']\n",
            "Skipping: ['STATE', 'HERALDIC', 'MUSEUM'] - ['स्टेट', 'हेराल्ड', 'िक', 'म्युज़ियम']\n",
            "Skipping: ['NYIKA', 'PLATEAU'] - ['न्याइका', 'प्लेट्', 'यू']\n",
            "Skipping: ['LAGARD', 'RE', 'GROUPE'] - ['लैगार्डेयर', 'ग्रुप']\n",
            "Skipping: ['PADMA', 'BHUSHAN'] - ['पद्', 'म', 'भूषण']\n",
            "Skipping: ['ROYAL', 'ONTARIO', 'MUSEUM'] - ['रॉयल', 'ऑन्टार', 'ियो', 'म्युज़ियम']\n",
            "Skipping: ['GOVERNMENT', 'MODEL', 'HIGHER', 'SECONDARY', 'SCHOOL', 'THIRUVANANTHAPURAM'] - ['गवर्नमेंट', 'मॉडेल', 'हायर', 'सेकंडरी', 'स्कूल', 'थ', 'िरुवनंतपुरम']\n",
            "Skipping: ['SRISAILAM'] - ['श्री', 'शैलम']\n",
            "Skipping: ['SPATHAREION', 'MUSEUM', 'OF', 'THE', 'SHADOW', 'THEATRE'] - ['स्पाथेर', 'ियन', 'म्युज़ियम', 'ऑफ', 'द', 'शैडो', 'थ', 'िएटर']\n",
            "Skipping: ['DAINIK', 'JANASADHARAN'] - ['दैन', 'िक', 'जनसाधारण']\n",
            "Skipping: ['COLVIN', 'TALUQADARS', 'COLLEGE', 'LUCKNOW'] - ['कॉल्व', 'िन', 'तालुकदार्स', 'कॉलेज', 'लखनऊ']\n",
            "Skipping: ['SADGUNA'] - ['सद्', 'गुण']\n",
            "Skipping: ['SUMMIT', 'AWARDS'] - ['सुमित', 'अवार्ड्', 'स']\n",
            "Skipping: ['THE', 'HINDU'] - ['द', 'ह', 'िन्दू']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'CORFU'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'कोरफू']\n",
            "Skipping: ['PORTSMOUTH'] - ['पोर्ट्', 'समाउथ']\n",
            "Skipping: ['FORT', 'STEURGAT'] - ['फोर्ट', 'स्ट्', 'यूर्गेट']\n",
            "Skipping: ['ADWAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['KAN', 'HAIYAALAAL'] - ['कन्हैयालाल']\n",
            "Skipping: ['KARA', 'KUM'] - ['काराकुम']\n",
            "Skipping: ['CHASHME', 'BADDOOR'] - ['चश्मे', 'बद्', 'दूर']\n",
            "Skipping: ['RAILWAY', 'MUSEUM', 'OF', 'THE', 'MUNICIPALITY', 'OF', 'KALAMATA'] - ['रेल्वे', 'म्युज़ियम', 'ऑफ', 'द', 'म्यून', 'िस', 'िपाल्टी', 'ऑफ', 'कलमाटा']\n",
            "Skipping: ['ADVAAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['MUSEUM', 'OF', 'ROMANI', 'CULTURE'] - ['म्युज़ियम', 'ऑफ', 'रोमान', 'ी', 'कल्चर']\n",
            "Skipping: ['SHAKE', 'HANDS'] - ['शेक', 'हैंड्', 'स']\n",
            "Skipping: ['WIND', 'RIVER'] - ['विंडरिवर']\n",
            "Skipping: ['HAMILTON', 'MASAKADZA'] - ['हैमिल्टन', 'मसाकद्', 'ज़ा']\n",
            "Skipping: ['CHARLES', 'ROBERTS', 'AWARD'] - ['चार्ल्स', 'रॉबर्ट्', 'स', 'अवार्ड']\n",
            "Skipping: ['PATHANAMTHITTA'] - ['पथानामथीट्', 'टा']\n",
            "Skipping: ['NETAJI', 'SUBHASH', 'CHANDRA', 'BOSE'] - ['नेताजी', 'सुभाषचंद्र', 'बोस']\n",
            "Skipping: ['ROCKBROOK', 'UNITED'] - ['रॉकब्रुक', 'यूनाइटेड', 'मेथोडिस्ट']\n",
            "Skipping: ['MUQADDAS'] - ['मुक़द्', 'दस']\n",
            "Skipping: ['WALTER', 'SCOTT'] - ['वॉल्टरस्कॉट']\n",
            "Skipping: ['COLOURPLUS', 'FASHIONS'] - ['कलर', 'प्लस', 'फ़ैशन्स']\n",
            "Skipping: ['ATTUR'] - ['अट्', 'टूर']\n",
            "Skipping: ['GEOLOGICAL', 'MUSEUM', 'OF', 'CHINA'] - ['ज', 'िओलॉजकिल', 'म्युज़ियम', 'ऑफ', 'चाइना']\n",
            "Skipping: ['BAL', 'KRISHNA'] - ['बालकृष्णा']\n",
            "Skipping: ['THE', 'MORNING', 'QUICK'] - ['द', 'मॉर्निंग', 'क्व', 'िक']\n",
            "Skipping: ['SHAILE', 'DR'] - ['शैलेंद्र']\n",
            "Skipping: ['RAAJEN', 'DR'] - ['राजेन्द्र']\n",
            "Skipping: ['PORTSDOWN', 'HILL'] - ['पोर्ट्', 'सडाउन', 'हिल']\n",
            "Skipping: ['WEST', 'SPITSBERGEN'] - ['वेस्ट', 'स्पीट्', 'सबर्गन']\n",
            "Skipping: ['STIRLING', 'SMITH', 'MUSEUM', 'AND', 'ART', 'GALLERY'] - ['स्टर्ल', 'िंग', 'स्म', 'िथ', 'म्युज़ियम', 'एण्ड', 'आर्ट', 'गैलरी']\n",
            "Skipping: ['ARMY', 'PUBLIC', 'SCHOOL', 'NEW', 'DELHI'] - ['आर्मी', 'पब्', 'लिक', 'स्', 'कूल', 'न्यू', 'डेल्ही']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHtNuQRKjS4n"
      },
      "source": [
        "X_train,Y_train  = data_store.generate_random_batch(2)\n",
        "\n",
        "#Generating a batch of 2 samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSpaVE4jXDh",
        "outputId": "ad77738e-c620-43c8-9a35-2985145ac151"
      },
      "source": [
        "print(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['BEKTASH', 'WALI'] ['बेकताश', 'वाली']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-SXTGu7jaCq"
      },
      "source": [
        "X_train_new = convert_eng_to_encoded(X_train)\n",
        "Y_train_new = convert_hindi_to_encoded(Y_train)\n",
        "\n",
        "#Calling the encoding functions for our data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivdKuMdWjeTM",
        "outputId": "5b99710a-2fb2-4328-ea60-5b60ece790fa"
      },
      "source": [
        "Y_train_new[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eLj1d7hjfK7"
      },
      "source": [
        "embedding = nn.Embedding(27,256)\n",
        "\n",
        "#We want to use embedding in our network which is a method that captures semantics between the letters, i.e the relationship amongst them\n",
        "#Embeddding assists in training our data so we will initially move our data which has one dimension that stores the index of the letter to 256 dimensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txrlB3O0lb5D"
      },
      "source": [
        "X_train_new1 = embedding(X_train_new[0].long())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVw64wGDlfuf",
        "outputId": "44cfdfb6-eb3a-42b3-dd79-122569f8b67c"
      },
      "source": [
        "X_train_new1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWGA76omm8tx"
      },
      "source": [
        "## Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vxo2yRWls9b"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size,emb_size,hidden_size):\n",
        "    super().__init__()  \n",
        "    self.input_size  = input_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.input_size,self.emb_size)\n",
        "\n",
        "    self.rnn = nn.LSTM(self.emb_size,self.hidden_size)\n",
        "  \n",
        "  def forward(self,input_word):\n",
        "    input_to_rnn = self.embedding(input_word.long())\n",
        "\n",
        "    output, (h_state,c_state) = self.rnn(input_to_rnn)\n",
        "\n",
        "    return h_state,c_state\n",
        "\n",
        "#Since we want to do a sequence to sequence problem, the following methodology is followed\n",
        "#First we will encode our given sequence into a fixed dimension\n",
        "#Then this data will be passed on to the decoder, which will return the transliterated version of our sequence\n",
        "#This task will be divided into three steps, the first being that the data is encoded, then decoded and finally a class that calls these classes as needed\n",
        "#This is the encoder of the model, we specify all the dimensions in the init function and the LSTM cell that we shall be using\n",
        "#So our input word is embedded first and is passed through the LSTM cell at once\n",
        "#The hidden and cell states of the LSTM cell are returned as the encoded form of the word to be used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aki3sevnoKQN"
      },
      "source": [
        "## Decoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fryk6UwOoIqg"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,output_size,emb_size,hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size,emb_size)\n",
        "    self.rnn = nn.LSTM(emb_size, hidden_size)\n",
        "    self.fc = nn.Linear(hidden_size,output_size)\n",
        "  \n",
        "  def forward(self,input,h_state,c_state):\n",
        "\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    input_to_rnn = self.embedding(input.long())\n",
        "\n",
        "    output,(h_state,c_state) = self.rnn(input_to_rnn,(h_state,c_state))\n",
        "\n",
        "    prediction = self.fc(output.squeeze(0))\n",
        "\n",
        "    return prediction, h_state, c_state\n",
        "\n",
        "#This is the decoder model, wherein the encoded word is converted to the output\n",
        "#This is a single cell of the decoder, i.e a single output will be given \n",
        "#We specify the dimensions as usual, but there is also a final linear layer that will give us a distribution over the output size\n",
        "#The distribution will form a part of the input of the next cell so that is also returned, as well as the hidden and cell states of the network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nTeBjb2rj1A"
      },
      "source": [
        "## Seq-2-seq Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRlvhvRHrhxl"
      },
      "source": [
        "class seq2seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self,input, target,teacher_forcing_ratio = 0.5):\n",
        "\n",
        "    target_length = target.shape[0]\n",
        "    batch_size = target.shape[1]\n",
        "    output_size = self.decoder.output_size\n",
        "\n",
        "    outputs_to_return = torch.zeros(target_length,batch_size,output_size)\n",
        "\n",
        "    h_state,c_state = self.encoder(input)\n",
        "\n",
        "    input = torch.zeros(1)\n",
        "\n",
        "    for i in range(target_length):\n",
        "      output,h_state,c_state = self.decoder(input,h_state,c_state)\n",
        "\n",
        "      outputs_to_return[i] = output\n",
        "\n",
        "      teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "\n",
        "      top1 = output.argmax(1)\n",
        "\n",
        "      input = target[i] if teacher_force else top1\n",
        "    \n",
        "    return outputs_to_return\n",
        "\n",
        "#Here the task comes all together in the sequence-to-sequence architecture\n",
        "#The encoder and decoder we have created are taken and a final output sequence structure is initialized to all zeros\n",
        "#The input is passed through the encoder all at once and the final states are taken\n",
        "#Then we loop over the target length and pass the hidden states as input to the first decoder cell to get an output, the initial input is zero\n",
        "#The output is appended to the outputs tensor and is also passed on as input to the next cell\n",
        "#Teacher-forcing is done at a probability of 50%, which means that initially the outputs are not passed on, rather the actual values are \n",
        "#Finally the outputs tensor is returned \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85oGQDnv17-W"
      },
      "source": [
        "## Setting up the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9y_cw3j14Wd"
      },
      "source": [
        "input_size = 27\n",
        "output_size = 129\n",
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = Encoder(input_size,embedding_size,hidden_size)\n",
        "decoder = Decoder(output_size,embedding_size,hidden_size)\n",
        "model = seq2seq(encoder,decoder)\n",
        "\n",
        "#Instantiating all classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN3EkfkA3A4F"
      },
      "source": [
        "### Initializing weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMwyaOEz2UKm",
        "outputId": "b3cc85bb-10a1-4bb9-c824-0f129e1024a0"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)\n",
        "\n",
        "#Method to initialize weights in the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(27, 256)\n",
              "    (rnn): LSTM(256, 512)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(129, 256)\n",
              "    (rnn): LSTM(256, 512)\n",
              "    (fc): Linear(in_features=512, out_features=129, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4yhNTxf3bSA"
      },
      "source": [
        "opt = optim.Adam(model.parameters(),lr=0.0005)\n",
        "\n",
        "#Optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eU5Pv7n3mn4"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "#Criterion which ignores the padding in the network "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u8ufVOW4jEA"
      },
      "source": [
        "### Setting up our training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zpTgqXm4nNi"
      },
      "source": [
        "def train(model, data_loader, loss_func, optimizer,batch_size):\n",
        "  \n",
        "  model.train()\n",
        "  X_train,Y_train = data_loader.generate_random_batch(batch_size)\n",
        "  X_train_new = convert_eng_to_encoded(X_train)\n",
        "  Y_train_new = convert_hindi_to_encoded(Y_train)\n",
        "  \n",
        "  total_loss = 0\n",
        "\n",
        "  for i in range(len(X_train_new)):\n",
        "    input = X_train_new[i]\n",
        "    target = Y_train_new[i]\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(input,target)\n",
        "\n",
        "    output_size = output.shape[-1]\n",
        "\n",
        "    loss = loss_func(output.view(-1,output_size),target.view(-1).long())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss/batch_size\n",
        "\n",
        "#Training function, which takes a batch of inputs and passes them through the model\n",
        "#loss is computed, back propagated and weights are updated\n",
        "#Clipping is done to avoid explosion of gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaZVilxp65mJ"
      },
      "source": [
        "### Evaluate function, similar to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhZH_uMt6713"
      },
      "source": [
        "def evaluate(model, test_data_loader, loss_func):\n",
        "  \n",
        "  model.eval()\n",
        "  X_test,Y_test = test_data_loader.generate_random_batch(32)\n",
        "  X_test_new = convert_eng_to_encoded(X_test)\n",
        "  Y_test_new = convert_hindi_to_encoded(Y_test)\n",
        "  \n",
        "  total_loss = 0\n",
        "\n",
        "  for i in range(len(X_test_new)):\n",
        "    input = X_test_new[i]\n",
        "    target = Y_test_new[i]\n",
        "\n",
        "\n",
        "    output = model(input,target)\n",
        "\n",
        "    output_size = output.shape[-1]\n",
        "\n",
        "    loss = loss_func(output.view(-1,output_size),target.view(-1).long())\n",
        "\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss/32\n",
        "\n",
        "#Evaluation function is similar, returns the loss on test data\n",
        "#The model is sent into eval mode and no optimization is done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df-9wLV2-31B"
      },
      "source": [
        "## Training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpBn1Hxc_Nml",
        "outputId": "70b55b50-f27f-4c5f-de4e-05fb4e90f164"
      },
      "source": [
        "data_tester = EncoderDecoderData('testing.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping: ['W', 'TTEMBERG'] - ['यूटमबर्ग']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "1792a028fd3f46d6b4ac14ea00fc58ea",
            "1773227a36984c7abeb5a04956ce29cf",
            "7e596b26d76145aabf8d02c38030f161",
            "765da8347d72407ca6d8711869f158b8",
            "52c2d1db9c604bbdb035db14f6dfa61e",
            "4cdc161be931416e972671d084a5f604",
            "74c0317e018d403dbddc1051d693de6b",
            "5974a930b9f04783b2b7739da8af98cb"
          ]
        },
        "id": "ttpp-alI-5DO",
        "outputId": "86c4e0bf-b94a-48cc-b17b-e508acc11de0"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "for i in tqdm_notebook(range(epochs)):\n",
        "  print('Training Loss:',train(model,data_store,loss_func,opt,128))\n",
        "  print('validation Loss:',evaluate(model,data_store,loss_func))\n",
        "\n",
        "#Training and evaluation functions are called"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1792a028fd3f46d6b4ac14ea00fc58ea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 3.8625217471271753\n",
            "validation Loss: 3.363831214606762\n",
            "Training Loss: 3.472673501819372\n",
            "validation Loss: 3.600444108247757\n",
            "Training Loss: 3.38296558894217\n",
            "validation Loss: 3.387877456843853\n",
            "Training Loss: 3.3665125723928213\n",
            "validation Loss: 3.367708809673786\n",
            "Training Loss: 3.308714421465993\n",
            "validation Loss: 3.291763760149479\n",
            "Training Loss: 3.1980430940166116\n",
            "validation Loss: 3.075355689972639\n",
            "Training Loss: 3.2233139565214515\n",
            "validation Loss: 3.281013384461403\n",
            "Training Loss: 3.2824756810441613\n",
            "validation Loss: 3.234028771519661\n",
            "Training Loss: 3.20284772105515\n",
            "validation Loss: 3.305533707141876\n",
            "Training Loss: 3.2274077744223177\n",
            "validation Loss: 3.132484659552574\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
