{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder_Decoder_Batched.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b259bd67330e43fcb87421582559b4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f25fd0ffa23947528f8f624936182e30",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b55b633bd4314fe69a6da6a9b7b6327f",
              "IPY_MODEL_f6b2dcbf188341b5a6401f50a8af3d82"
            ]
          }
        },
        "f25fd0ffa23947528f8f624936182e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b55b633bd4314fe69a6da6a9b7b6327f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23b23ad8c6c64c558a73a860fa7663c0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db7a5ce286184825820fe55502efc5d8"
          }
        },
        "f6b2dcbf188341b5a6401f50a8af3d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32cd702b7f624c51b2cf883578b27467",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [19:17&lt;00:00, 11.57s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c02f9decfd7e48d89387595fe0e31e71"
          }
        },
        "23b23ad8c6c64c558a73a860fa7663c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db7a5ce286184825820fe55502efc5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32cd702b7f624c51b2cf883578b27467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c02f9decfd7e48d89387595fe0e31e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88a42681d96b44908686caf395bf4129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0fb3755ca9cf451fa866ee826170fe1a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a7c0376409b84b4c8c5c3e77886b4a22",
              "IPY_MODEL_e6e6fdb0b52541bab8ecf52f2d79d523"
            ]
          }
        },
        "0fb3755ca9cf451fa866ee826170fe1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7c0376409b84b4c8c5c3e77886b4a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ec453478080c4e27970ea579100b7ac2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_254d78e0d0d3477cb83ff54052e71121"
          }
        },
        "e6e6fdb0b52541bab8ecf52f2d79d523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8e716b63f17421db7d22cb2729d761d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [18:57&lt;00:00, 11.37s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ed947ccd2b34c2393422e0b221fa275"
          }
        },
        "ec453478080c4e27970ea579100b7ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "254d78e0d0d3477cb83ff54052e71121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8e716b63f17421db7d22cb2729d761d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ed947ccd2b34c2393422e0b221fa275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgohrani/Recurrent_Neural_Networks/blob/main/3.Project_Encoder_Decoder_Batched.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18EWGbGeXEp7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score , mean_squared_error\n",
        "import matplotlib.colors\n",
        "import math\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import time\n",
        "sns.set()\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "#importing essential libraries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCbI5RHXdE8b"
      },
      "source": [
        "## English and Hindi Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-ylbPvNcoNM",
        "outputId": "9f0e8b52-3a68-438f-d5c0-0f2d5803cbe1"
      },
      "source": [
        "char_eng = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_eng = '<pad>'\n",
        "eng_dict = {}\n",
        "eng_dict[pad_eng] = 0\n",
        "\n",
        "for i,letter in enumerate(char_eng):\n",
        "  eng_dict[letter] = i+1\n",
        "\n",
        "print(eng_dict)\n",
        "\n",
        "#Creating a dictionary for english and hindi letters which will be used for encoding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3O-Agp_crS8",
        "outputId": "956fb3c4-2af6-4b81-ea87-d5d5cc072028"
      },
      "source": [
        "hindi_dict = {}\n",
        "hindi_dict['<pad>'] = 0\n",
        "\n",
        "char_hindi = ''\n",
        "for i in range(2304, 2432):\n",
        "  char_hindi += chr(i)\n",
        "\n",
        "for i, letter in enumerate(char_hindi):\n",
        "  hindi_dict[letter] = i+1\n",
        "\n",
        "print(hindi_dict)\n",
        "\n",
        "#Hindi Dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irmgBQ_LdJbF"
      },
      "source": [
        "## Text Preprocessing helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsnyBKKscssl"
      },
      "source": [
        "def split_hindi_words(hindi_list):\n",
        "  new_hindi_list = []\n",
        "  for hind_name in hindi_list:\n",
        "    hind_name = hind_name.replace(',',' ').replace('_',' ').replace('.',' ').replace(\"'\",' ').replace('-',' ').replace('/',' ').replace('\\u200d',' ').replace('(',' ').replace(')',' ').replace('?',' ')\n",
        "    hind_name = hind_name.split()\n",
        "    new_hindi_list.append(hind_name)\n",
        "  return new_hindi_list\n",
        "\n",
        "def split_english_words(english_list):\n",
        "  new_english_list = []\n",
        "  regex = re.compile('[^a-zA-Z]')\n",
        "  for eng_name in english_list:\n",
        "    eng_name = eng_name.upper()\n",
        "    eng_name = eng_name.replace(\"'\",'').replace('/',' ')\n",
        "    eng_name = regex.sub(' ', eng_name)\n",
        "    eng_name = eng_name.split()\n",
        "    new_english_list.append(eng_name)\n",
        "  return new_english_list\n",
        "\n",
        "def clean_english_list(eng_list):\n",
        "  regex = re.compile('[^a-zA-Z]')\n",
        "  new_english_names = []\n",
        "  for word in eng_list:\n",
        "    new_english_names.append(regex.sub('', word))\n",
        "  return new_english_names\n",
        "\n",
        "def clean_hindi_list(hindi_list):\n",
        "  new_hindi_names = []\n",
        "  for word in hindi_list:\n",
        "    word = word.replace(',','').replace('_','').replace('.','').replace(\"'\",'')\n",
        "    new_hindi_names.append(word)\n",
        "  return new_hindi_names\n",
        "\n",
        "#Some helper functions to help preprocess the text we have \n",
        "#First functions splits the hindi strings into single words and returns a list of the same, same for english\n",
        "#The next two functions removes unnecessary characters from english and hindi words\n",
        "#These functions will be used later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pebQTs9dNqQ"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgkLPqGicuO2"
      },
      "source": [
        "def convert_eng_to_encoded(X_train):\n",
        "  list_to_return = []\n",
        "  for word in X_train:\n",
        "    onehotstart = torch.zeros([len(word)+1,1])\n",
        "    onehotstart[len(word)][0] = 1\n",
        "    for i,letter in enumerate(word):\n",
        "      index = eng_dict[letter]\n",
        "      onehotstart[i][0] = index\n",
        "    list_to_return.append(onehotstart)\n",
        "  \n",
        "  return list_to_return \n",
        "\n",
        "def convert_hindi_to_encoded(Y_train):\n",
        "  list_to_return = []\n",
        "  for word in Y_train:\n",
        "    onehotstart = torch.zeros([len(word)+1,1])\n",
        "    onehotstart[len(word)][0] = 0\n",
        "    for i,letter in enumerate(word):\n",
        "      index = hindi_dict[letter]\n",
        "      onehotstart[i][0] = index\n",
        "    list_to_return.append(onehotstart)\n",
        "  \n",
        "  return list_to_return\n",
        "\n",
        "#Functions for encoding but not one hot encoding, the index value is directly used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg3t-IO_dP7R"
      },
      "source": [
        "## Class to process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTWLoBEMcv4n"
      },
      "source": [
        "class EncoderDecoderData():\n",
        "\n",
        "  def __init__(self,filename):\n",
        "    self.final_eng_list, self.final_hindi_list = self.create_data_from_XML(filename)\n",
        "\n",
        " \n",
        "  def create_data_from_XML(self,filename):\n",
        "    tree = ET.parse(filename)\n",
        "    root = tree.getroot()\n",
        "    english_names = []\n",
        "    hindi_names = []\n",
        "    for elem in root.iter():\n",
        "      if elem.tag == 'SourceName':\n",
        "        english_names.append(elem.text)\n",
        "      if elem.tag == 'TargetName' and elem.attrib['ID'] == '1':\n",
        "        hindi_names.append(elem.text)\n",
        "    \n",
        "    new_english_names = split_english_words(english_names)\n",
        "    new_hindi_names = split_hindi_words(hindi_names)\n",
        "\n",
        "    final_hindi_data = []\n",
        "    final_english_data = []\n",
        "    \n",
        "    for eng_word, hindi_word in zip(new_english_names, new_hindi_names):\n",
        "      if len(eng_word) != len(hindi_word):\n",
        "        print('Skipping:', eng_word, '-', hindi_word)\n",
        "      else:\n",
        "        for eng_word_part, hindi_word_part in zip(eng_word, hindi_word):\n",
        "          final_hindi_data.append(hindi_word_part)\n",
        "          final_english_data.append(eng_word_part)\n",
        "    \n",
        "    final_hindi_data = clean_hindi_list(final_hindi_data)\n",
        "    final_english_data = clean_english_list(final_english_data)\n",
        "    self.final_eng_list = final_english_data\n",
        "    self.final_hindi_list = final_hindi_data\n",
        "    return self.final_eng_list, self.final_hindi_list\n",
        "\n",
        "\n",
        "  def generate_random_sample(self):\n",
        "    index = np.random.randint(len(self.final_eng_list))\n",
        "    return self.final_eng_list[index], self.final_hindi_list[index]\n",
        "  \n",
        "  def generate_random_batch(self,batch_size):\n",
        "    index = np.random.randint(len(self.final_eng_list))\n",
        "    batch_list_english = []\n",
        "    batch_list_hindi = []\n",
        "    for i in range(index,index+batch_size,1):\n",
        "      if i >= len(self.final_eng_list):\n",
        "        batch_list_english.append(self.final_eng_list[i-len(self.final_eng_list)])\n",
        "        batch_list_hindi.append(self.final_hindi_list[i-len(self.final_eng_list)])\n",
        "      else:\n",
        "        batch_list_english.append(self.final_eng_list[i])\n",
        "        batch_list_hindi.append(self.final_hindi_list[i])\n",
        "    return batch_list_english, batch_list_hindi\n",
        "  \n",
        "#This is a class we will call on the xml data we have for english and hindi\n",
        "#In the init function, a create_data_from_xml is called which is defined below\n",
        "#In create_data_from XML, the file is parsed and its root is iterated over to get the hindi strings and their corresponding english strings\n",
        "#ID = 1 is used since one string might have multiple others\n",
        "#We now have equivalent strings in the form of a nested list. Our next step is to remove strings which have unequal number of words, so we compare and skip\n",
        "#Finally individual words are added into a new list, and the cleaning functions are called as usual\n",
        "#Two functions to create data for us are used, one which returns a sample of a hindi and english word and the other which returns a consecutive batch of words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioRVAMgFdSLL"
      },
      "source": [
        "## Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-RoeyRscylY"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size,emb_size,hidden_size):\n",
        "    super().__init__()  \n",
        "    self.input_size  = input_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.input_size,self.emb_size)\n",
        "\n",
        "    self.rnn = nn.LSTM(self.emb_size,self.hidden_size)\n",
        "  \n",
        "  def forward(self,input_word):\n",
        "    input_to_rnn = self.embedding(input_word.long())\n",
        "\n",
        "    output, (h_state,c_state) = self.rnn(input_to_rnn)\n",
        "\n",
        "    return h_state,c_state\n",
        "\n",
        "#Since we want to do a sequence to sequence problem, the following methodology is followed\n",
        "#First we will encode our given sequence into a fixed dimension\n",
        "#Then this data will be passed on to the decoder, which will return the transliterated version of our sequence\n",
        "#This task will be divided into three steps, the first being that the data is encoded, then decoded and finally a class that calls these classes as needed\n",
        "#This is the encoder of the model, we specify all the dimensions in the init function and the LSTM cell that we shall be using\n",
        "#So our input word is embedded first and is passed through the LSTM cell at once\n",
        "#The hidden and cell states of the LSTM cell are returned as the encoded form of the word to be used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psib9-GQdURc"
      },
      "source": [
        "## Decoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cmQeDSDczsC"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,output_size,emb_size,hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size,emb_size)\n",
        "    self.rnn = nn.LSTM(emb_size, hidden_size)\n",
        "    self.fc = nn.Linear(hidden_size,output_size)\n",
        "  \n",
        "  def forward(self,input,h_state,c_state):\n",
        "\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    input_to_rnn = self.embedding(input.long())\n",
        "\n",
        "    output,(h_state,c_state) = self.rnn(input_to_rnn,(h_state,c_state))\n",
        "\n",
        "    prediction = self.fc(output.squeeze(0))\n",
        "\n",
        "    return prediction, h_state, c_state\n",
        "\n",
        "#This is the decoder model, wherein the encoded word is converted to the output\n",
        "#This is a single cell of the decoder, i.e a single output will be given \n",
        "#We specify the dimensions as usual, but there is also a final linear layer that will give us a distribution over the output size\n",
        "#The distribution will form a part of the input of the next cell so that is also returned, as well as the hidden and cell states of the network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhxbO2xjdWH9"
      },
      "source": [
        "## Seq-2-Seq Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L640tfLkc04k"
      },
      "source": [
        "class seq2seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self,input, target,teacher_forcing_ratio = 0.5):\n",
        "\n",
        "    target_length = target.shape[0]\n",
        "    batch_size = target.shape[1]\n",
        "    output_size = self.decoder.output_size\n",
        "\n",
        "    outputs_to_return = torch.zeros(target_length,batch_size,output_size)\n",
        "\n",
        "    h_state,c_state = self.encoder(input)\n",
        "\n",
        "    input = torch.zeros(batch_size)\n",
        "\n",
        "    for i in range(target_length):\n",
        "      output,h_state,c_state = self.decoder(input,h_state,c_state)\n",
        "\n",
        "      outputs_to_return[i] = output\n",
        "\n",
        "      teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "\n",
        "      top1 = output.argmax(1)\n",
        "\n",
        "      input = target[i] if teacher_force else top1\n",
        "    \n",
        "    return outputs_to_return\n",
        "\n",
        "#Here the task comes all together in the sequence-to-sequence architecture\n",
        "#The encoder and decoder we have created are taken and a final output sequence structure is initialized to all zeros\n",
        "#The input is passed through the encoder all at once and the final states are taken\n",
        "#Then we loop over the target length and pass the hidden states as input to the first decoder cell to get an output, the initial input is zero\n",
        "#The output is appended to the outputs tensor and is also passed on as input to the next cell\n",
        "#Teacher-forcing is done at a probability of 50%, which means that initially the outputs are not passed on, rather the actual values are \n",
        "#Finally the outputs tensor is returned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-6uBh0HdbvA"
      },
      "source": [
        "## Setting up the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z279CrCTc2JL"
      },
      "source": [
        "input_size = 27\n",
        "output_size = 129\n",
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = Encoder(input_size,embedding_size,hidden_size)\n",
        "decoder = Decoder(output_size,embedding_size,hidden_size)\n",
        "model = seq2seq(encoder,decoder)\n",
        "\n",
        "#Instantiating all classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G36-oiCxdfXa"
      },
      "source": [
        "### Weight initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N39z0Svc3nN",
        "outputId": "00ade91e-a35c-4a58-aa86-139970ba5d2e"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)\n",
        "\n",
        "#Method to initialize weights in the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(27, 256)\n",
              "    (rnn): LSTM(256, 512)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(129, 256)\n",
              "    (rnn): LSTM(256, 512)\n",
              "    (fc): Linear(in_features=512, out_features=129, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDWoLCUXdhfc"
      },
      "source": [
        "### Loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrRsMVEdc42s"
      },
      "source": [
        "opt = optim.Adam(model.parameters(),lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP-yZtiYc61t"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diagyyLUdjil"
      },
      "source": [
        "### Training Helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0jzGDiHc76u"
      },
      "source": [
        "def train(model, data_loader, loss_func, optimizer,no_of_batches):\n",
        "  \n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  for i in range(no_of_batches):\n",
        "    X_train,Y_train = data_store.generate_random_batch(5000)\n",
        "    X_train, Y_train = convert_eng_to_encoded(X_train), convert_hindi_to_encoded(Y_train)\n",
        "    X_batch = []\n",
        "    Y_batch = []\n",
        "    sequence_length = np.random.randint(3,10)\n",
        "    for xpoint, ypoint in zip(X_train,Y_train):\n",
        "      if len(X_batch) == 16:\n",
        "        break\n",
        "      if ypoint.shape[0] == sequence_length:\n",
        "        X_batch.append(xpoint)\n",
        "        Y_batch.append(ypoint)\n",
        "    max_input_length = 0\n",
        "    for xpoint in X_batch:\n",
        "      if xpoint.shape[0] > max_input_length:\n",
        "        max_input_length = xpoint.shape[0]\n",
        "    X_final_batch = torch.zeros(max_input_length,16)\n",
        "    Y_final_batch = torch.zeros(sequence_length,16)\n",
        "    for i in range(16):\n",
        "      actual = X_batch[i].shape[0]\n",
        "      X_final_batch[:actual,i] = X_batch[i].view(-1)\n",
        "    for i in range(16):\n",
        "      Y_final_batch[:,i] = Y_batch[i].view(-1)\n",
        "    \n",
        "        \n",
        "    input = X_final_batch\n",
        "    target = Y_final_batch\n",
        "    \n",
        "    \n",
        "\n",
        "    output = model(input,target)\n",
        "\n",
        "    output_size = output.shape[-1]\n",
        "\n",
        "    loss = loss_func(output.view(-1,output_size),target.view(-1).long())\n",
        "    total_loss += loss.item()\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  \n",
        "\n",
        "  return total_loss/no_of_batches\n",
        "\n",
        "#Training function, which takes a batch of inputs and passes them through the model\n",
        "#loss is computed, back propagated and weights are updated\n",
        "#Clipping is done to avoid explosion of gradients\n",
        "\n",
        "#The difference here is that we want to use batching in our model to train faster\n",
        "#We take a batch of 5000 words and encode it\n",
        "#We then get 16 samples for which the y values are of the same size, x need not be the same \n",
        "#Once we have those, we find the largest x length and make all x values equal to that length by adding zeros\n",
        "#We then take all of them together as batches so we now have x of a particular size and y of a particular size\n",
        "#These batches are passed as input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ZKzP99dni_"
      },
      "source": [
        "### Evaluation helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg_oaLQEc9Iv"
      },
      "source": [
        "def evaluate(model, test_data_loader, loss_func):\n",
        "  \n",
        "  model.eval()\n",
        "  X_test,Y_test = test_data_loader.generate_random_batch(32)\n",
        "  X_test_new = convert_eng_to_encoded(X_test)\n",
        "  Y_test_new = convert_hindi_to_encoded(Y_test)\n",
        "  \n",
        "  total_loss = 0\n",
        "\n",
        "  for i in range(len(X_test_new)):\n",
        "    input = X_test_new[i]\n",
        "    target = Y_test_new[i]\n",
        "\n",
        "\n",
        "    output = model(input,target)\n",
        "\n",
        "    output_size = output.shape[-1]\n",
        "\n",
        "    loss = loss_func(output.view(-1,output_size),target.view(-1).long())\n",
        "\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss/32\n",
        "\n",
        "#Evaluation function is similar, returns the loss on test data\n",
        "#The model is sent into eval mode and no optimization is done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkeMkZ3Ndp1Q"
      },
      "source": [
        "## Starting the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dRhBx7LhkcZ",
        "outputId": "360c104f-0131-441b-b7e5-4ef7e116e8d8"
      },
      "source": [
        "data_store = EncoderDecoderData('training.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping: ['MAHARANI', 'PADMINI'] - ['महारानी', 'पद्', 'मिनी']\n",
            "Skipping: ['STATE', 'MUSEUM', 'OF', 'THE', 'VERMONT', 'HISTORICAL', 'SOCIETY'] - ['स्टेट', 'म्युज़ियम', 'ऑफ', 'द', 'वरमाउंट', 'हिस्टॉरिकल', 'सोसायट', 'ी']\n",
            "Skipping: ['I', 'DUKAANT'] - ['इंदुकांत']\n",
            "Skipping: ['EFFIE', 'AWARDS'] - ['एफी', 'अवार्ड्', 'स']\n",
            "Skipping: ['LAURENCE', 'OLIVIER', 'AWARDS'] - ['लॉरेंस', 'ओलिवर', 'अवार्ड्', 'स']\n",
            "Skipping: ['ETTA'] - ['एट्', 'टा']\n",
            "Skipping: ['COLLEGE', 'FOOTBALL', 'AWARDS'] - ['कॉलेज', 'फुटबॉल', 'अवार्ड्', 'स']\n",
            "Skipping: ['STEVE', 'RHODES'] - ['स्टीव', 'रोड्', 'स']\n",
            "Skipping: ['WINDHAM', 'COUNTY', 'HISTORICAL', 'MUSEUM'] - ['व', 'िंडहैम', 'काउंट', 'ी', 'ह', 'िस्टॉर', 'िकल', 'म्युज़ियम']\n",
            "Skipping: ['PLAZA'] - ['प्लाज़ा', '66']\n",
            "Skipping: ['ADVAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['BHAALACHAN', 'DR'] - ['भालचन्द्र']\n",
            "Skipping: ['BARHARWA', 'JUNCTION'] - ['बरहरवा']\n",
            "Skipping: ['STATE', 'BNK', 'TR'] - ['स्टेट', 'बैंक', 'ऑफ', 'त्रावणकोर']\n",
            "Skipping: ['IN', 'DRAJEET'] - ['इन्द्रजीत']\n",
            "Skipping: ['WOODSTOCK', 'HISTORICAL', 'SOCIETY'] - ['वुडस्टॉक', 'ह', 'िस्टॉर', 'िकल', 'सोसायटी']\n",
            "Skipping: ['BROOKLYN', 'MUSEUM'] - ['ब्रुकल', 'िन', 'म्युज़ियम']\n",
            "Skipping: ['SAINT', 'FRANCIS', 'DASSISI', 'HIGH', 'SCHOOL'] - ['सेंट', 'फ्रांसिस', 'ड', 'िअस', 'ीसी', 'हाई', 'स्कूल']\n",
            "Skipping: ['PADMA', 'VIBHUSHAN'] - ['पद्', 'म', 'विभूषण']\n",
            "Skipping: ['GROUPE', 'CAISSE', 'D', 'PARGME'] - ['ग्रुप', 'कैस', 'डेपॉर्मे']\n",
            "Skipping: ['BRITISH', 'BOOK', 'AWARDS'] - ['ब्रिटिश', 'बुक', 'अवार्ड्', 'स']\n",
            "Skipping: ['SOUTH', 'ARLINGTON', 'CHURCH', 'OF', 'CHRIST'] - ['साउथ', 'अर्लिंग्टन']\n",
            "Skipping: ['JACK', 'RICHARDS'] - ['जैक', 'रिचर्ड्', 'स']\n",
            "Skipping: ['I', 'DIYAA'] - ['इंडिया']\n",
            "Skipping: ['DAINIK', 'JUGASANKHA'] - ['दैन', 'िक', 'जुगसंखा']\n",
            "Skipping: ['EISNER', 'AWARDS'] - ['ईस्नर', 'अवार्ड्', 'स']\n",
            "Skipping: ['THE', 'PIONEER', 'DAILY'] - ['द', 'पायोन', 'ियर', 'डेली']\n",
            "Skipping: ['SIYASAT'] - ['स', 'ियासत']\n",
            "Skipping: ['SCOTTISH', 'CHURCH', 'COLLEGE', 'CALCUTTA'] - ['स्कॉट', 'िश', 'चर्च', 'कॉलेज', 'कैलकटा']\n",
            "Skipping: ['ISMAIL'] - ['इस्मा', 'ईल']\n",
            "Skipping: ['DESHONNATI'] - ['देशोन्नत', 'ि']\n",
            "Skipping: ['PEARSE', 'MUSEUM'] - ['प', 'ियर्स', 'म्युज़ियम']\n",
            "Skipping: ['TAMBURITZA'] - ['तम्बुरिट्', 'ज़ा']\n",
            "Skipping: ['ANAIKUTTAM', 'RESERVOIR'] - ['अनइकुट्', 'टम', 'रिज़रवायर']\n",
            "Skipping: ['CENTRAL', 'CHRONICLE'] - ['सेंट्रल', 'क्रॉन', 'िकल']\n",
            "Skipping: ['SOCI', 'T', 'G', 'N', 'RALE'] - ['सोसायटी', 'जनरल']\n",
            "Skipping: ['KING', 'EDWARD', 'VII'] - ['किंग', 'एडवर्ड']\n",
            "Skipping: ['ORDER', 'OF', 'SPORTS', 'MERIT'] - ['ऑडर', 'ऑफ', 'स्पोर्ट्', 'स', 'मेरिट']\n",
            "Skipping: ['SRI', 'RAMAKRISHNA', 'VIDYASHALA', 'MYSORE'] - ['श्री', 'रामकृष्ण', 'व', 'िद्याशाला', 'मैसूर']\n",
            "Skipping: ['BRAJEN', 'DR'] - ['ब्रजेन्द्र']\n",
            "Skipping: ['DIBANG', 'VALLEY'] - ['दिबंगवैली']\n",
            "Skipping: ['THATTHA'] - ['ठट्', 'ठा']\n",
            "Skipping: ['LULU'] - ['लु', 'लु']\n",
            "Skipping: ['WILHEM', 'BUSCH', 'MUSEUM'] - ['व', 'िल्हेम', 'बुश', 'म्युज़ियम']\n",
            "Skipping: ['CHITTAR', 'RESERVOIR'] - ['चित्तर', 'रिज़रवायर', '2']\n",
            "Skipping: ['FIELDS', 'MEDAL'] - ['फील्ड्', 'स', 'मेडल']\n",
            "Skipping: ['SQUADS'] - ['सक्वैड्', 'स']\n",
            "Skipping: ['CARMEL', 'CONVENT', 'SCHOOL', 'CHANAKYA', 'PURI'] - ['कारमेल', 'कॉन्', 'वेंट', 'स्कूल', 'चाणक्य', 'पुरी']\n",
            "Skipping: ['VITTHAL'] - ['विट्', 'ठल']\n",
            "Skipping: ['LOGIE', 'AWARD'] - ['लॉग', 'अवार्ड्', 'स']\n",
            "Skipping: ['LOLA', 'AWARDS'] - ['लोला', 'अवार्ड्', 'स']\n",
            "Skipping: ['NOTTINGHAM', 'EAST', 'MIDLANDS'] - ['नॉटिंगहॅम', 'ईस्ट', 'मिडलैंड्', 'स']\n",
            "Skipping: ['MAHINDRA', 'UNITED', 'WORLD', 'COLLEGE', 'OF', 'INDIA'] - ['मह', 'िंद्रा', 'यूनाइटेड', 'वर्ल्ड', 'कॉलेज', 'ऑफ', 'इंडिया']\n",
            "Skipping: ['ADWAAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['ADWAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['THE', 'ECONOMIC', 'TIMES'] - ['द', 'इकनॉम', 'िक', 'टाइम्स']\n",
            "Skipping: ['I', 'JEENIYAR'] - ['इंजीनियर']\n",
            "Skipping: ['JITEN', 'DR'] - ['जितेन्द्र']\n",
            "Skipping: ['PRECISION', 'VALLEY', 'CORVETTE', 'MUSEUM'] - ['प्रिस', 'िज़न', 'वैली', 'कॉरवेट', 'म्युज़ियम']\n",
            "Skipping: ['SCHOCK', 'PRIZE', 'IN', 'VISUAL', 'ARTS'] - ['श्कॉक', 'प्राइज़', 'इन', 'विज़्युल', 'आर्ट्', 'स']\n",
            "Skipping: ['NATUN', 'DIN'] - ['नतुन', 'द', 'िन']\n",
            "Skipping: ['LYCHNOSTATIS', 'OPEN', 'AIR', 'MUSEUM'] - ['ल', 'िक्नोस्टेट', 'िस', 'ओपन', 'एयर', 'म्युज़ियम']\n",
            "Skipping: ['ADVAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['THE', 'TYNE', 'TURRETS'] - ['द', 'टायने', 'टरेट्', 'स']\n",
            "Skipping: ['VERIA', 'FOLKLORE', 'MUSEUM'] - ['वेर', 'िया', 'फोकलोर', 'म्युज़ियम']\n",
            "Skipping: ['DUBLIN', 'WRITERS', 'MUSEUM'] - ['डब्ल', 'िन', 'राइटर्स', 'म्युज़ियम']\n",
            "Skipping: ['NAIDUNIYA'] - ['नईदुन', 'िया']\n",
            "Skipping: ['FIDEL', 'EDWARDS'] - ['फिडल', 'एडवर्ड्', 'स']\n",
            "Skipping: ['ROSTER'] - ['रोस्', 'टर']\n",
            "Skipping: ['ORDER', 'OF', 'VASA'] - ['ऑडर', 'ऑफ़', 'द', 'वासा']\n",
            "Skipping: ['RUBIN', 'MUSEUM', 'OF', 'ART'] - ['रुब', 'िन', 'म्युज़ियम', 'ऑफ', 'आर्ट']\n",
            "Skipping: ['DINAKARAN'] - ['द', 'िनाकरण']\n",
            "Skipping: ['HARVEY', 'AWARDS'] - ['हार्वे', 'अवार्ड्', 'स']\n",
            "Skipping: ['ARGENTINA', 'MUSEUM', 'OF', 'NATURAL', 'SCIENCES'] - ['अर्जेंट', 'ीना', 'म्युज़ियम', 'ऑफ', 'नैचरल', 'साइंसेस']\n",
            "Skipping: ['BIG', 'BROTHER', 'AWARDS'] - ['बिग', 'ब्रदर', 'अवार्ड्', 'स']\n",
            "Skipping: ['ANDROS', 'MARITIME', 'MUSEUM'] - ['एण्ड्रॉस', 'मैर', 'िटाइम', 'म्युज़ियम']\n",
            "Skipping: ['CALCIUM'] - ['कै', 'ल्शियम']\n",
            "Skipping: ['TAKHAT', 'SHRI', 'SACHKHAND', 'SAHIB', 'NANDED', 'MAHARASHTRA'] - ['तखत', 'श्री', 'सचखंड', 'साहिब', 'नांदेड़', 'महाराष्', 'ट्र']\n",
            "Skipping: ['AZAMNAGAR', 'ROAD'] - ['आज़मनगर']\n",
            "Skipping: ['COMMIE', 'AWARDS'] - ['कॉमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['VIDUTHALAI'] - ['व', 'िदुथलई']\n",
            "Skipping: ['AKTIESELSKABET', 'DAMPSKIBSSELSKABET', 'TORM'] - ['एक्टीसल्:कॅबेट', 'डॅम्प्सकीबेसेल्सकॅबेट', 'टॉर्म', '4']\n",
            "Skipping: ['HISTORICAL', 'MUSEUM', 'OF', 'CRETE'] - ['ह', 'िस्टॉर', 'िकल', 'म्युज़ियम', 'ऑफ', 'क्रेट']\n",
            "Skipping: ['FRANCE', 'T', 'L', 'COM'] - ['फ़्रांस', 'टेलीकॉम']\n",
            "Skipping: ['MEDZHYBIZH'] - ['मेझ्ही', 'बिज़']\n",
            "Skipping: ['BRIT', 'AWARDS'] - ['ब्रिट', 'अवार्ड्', 'स']\n",
            "Skipping: ['ASOMIYA', 'PRATIDIN'] - ['असोम', 'िया', 'प्रतिद', 'िन']\n",
            "Skipping: ['RUTGERS'] - ['रट्', 'जर्स']\n",
            "Skipping: ['DAINIK', 'JANAMBHUMI'] - ['दैन', 'िक', 'जन्मभूमि']\n",
            "Skipping: ['CAPE', 'TOWN'] - ['केपटाउन']\n",
            "Skipping: ['PETRONAS', 'TOWER'] - ['प्रेट्रोनॉस', 'टॉवर', '2']\n",
            "Skipping: ['CHATTAN', 'SINGH'] - ['चट्', 'टान', 'सिंह']\n",
            "Skipping: ['AUSTRALIAN', 'WAR', 'MEMORIAL'] - ['ऑस्ट्रेल', 'ियन', 'वार', 'मेमोर', 'ियल']\n",
            "Skipping: ['EADS'] - ['इएड्', 'स']\n",
            "Skipping: ['NEW', 'ZEALAND'] - ['न्यूज़ीलैंड']\n",
            "Skipping: ['MUSEUM', 'AND', 'STUDY', 'CENTRE', 'OF', 'THE', 'GREEK', 'THEATRE'] - ['म्युज़ियम', 'एण्ड', 'स्टडी', 'सेंटर', 'ऑफ', 'द', 'ग्रीक', 'थ', 'िएटर']\n",
            "Skipping: ['HISTORICAL', 'AND', 'FOLKLORE', 'MUSEUM', 'OF', 'CORINTH'] - ['ह', 'िस्टॉर', 'िकल', 'एण्ड', 'फोकलोर', 'म्युज़ियम', 'ऑफ', 'कॉर', 'िन्थ']\n",
            "Skipping: ['RAMCOIND'] - ['राम्को', 'इंड']\n",
            "Skipping: ['NAWAB', 'SIRAZUDDAULA'] - ['नवाब', 'सिराज़ुद्', 'दौला']\n",
            "Skipping: ['PAUL', 'VRELLIS', 'GREEK', 'HISTORY', 'MUSEUM'] - ['पॉल', 'व्रेलिस', 'ग्रीक', 'ह', 'िस्ट्री', 'म्युज़ियम']\n",
            "Skipping: ['MANEE', 'DR'] - ['मणींद्र']\n",
            "Skipping: ['BILLIARDS'] - ['बिलियर्ड्', 'स']\n",
            "Skipping: ['PETROBR', 'S'] - ['पेट्रोब्रस']\n",
            "Skipping: ['DHARITRI'] - ['धर', 'ित्री']\n",
            "Skipping: ['VISHVEN', 'DR'] - ['विश्वेन्द्र']\n",
            "Skipping: ['NANJING', 'MUSEUM'] - ['नंज', 'िंग', 'म्युज़ियम']\n",
            "Skipping: ['MUSEUM', 'OF', 'DIONYSIOS', 'SOLOMOS', 'AND', 'EMINENT', 'ZAKYNTHIANS'] - ['म्युज़ियम', 'ऑफ', 'डीयॉन', 'िस', 'ियॉस', 'सोलोमॉस', 'एण्ड', 'एम', 'िनेंट', 'झॅक', 'िन्थ', 'ियन्स']\n",
            "Skipping: ['NAHANNI'] - ['ना', 'हान्नी']\n",
            "Skipping: ['AUSTRALIAN', 'NATIONAL', 'UNIVERSITY'] - ['ऑस्ट्रेलियननेशनल', 'यूनिवर्सिटी']\n",
            "Skipping: ['GOKHALE', 'MEMORIAL', 'GIRLS', 'SCHOOL', 'CALCUTTA'] - ['गोखले', 'मेमोर', 'ियल', 'गर्ल्स', 'स्कूल', 'कैलकटा']\n",
            "Skipping: ['JAHAN', 'AARA'] - ['जहाँआरा']\n",
            "Skipping: ['NAVABHARAT', 'FERRO', 'ALLOYS'] - ['नव', 'भारत', 'फ़ैरो', 'अलॉय']\n",
            "Skipping: ['UNIVERSITY', 'OF', 'LEEDS'] - ['यूनिवर्सिटी', 'ऑफ', 'लीड्', 'स']\n",
            "Skipping: ['ADBUL', 'QAWI'] - ['अद्', 'बुल', 'कावी']\n",
            "Skipping: ['GONCI', 'RE', 'EURIS'] - ['गॉन्सियर', 'यूरिस']\n",
            "Skipping: ['JEEVAN', 'MRITYU'] - ['जीवन', 'मृ', 'त्यु']\n",
            "Skipping: ['DIVYA', 'BHASKAR'] - ['द', 'िव्य', 'भास्कर']\n",
            "Skipping: ['RAMA', 'LINGESHWARA'] - ['रामालिंगेश्वर']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'RHODES'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'रोड्स']\n",
            "Skipping: ['FAKHRUN', 'NISA'] - ['फखरुन्निसा']\n",
            "Skipping: ['DEVEN', 'DR'] - ['देवेन्द्र']\n",
            "Skipping: ['FORT', 'LAFAYETTE'] - ['फोर्ट', 'लैफायेट्', 'ट']\n",
            "Skipping: ['MAJOR', 'LEAGUE', 'LACROSSE', 'SPORTSMAN', 'OF', 'THE', 'YEAR', 'AWARD'] - ['मेजर', 'लीग', 'लैक्रोस', 'स्पोर्ट्', 'समैन', 'ऑफ', 'द', 'ईयर', 'अवार्ड']\n",
            "Skipping: ['DAINIK', 'BHASKAR'] - ['दैन', 'िक', 'भास्कर']\n",
            "Skipping: ['FOUR', 'CONTINENTS', 'FIGURE', 'SKATING', 'CHAMPIONS'] - ['फोर', 'कॉनटिनेंट्', 'स', 'फिगर', 'स्केटिंग', 'चैंपियन्स']\n",
            "Skipping: ['GALLANTS', 'BOWER'] - ['गैलंट्', 'स', 'बॉवर']\n",
            "Skipping: ['GUINESS', 'WORLD', 'OF', 'RECORD', 'MUSEUM'] - ['ग', 'िनीज', 'वर्ल्ड', 'ऑफ', 'रेकॉर्ड्स', 'म्युज़ियम']\n",
            "Skipping: ['REDIFF', 'COM', 'INDIA', 'LIMITED'] - ['रेडिफ़', 'डॉट', 'कॉम', 'इंडिया', 'लिमिटेड']\n",
            "Skipping: ['ORDER', 'OF', 'THE', 'NATIONAL', 'COAT', 'OF', 'ARMS'] - ['ऑडर', 'ऑफ', 'द', 'नेशनल', 'कोट', 'ऑफ', 'आर्म्', 'स']\n",
            "Skipping: ['NATIONAL', 'MUSEUM', 'OF', 'AMERICAN', 'INDIAN'] - ['नेशनल', 'म्युज़ियम', 'ऑफ', 'अमेर', 'िकन', 'इंड', 'ियन']\n",
            "Skipping: ['THE', 'HUMBER', 'FORTS'] - ['द', 'हंबर', 'फोर्ट्', 'स']\n",
            "Skipping: ['OMKARNATH', 'THAKUR'] - ['ओंकार', 'नाथ', 'ठाकुर']\n",
            "Skipping: ['NAOMI', 'AWARDS'] - ['नाओमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['AMERICAN', 'NUMISMATIC', 'SOCIETY', 'MUSEUM'] - ['अमेर', 'िकन', 'न्यूम', 'िस्मेट', 'िक', 'सोसायटी', 'म्युज़ियम']\n",
            "Skipping: ['RASTRIYA', 'SAHARA'] - ['राष्', 'ट्रीय', 'सहारा']\n",
            "Skipping: ['GARY', 'ROBERTSON'] - ['गेरी', 'रॉबर्ट्', 'सन']\n",
            "Skipping: ['ANDREW', 'SYMONDS'] - ['एण्ड्रयू', 'सिमंड्', 'स']\n",
            "Skipping: ['NEW', 'YORK', 'HISTORICAL', 'SOCIETY'] - ['न्यू', 'यॉर्क', 'ह', 'िस्टॉर', 'िकल', 'सोसायटी']\n",
            "Skipping: ['IDJWI'] - ['इड्', 'ज्वी']\n",
            "Skipping: ['OPENTV'] - ['ओपन', 'टीवी']\n",
            "Skipping: ['BRITISH', 'COMEDY', 'AWARDS'] - ['ब्रिटिश', 'कॉमेडी', 'अवार्ड्', 'स']\n",
            "Skipping: ['SHAILEN', 'DR'] - ['शैलेन्द्र']\n",
            "Skipping: ['FORT', 'RONDUIT'] - ['फोर्ट', 'रॉनड्', 'यूट']\n",
            "Skipping: ['GERMAN', 'HISTORICAL', 'MUSEUM'] - ['जर्मन', 'ह', 'िस्टॉर', 'िकल', 'म्युज़ियम']\n",
            "Skipping: ['STUDIO', 'MUSEUM', 'IN', 'HARLEN'] - ['स्टुड', 'ियो', 'म्युज़ियम', 'इन', 'हार्लेन']\n",
            "Skipping: ['UN', 'MANAA'] - ['उन्मना']\n",
            "Skipping: ['ENVOY', 'COMMUNICATIONS', 'GROUP'] - ['एन्वॉय', 'कम्युनिकेशंस']\n",
            "Skipping: ['ARISTOTLE', 'ONASSIS'] - ['एरीस्टोटल', 'ओनासि', 'स']\n",
            "Skipping: ['DAINIK', 'AGRADOOT'] - ['दैन', 'िक', 'अग्रदूत']\n",
            "Skipping: ['WAR', 'OF', 'THE', 'HOLY', 'LEAGUE'] - ['वार', 'ऑफ', 'होली', 'लीग']\n",
            "Skipping: ['PHIL', 'EDMONDS'] - ['फिल', 'एडमंड्', 'स']\n",
            "Skipping: ['UNITED', 'STATES', 'MILITARY', 'ACADEMY'] - ['यूनाइटेड', 'स्टेट्', 'स', 'मिलिट्री', 'अकेडमी']\n",
            "Skipping: ['HINDI', 'MILAP'] - ['ह', 'िन्दी', 'म', 'िलाप']\n",
            "Skipping: ['THE', 'GOULANDRIS', 'MUSEUM', 'OF', 'NATURAL', 'HISTORY'] - ['द', 'गॉलैंड्रीस', 'म्युज़ियम', 'ऑफ', 'नैचरल', 'ह', 'िस्टरी']\n",
            "Skipping: ['PRICKETTS', 'FORT'] - ['प्रिकेट्', 'स', 'फोर्ट']\n",
            "Skipping: ['ROBERT', 'BRIDGES'] - ['रॉबर्ट', 'ब्रिजि', 'ज']\n",
            "Skipping: ['MUSEUM', 'OF', 'ENGRAVINGS', 'AND', 'GRAPHIC', 'ARTS'] - ['म्युज़ियम', 'ऑफ', 'एन्ग्रेव', 'िंग्स', 'एण्ड', 'ग्राफ', 'िक', 'आर्ट्स']\n",
            "Skipping: ['WHITNEY', 'MUSEUM', 'OF', 'AMERICAN', 'ART'] - ['व्ह', 'िटनी', 'म्युज़ियम', 'ऑफ', 'अमेर', 'िकन', 'आर्ट']\n",
            "Skipping: ['SAPTAHIK', 'HINDUSTAN'] - ['साप्ताह', 'िक', 'ह', 'िन्दुस्तान']\n",
            "Skipping: ['VIJAYA', 'KARNATAKA'] - ['व', 'िजय', 'कर्नाटका']\n",
            "Skipping: ['CORNISH', 'COLONY', 'MUSEUM'] - ['कॉर्न', 'िश', 'कॉलोनी', 'म्युज़ियम']\n",
            "Skipping: ['MUSEUM', 'OF', 'ISLAMIC', 'CERAMICS'] - ['म्युज़ियम', 'ऑफ', 'इस्लाम', 'िक', 'स', 'िरेम', 'िक्स']\n",
            "Skipping: ['SUREN', 'DAR'] - ['सुरेन्दर']\n",
            "Skipping: ['SAINT', 'JOHNS', 'HIGH', 'SCHOOL', 'CHANDIGARH'] - ['सेंट', 'जॉन्', 'स', 'हाई', 'स्कूल', 'चंडीगढ़']\n",
            "Skipping: ['DENIRO'] - ['डी', 'निरो']\n",
            "Skipping: ['LOKAMAAN', 'Y'] - ['लोकमान्य']\n",
            "Skipping: ['VAPARAISO', 'CHURCH', 'OF', 'CHRIST'] - ['व्हापरासिओ']\n",
            "Skipping: ['AJI', 'ASSAMESE', 'DAILY'] - ['अज', 'ि', 'असमीज', 'डेल्ही']\n",
            "Skipping: ['PHILATELIC', 'MUSEUM'] - ['फ', 'िलेटेलिक', 'म्युज़ियम']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'PIRAEUS'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'पायरस']\n",
            "Skipping: ['SAUJAN', 'Y'] - ['सौजन्य']\n",
            "Skipping: ['PARIS', 'CHARLES', 'DE', 'GAULLE'] - ['पेरिस', 'रॉसे', 'चार्ल्स', 'डे', 'ग्यूले']\n",
            "Skipping: ['PARKWAY', 'APOSTOLIC'] - ['पार्क', 'वे', 'अपोस्टोलिक']\n",
            "Skipping: ['PRATIDIN'] - ['प्रत', 'िद', 'िन']\n",
            "Skipping: ['RADIO', 'ACADEMY', 'AWARDS'] - ['रेडियो', 'अकेडमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['KAPEE', 'DR'] - ['कपींद्र']\n",
            "Skipping: ['NETHERLANDS'] - ['नीदरलैंड्', 'स']\n",
            "Skipping: ['CHAITANYA', 'ENGLISH', 'MEDIUM', 'HIGH', 'SCHOOL', 'TIRUPATHI'] - ['चैतन्य', 'इंग्ल', 'िश', 'म', 'ीड', 'ियम', 'हाई', 'स्कलू', 'त', 'िरुपती']\n",
            "Skipping: ['SORRENTO'] - ['सॉरेन्टो', '1']\n",
            "Skipping: ['PRIX', 'DES', 'DEUX', 'MAGOTS'] - ['प्रिक्स', 'डेस', 'ड्', 'यूक्स', 'मैगट्', 'स']\n",
            "Skipping: ['MAUNA', 'LOA'] - ['मौनालोआ']\n",
            "Skipping: ['HANSRAJ', 'MORARJI', 'PUBLIC', 'SCHOOL'] - ['हंसराज', 'मोरारजी', 'पब्ल', 'िक', 'स्कूल']\n",
            "Skipping: ['BUDNASEEB'] - ['बद', 'नसीब']\n",
            "Skipping: ['SQUIDDY', 'AWARDS'] - ['स्क्विडी', 'अवार्ड्', 'स']\n",
            "Skipping: ['MASS', 'MUTUAL', 'LIFE'] - ['मास', 'म्युच्युअल', 'लाइफ़', 'इंश्योरेंस']\n",
            "Skipping: ['STATS', 'CHIPPAC'] - ['स्टेट्सचिपपैक']\n",
            "Skipping: ['KAREN', 'BLIXEN', 'MUSEUM'] - ['कैरेन', 'ब्ल', 'िक्सन', 'म्युज़ियम']\n",
            "Skipping: ['YESHIVA', 'UNIVERSITY', 'MUSEUM'] - ['येश', 'िवा', 'युन', 'िवर्स', 'िटी', 'म्युज़ियम']\n",
            "Skipping: ['KAVIGNAR', 'INKULAB'] - ['कव', 'िग्नर', 'इंकलाब']\n",
            "Skipping: ['DRAFTS'] - ['ड्राफ्ट्', 'स']\n",
            "Skipping: ['SATISH', 'GUJRAL'] - ['स', 'तीश', 'गुजराल']\n",
            "Skipping: ['BHATTU'] - ['भट्', 'टु']\n",
            "Skipping: ['DAVIS', 'MUSEUM', 'AND', 'CULTURAL', 'ART'] - ['डेव', 'िस', 'म्युज़ियम', 'एण्ड', 'कल्चरल', 'आर्ट']\n",
            "Skipping: ['AMERICAN', 'ACADEMY', 'OF', 'ARTS', 'AND', 'LETTERS', 'FICTION', 'AWARD'] - ['अमेरिकन', 'अकेडमी', 'ऑफ', 'आर्ट्', 'स', 'एण्ड', 'लेटर्स', 'फिक्शन', 'अवार्ड']\n",
            "Skipping: ['NEWFOUNDLAND'] - ['न्यू', 'फाउंडलैंड']\n",
            "Skipping: ['FLORINA', 'MUSEUM', 'OF', 'MODERN', 'ART'] - ['फ्लोर', 'िना', 'म्युज़ियम', 'ऑफ', 'मॉडर्न', 'आर्ट']\n",
            "Skipping: ['LONDONHEATHROW'] - ['लंदन', 'हीथ्रो']\n",
            "Skipping: ['RYAN', 'HINDS'] - ['रियान', 'हिन्ड्', 'स']\n",
            "Skipping: ['MUSEUM', 'OF', 'THE', 'CITY', 'OF', 'NEW', 'YORK'] - ['म्युज़ियम', 'ऑफ', 'द', 'स', 'िट', 'ी', 'ऑफ', 'न्यू', 'यॉर्क']\n",
            "Skipping: ['GAJEN', 'DR'] - ['गजेन्द्र']\n",
            "Skipping: ['NAREN', 'DAR'] - ['नरेन्दर']\n",
            "Skipping: ['KOSTAS', 'FRONTZOS', 'MUSEUM', 'OF', 'EPIRUS', 'FOLK', 'ART'] - ['कोस्टस', 'फ्रंटज़ॉस', 'म्युज़ियम', 'ऑफ', 'एप', 'िरस', 'फोक', 'आर्ट']\n",
            "Skipping: ['THE', 'INTERNATIONAL', 'ANDY', 'AWARDS'] - ['द', 'इंटरनेशनल', 'एण्डी', 'अवार्ड्', 'स']\n",
            "Skipping: ['DELHI', 'PUBLIC', 'SCHOOL', 'INDIRAPURAM'] - ['डेल्ही', 'पब्ल', 'िक', 'स्कूल', 'इंदिरापुरम']\n",
            "Skipping: ['HUKUMACHAN', 'D'] - ['हुकुमचन्द']\n",
            "Skipping: ['ADWAAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['DAYTON', 'ART', 'INSTITUTE'] - ['डेटन', 'आर्ट', 'इंस्टीट्', 'यूट']\n",
            "Skipping: ['NATIONAL', 'TELEVISION', 'AWARDS'] - ['नेशनल', 'टेलीविज़न', 'अवार्ड्', 'स']\n",
            "Skipping: ['VANAKAN', 'YAA'] - ['वनकन्या']\n",
            "Skipping: ['COBH', 'HERITAGE', 'CENTER'] - ['कोब', 'हेर', 'िटेज', 'सेंटर']\n",
            "Skipping: ['RETALIX'] - ['रेटालिक्स', 'लि']\n",
            "Skipping: ['CARMEL', 'CONVENT', 'SCHOOL', 'CHANDIGARH'] - ['कारमेल', 'कॉन्', 'वेंट', 'स्कूल', 'चंडीगढ़']\n",
            "Skipping: ['MUSEUM', 'OF', 'WORKS', 'BY', 'THEOPHILOS'] - ['म्युज़ियम', 'ऑफ', 'वर्क्स', 'बाय', 'थ', 'ियोफ', 'िलस']\n",
            "Skipping: ['CHENGALPATTU'] - ['चेंगलपट्', 'टु']\n",
            "Skipping: ['VIV', 'RICHARDS'] - ['विव', 'रिचर्ड्', 'स']\n",
            "Skipping: ['LAUREUS', 'WORLD', 'SPORTS', 'AWARDS'] - ['लॉरेस', 'वर्ल्ड', 'स्पोर्ट्', 'स', 'अवार्ड्', 'स']\n",
            "Skipping: ['WILSON', 'CASTLE'] - ['व', 'िल्सन', 'कॅसल']\n",
            "Skipping: ['STATE', 'HERALDIC', 'MUSEUM'] - ['स्टेट', 'हेराल्ड', 'िक', 'म्युज़ियम']\n",
            "Skipping: ['NYIKA', 'PLATEAU'] - ['न्याइका', 'प्लेट्', 'यू']\n",
            "Skipping: ['LAGARD', 'RE', 'GROUPE'] - ['लैगार्डेयर', 'ग्रुप']\n",
            "Skipping: ['PADMA', 'BHUSHAN'] - ['पद्', 'म', 'भूषण']\n",
            "Skipping: ['ROYAL', 'ONTARIO', 'MUSEUM'] - ['रॉयल', 'ऑन्टार', 'ियो', 'म्युज़ियम']\n",
            "Skipping: ['GOVERNMENT', 'MODEL', 'HIGHER', 'SECONDARY', 'SCHOOL', 'THIRUVANANTHAPURAM'] - ['गवर्नमेंट', 'मॉडेल', 'हायर', 'सेकंडरी', 'स्कूल', 'थ', 'िरुवनंतपुरम']\n",
            "Skipping: ['SRISAILAM'] - ['श्री', 'शैलम']\n",
            "Skipping: ['SPATHAREION', 'MUSEUM', 'OF', 'THE', 'SHADOW', 'THEATRE'] - ['स्पाथेर', 'ियन', 'म्युज़ियम', 'ऑफ', 'द', 'शैडो', 'थ', 'िएटर']\n",
            "Skipping: ['DAINIK', 'JANASADHARAN'] - ['दैन', 'िक', 'जनसाधारण']\n",
            "Skipping: ['COLVIN', 'TALUQADARS', 'COLLEGE', 'LUCKNOW'] - ['कॉल्व', 'िन', 'तालुकदार्स', 'कॉलेज', 'लखनऊ']\n",
            "Skipping: ['SADGUNA'] - ['सद्', 'गुण']\n",
            "Skipping: ['SUMMIT', 'AWARDS'] - ['सुमित', 'अवार्ड्', 'स']\n",
            "Skipping: ['THE', 'HINDU'] - ['द', 'ह', 'िन्दू']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'CORFU'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'कोरफू']\n",
            "Skipping: ['PORTSMOUTH'] - ['पोर्ट्', 'समाउथ']\n",
            "Skipping: ['FORT', 'STEURGAT'] - ['फोर्ट', 'स्ट्', 'यूर्गेट']\n",
            "Skipping: ['ADWAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['KAN', 'HAIYAALAAL'] - ['कन्हैयालाल']\n",
            "Skipping: ['KARA', 'KUM'] - ['काराकुम']\n",
            "Skipping: ['CHASHME', 'BADDOOR'] - ['चश्मे', 'बद्', 'दूर']\n",
            "Skipping: ['RAILWAY', 'MUSEUM', 'OF', 'THE', 'MUNICIPALITY', 'OF', 'KALAMATA'] - ['रेल्वे', 'म्युज़ियम', 'ऑफ', 'द', 'म्यून', 'िस', 'िपाल्टी', 'ऑफ', 'कलमाटा']\n",
            "Skipping: ['ADVAAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['MUSEUM', 'OF', 'ROMANI', 'CULTURE'] - ['म्युज़ियम', 'ऑफ', 'रोमान', 'ी', 'कल्चर']\n",
            "Skipping: ['SHAKE', 'HANDS'] - ['शेक', 'हैंड्', 'स']\n",
            "Skipping: ['WIND', 'RIVER'] - ['विंडरिवर']\n",
            "Skipping: ['HAMILTON', 'MASAKADZA'] - ['हैमिल्टन', 'मसाकद्', 'ज़ा']\n",
            "Skipping: ['CHARLES', 'ROBERTS', 'AWARD'] - ['चार्ल्स', 'रॉबर्ट्', 'स', 'अवार्ड']\n",
            "Skipping: ['PATHANAMTHITTA'] - ['पथानामथीट्', 'टा']\n",
            "Skipping: ['NETAJI', 'SUBHASH', 'CHANDRA', 'BOSE'] - ['नेताजी', 'सुभाषचंद्र', 'बोस']\n",
            "Skipping: ['ROCKBROOK', 'UNITED'] - ['रॉकब्रुक', 'यूनाइटेड', 'मेथोडिस्ट']\n",
            "Skipping: ['MUQADDAS'] - ['मुक़द्', 'दस']\n",
            "Skipping: ['WALTER', 'SCOTT'] - ['वॉल्टरस्कॉट']\n",
            "Skipping: ['COLOURPLUS', 'FASHIONS'] - ['कलर', 'प्लस', 'फ़ैशन्स']\n",
            "Skipping: ['ATTUR'] - ['अट्', 'टूर']\n",
            "Skipping: ['GEOLOGICAL', 'MUSEUM', 'OF', 'CHINA'] - ['ज', 'िओलॉजकिल', 'म्युज़ियम', 'ऑफ', 'चाइना']\n",
            "Skipping: ['BAL', 'KRISHNA'] - ['बालकृष्णा']\n",
            "Skipping: ['THE', 'MORNING', 'QUICK'] - ['द', 'मॉर्निंग', 'क्व', 'िक']\n",
            "Skipping: ['SHAILE', 'DR'] - ['शैलेंद्र']\n",
            "Skipping: ['RAAJEN', 'DR'] - ['राजेन्द्र']\n",
            "Skipping: ['PORTSDOWN', 'HILL'] - ['पोर्ट्', 'सडाउन', 'हिल']\n",
            "Skipping: ['WEST', 'SPITSBERGEN'] - ['वेस्ट', 'स्पीट्', 'सबर्गन']\n",
            "Skipping: ['STIRLING', 'SMITH', 'MUSEUM', 'AND', 'ART', 'GALLERY'] - ['स्टर्ल', 'िंग', 'स्म', 'िथ', 'म्युज़ियम', 'एण्ड', 'आर्ट', 'गैलरी']\n",
            "Skipping: ['ARMY', 'PUBLIC', 'SCHOOL', 'NEW', 'DELHI'] - ['आर्मी', 'पब्', 'लिक', 'स्', 'कूल', 'न्यू', 'डेल्ही']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMDw8DgVc-hg",
        "outputId": "67b26f7d-a567-4f17-fb15-7e7a1071ca45"
      },
      "source": [
        "data_tester = EncoderDecoderData('testing.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping: ['W', 'TTEMBERG'] - ['यूटमबर्ग']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b259bd67330e43fcb87421582559b4e2",
            "f25fd0ffa23947528f8f624936182e30",
            "b55b633bd4314fe69a6da6a9b7b6327f",
            "f6b2dcbf188341b5a6401f50a8af3d82",
            "23b23ad8c6c64c558a73a860fa7663c0",
            "db7a5ce286184825820fe55502efc5d8",
            "32cd702b7f624c51b2cf883578b27467",
            "c02f9decfd7e48d89387595fe0e31e71"
          ]
        },
        "id": "4lGDs08Jc_sA",
        "outputId": "d51e161d-d3c9-4627-c9f6-71610b13e58d"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "for i in tqdm_notebook(range(epochs)):\n",
        "  print('Training Loss:',train(model,data_store,loss_func,opt,20))\n",
        "  print('validation Loss:',evaluate(model,data_store,loss_func))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b259bd67330e43fcb87421582559b4e2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 4.867519044876099\n",
            "validation Loss: 4.817363411188126\n",
            "Training Loss: 4.80725462436676\n",
            "validation Loss: 4.745693862438202\n",
            "Training Loss: 4.7375040531158445\n",
            "validation Loss: 4.6576035767793655\n",
            "Training Loss: 4.6462595701217655\n",
            "validation Loss: 4.498365253210068\n",
            "Training Loss: 4.441825222969055\n",
            "validation Loss: 4.193519853055477\n",
            "Training Loss: 4.060336399078369\n",
            "validation Loss: 3.8106859177351\n",
            "Training Loss: 3.8439319133758545\n",
            "validation Loss: 3.718235857784748\n",
            "Training Loss: 3.7936527729034424\n",
            "validation Loss: 3.9040592685341835\n",
            "Training Loss: 3.633707642555237\n",
            "validation Loss: 3.4685273990035057\n",
            "Training Loss: 3.62103590965271\n",
            "validation Loss: 3.591299779713154\n",
            "Training Loss: 3.596520471572876\n",
            "validation Loss: 3.449893295764923\n",
            "Training Loss: 3.580352222919464\n",
            "validation Loss: 3.607529856264591\n",
            "Training Loss: 3.571495199203491\n",
            "validation Loss: 3.458219677209854\n",
            "Training Loss: 3.6018193125724793\n",
            "validation Loss: 3.7150397300720215\n",
            "Training Loss: 3.473277199268341\n",
            "validation Loss: 3.386507861316204\n",
            "Training Loss: 3.472463047504425\n",
            "validation Loss: 3.5614813193678856\n",
            "Training Loss: 3.4176332712173463\n",
            "validation Loss: 3.287253074347973\n",
            "Training Loss: 3.4103801012039185\n",
            "validation Loss: 3.3693887069821358\n",
            "Training Loss: 3.4293496012687683\n",
            "validation Loss: 3.3979932069778442\n",
            "Training Loss: 3.3791065216064453\n",
            "validation Loss: 3.316016599535942\n",
            "Training Loss: 3.397525668144226\n",
            "validation Loss: 3.5074002519249916\n",
            "Training Loss: 3.325480544567108\n",
            "validation Loss: 3.399383507668972\n",
            "Training Loss: 3.365105855464935\n",
            "validation Loss: 3.3963573202490807\n",
            "Training Loss: 3.3537644505500794\n",
            "validation Loss: 3.469706065952778\n",
            "Training Loss: 3.309785258769989\n",
            "validation Loss: 3.484811097383499\n",
            "Training Loss: 3.2979041934013367\n",
            "validation Loss: 3.3348835483193398\n",
            "Training Loss: 3.2855018973350525\n",
            "validation Loss: 3.1928415298461914\n",
            "Training Loss: 3.3218152642250063\n",
            "validation Loss: 3.336435854434967\n",
            "Training Loss: 3.303052508831024\n",
            "validation Loss: 3.293424427509308\n",
            "Training Loss: 3.259213316440582\n",
            "validation Loss: 3.32704209536314\n",
            "Training Loss: 3.220958173274994\n",
            "validation Loss: 3.29044459015131\n",
            "Training Loss: 3.2259106278419494\n",
            "validation Loss: 3.2634280622005463\n",
            "Training Loss: 3.2932313442230225\n",
            "validation Loss: 3.232436642050743\n",
            "Training Loss: 3.2911441802978514\n",
            "validation Loss: 3.308542527258396\n",
            "Training Loss: 3.302607560157776\n",
            "validation Loss: 3.3653258681297302\n",
            "Training Loss: 3.243564450740814\n",
            "validation Loss: 3.178849656134844\n",
            "Training Loss: 3.2668070554733277\n",
            "validation Loss: 3.1969451159238815\n",
            "Training Loss: 3.244948613643646\n",
            "validation Loss: 3.3492105826735497\n",
            "Training Loss: 3.162219226360321\n",
            "validation Loss: 3.37690619379282\n",
            "Training Loss: 3.221141684055328\n",
            "validation Loss: 3.3201036602258682\n",
            "Training Loss: 3.289680016040802\n",
            "validation Loss: 3.241241082549095\n",
            "Training Loss: 3.184774649143219\n",
            "validation Loss: 3.3816525638103485\n",
            "Training Loss: 3.1484174728393555\n",
            "validation Loss: 3.342209167778492\n",
            "Training Loss: 3.2021621584892275\n",
            "validation Loss: 3.3649494498968124\n",
            "Training Loss: 3.198642349243164\n",
            "validation Loss: 3.3108857199549675\n",
            "Training Loss: 3.185715889930725\n",
            "validation Loss: 3.2678197622299194\n",
            "Training Loss: 3.16513409614563\n",
            "validation Loss: 3.398336671292782\n",
            "Training Loss: 3.084396982192993\n",
            "validation Loss: 3.2675109580159187\n",
            "Training Loss: 3.203082251548767\n",
            "validation Loss: 3.2018549144268036\n",
            "Training Loss: 3.251899778842926\n",
            "validation Loss: 3.1557934805750847\n",
            "Training Loss: 3.105971610546112\n",
            "validation Loss: 3.1558741629123688\n",
            "Training Loss: 3.223919355869293\n",
            "validation Loss: 3.1891040802001953\n",
            "Training Loss: 3.041901004314423\n",
            "validation Loss: 3.3728181198239326\n",
            "Training Loss: 3.1038313627243044\n",
            "validation Loss: 3.2144435942173004\n",
            "Training Loss: 3.1978858351707458\n",
            "validation Loss: 3.3733188435435295\n",
            "Training Loss: 3.157791864871979\n",
            "validation Loss: 3.2452134266495705\n",
            "Training Loss: 3.177377533912659\n",
            "validation Loss: 3.3685706555843353\n",
            "Training Loss: 3.1912093997001647\n",
            "validation Loss: 3.3201208785176277\n",
            "Training Loss: 3.0527130007743835\n",
            "validation Loss: 3.314801700413227\n",
            "Training Loss: 3.1122183918952944\n",
            "validation Loss: 3.1858887299895287\n",
            "Training Loss: 3.109749162197113\n",
            "validation Loss: 3.2244355380535126\n",
            "Training Loss: 3.020568871498108\n",
            "validation Loss: 3.175584226846695\n",
            "Training Loss: 3.1175785660743713\n",
            "validation Loss: 3.3710450753569603\n",
            "Training Loss: 3.1631444931030273\n",
            "validation Loss: 3.166355699300766\n",
            "Training Loss: 2.981727385520935\n",
            "validation Loss: 3.2241579480469227\n",
            "Training Loss: 3.0355343222618103\n",
            "validation Loss: 3.1443083360791206\n",
            "Training Loss: 3.033354127407074\n",
            "validation Loss: 3.0797252133488655\n",
            "Training Loss: 2.9886464834213258\n",
            "validation Loss: 3.1294394209980965\n",
            "Training Loss: 2.976009285449982\n",
            "validation Loss: 3.3498719856142998\n",
            "Training Loss: 3.110897660255432\n",
            "validation Loss: 3.0431871339678764\n",
            "Training Loss: 3.0965980052948\n",
            "validation Loss: 3.1298221573233604\n",
            "Training Loss: 3.0637207448482515\n",
            "validation Loss: 3.005928687751293\n",
            "Training Loss: 2.892155021429062\n",
            "validation Loss: 3.0063228607177734\n",
            "Training Loss: 2.9047332048416137\n",
            "validation Loss: 3.087807819247246\n",
            "Training Loss: 2.8582026302814483\n",
            "validation Loss: 3.0693524442613125\n",
            "Training Loss: 2.9480955064296723\n",
            "validation Loss: 3.1170936971902847\n",
            "Training Loss: 2.9305759012699126\n",
            "validation Loss: 3.1161083467304707\n",
            "Training Loss: 2.8863030850887297\n",
            "validation Loss: 3.0395107716321945\n",
            "Training Loss: 2.875949835777283\n",
            "validation Loss: 3.121004641056061\n",
            "Training Loss: 2.8693272352218626\n",
            "validation Loss: 3.074651412665844\n",
            "Training Loss: 2.9552582800388336\n",
            "validation Loss: 2.957228794693947\n",
            "Training Loss: 2.909145599603653\n",
            "validation Loss: 2.856123674660921\n",
            "Training Loss: 2.934075081348419\n",
            "validation Loss: 2.8680862560868263\n",
            "Training Loss: 2.850408285856247\n",
            "validation Loss: 2.805250473320484\n",
            "Training Loss: 2.8756579995155334\n",
            "validation Loss: 2.955732874572277\n",
            "Training Loss: 2.8783920764923097\n",
            "validation Loss: 2.913248471915722\n",
            "Training Loss: 2.784126365184784\n",
            "validation Loss: 2.970811113715172\n",
            "Training Loss: 2.856808769702911\n",
            "validation Loss: 2.7634282195940614\n",
            "Training Loss: 2.864574611186981\n",
            "validation Loss: 2.913061410188675\n",
            "Training Loss: 2.796070671081543\n",
            "validation Loss: 3.2311787456274033\n",
            "Training Loss: 2.9181800484657288\n",
            "validation Loss: 2.985154442489147\n",
            "Training Loss: 2.849115240573883\n",
            "validation Loss: 2.868775526061654\n",
            "Training Loss: 2.769858855009079\n",
            "validation Loss: 2.96019272133708\n",
            "Training Loss: 2.6858872175216675\n",
            "validation Loss: 3.0129067562520504\n",
            "Training Loss: 2.6924136340618134\n",
            "validation Loss: 2.911511816084385\n",
            "Training Loss: 2.7466925740242005\n",
            "validation Loss: 2.891113292425871\n",
            "Training Loss: 2.8152366399765016\n",
            "validation Loss: 2.920440535992384\n",
            "Training Loss: 2.6364765048027037\n",
            "validation Loss: 2.8744691982865334\n",
            "Training Loss: 2.729189854860306\n",
            "validation Loss: 3.183134902268648\n",
            "Training Loss: 2.5625135362148286\n",
            "validation Loss: 2.692153532989323\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88a42681d96b44908686caf395bf4129",
            "0fb3755ca9cf451fa866ee826170fe1a",
            "a7c0376409b84b4c8c5c3e77886b4a22",
            "e6e6fdb0b52541bab8ecf52f2d79d523",
            "ec453478080c4e27970ea579100b7ac2",
            "254d78e0d0d3477cb83ff54052e71121",
            "c8e716b63f17421db7d22cb2729d761d",
            "4ed947ccd2b34c2393422e0b221fa275"
          ]
        },
        "id": "Ok4FKf5sqE9E",
        "outputId": "7b2979bd-9687-4461-9b71-6d941c33e113"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "for i in tqdm_notebook(range(epochs)):\n",
        "  print('Training Loss:',train(model,data_store,loss_func,opt,20))\n",
        "  print('validation Loss:',evaluate(model,data_store,loss_func))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a42681d96b44908686caf395bf4129",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 2.706491819024086\n",
            "validation Loss: 2.9528014212846756\n",
            "Training Loss: 2.653440684080124\n",
            "validation Loss: 2.6931162290275097\n",
            "Training Loss: 2.77426078915596\n",
            "validation Loss: 2.917041133157909\n",
            "Training Loss: 2.757438653707504\n",
            "validation Loss: 2.6366179417818785\n",
            "Training Loss: 2.5670855700969697\n",
            "validation Loss: 2.7171876579523087\n",
            "Training Loss: 2.601834911108017\n",
            "validation Loss: 2.8337081354111433\n",
            "Training Loss: 2.6417100012302397\n",
            "validation Loss: 2.859104059636593\n",
            "Training Loss: 2.6237579762935637\n",
            "validation Loss: 2.9127277731895447\n",
            "Training Loss: 2.6222614467144014\n",
            "validation Loss: 2.723024219274521\n",
            "Training Loss: 2.508723509311676\n",
            "validation Loss: 2.618336282670498\n",
            "Training Loss: 2.5753480434417724\n",
            "validation Loss: 2.6094206934794784\n",
            "Training Loss: 2.6305887520313265\n",
            "validation Loss: 2.6566354297101498\n",
            "Training Loss: 2.629073303937912\n",
            "validation Loss: 2.699669197201729\n",
            "Training Loss: 2.527462565898895\n",
            "validation Loss: 2.6516761500388384\n",
            "Training Loss: 2.5356066286563874\n",
            "validation Loss: 2.9705027267336845\n",
            "Training Loss: 2.5385329097509386\n",
            "validation Loss: 2.84349812194705\n",
            "Training Loss: 2.397837719321251\n",
            "validation Loss: 2.8211109787225723\n",
            "Training Loss: 2.4594472348690033\n",
            "validation Loss: 2.7947999872267246\n",
            "Training Loss: 2.378274345397949\n",
            "validation Loss: 2.669869512319565\n",
            "Training Loss: 2.32230721116066\n",
            "validation Loss: 2.66107789427042\n",
            "Training Loss: 2.6426981806755068\n",
            "validation Loss: 2.9069378934800625\n",
            "Training Loss: 2.540049135684967\n",
            "validation Loss: 2.6875992082059383\n",
            "Training Loss: 2.280901101231575\n",
            "validation Loss: 2.600029468536377\n",
            "Training Loss: 2.3573152303695677\n",
            "validation Loss: 2.519690241664648\n",
            "Training Loss: 2.3419261902570723\n",
            "validation Loss: 2.791353849694133\n",
            "Training Loss: 2.6085764110088348\n",
            "validation Loss: 2.667384594678879\n",
            "Training Loss: 2.3951346278190613\n",
            "validation Loss: 2.793066270649433\n",
            "Training Loss: 2.3559027343988417\n",
            "validation Loss: 2.6517892070114613\n",
            "Training Loss: 2.4882336795330047\n",
            "validation Loss: 2.682580128312111\n",
            "Training Loss: 2.305891692638397\n",
            "validation Loss: 2.8366495594382286\n",
            "Training Loss: 2.270737773180008\n",
            "validation Loss: 2.6730691231787205\n",
            "Training Loss: 2.2038894653320313\n",
            "validation Loss: 2.331763249821961\n",
            "Training Loss: 2.347119468450546\n",
            "validation Loss: 2.805491331964731\n",
            "Training Loss: 2.1543496549129486\n",
            "validation Loss: 2.6739739887416363\n",
            "Training Loss: 2.223077416419983\n",
            "validation Loss: 2.6049456279724836\n",
            "Training Loss: 2.4213114082813263\n",
            "validation Loss: 2.552558058872819\n",
            "Training Loss: 2.2885056734085083\n",
            "validation Loss: 2.5301561672240496\n",
            "Training Loss: 2.2511673152446745\n",
            "validation Loss: 2.6252202950417995\n",
            "Training Loss: 2.245358058810234\n",
            "validation Loss: 2.625521417707205\n",
            "Training Loss: 2.3753776431083677\n",
            "validation Loss: 2.7686303816735744\n",
            "Training Loss: 2.188423389196396\n",
            "validation Loss: 2.362149342894554\n",
            "Training Loss: 1.9101393878459931\n",
            "validation Loss: 2.5078642554581165\n",
            "Training Loss: 2.4338992655277254\n",
            "validation Loss: 2.4864949136972427\n",
            "Training Loss: 2.3330608904361725\n",
            "validation Loss: 2.4422111473977566\n",
            "Training Loss: 2.090571901202202\n",
            "validation Loss: 2.59742190875113\n",
            "Training Loss: 2.288750922679901\n",
            "validation Loss: 2.5742716901004314\n",
            "Training Loss: 2.1627216011285784\n",
            "validation Loss: 2.750231832265854\n",
            "Training Loss: 2.2662894666194915\n",
            "validation Loss: 2.4813834317028522\n",
            "Training Loss: 2.0768981724977493\n",
            "validation Loss: 2.4300162754952908\n",
            "Training Loss: 2.3578022599220274\n",
            "validation Loss: 2.1260857335291803\n",
            "Training Loss: 2.2569941699504854\n",
            "validation Loss: 2.2516041211783886\n",
            "Training Loss: 2.1033622145652773\n",
            "validation Loss: 2.0745314229279757\n",
            "Training Loss: 2.193646305799484\n",
            "validation Loss: 2.395619163289666\n",
            "Training Loss: 2.1942660212516785\n",
            "validation Loss: 2.492645964026451\n",
            "Training Loss: 2.088055655360222\n",
            "validation Loss: 2.537781272083521\n",
            "Training Loss: 2.1964196443557737\n",
            "validation Loss: 2.5357617866247892\n",
            "Training Loss: 2.06219282746315\n",
            "validation Loss: 2.5042863339185715\n",
            "Training Loss: 2.1801748275756836\n",
            "validation Loss: 2.360862698405981\n",
            "Training Loss: 2.301033502817154\n",
            "validation Loss: 2.4014869928359985\n",
            "Training Loss: 2.1436901897192002\n",
            "validation Loss: 2.1779352510347962\n",
            "Training Loss: 2.029336151480675\n",
            "validation Loss: 2.184988748282194\n",
            "Training Loss: 2.070391780138016\n",
            "validation Loss: 2.1760836094617844\n",
            "Training Loss: 2.173155876994133\n",
            "validation Loss: 2.330109942704439\n",
            "Training Loss: 2.009916627407074\n",
            "validation Loss: 2.0204433212056756\n",
            "Training Loss: 1.860495349764824\n",
            "validation Loss: 2.385785523802042\n",
            "Training Loss: 1.9965246587991714\n",
            "validation Loss: 2.338801760226488\n",
            "Training Loss: 1.8304174154996873\n",
            "validation Loss: 2.245085078291595\n",
            "Training Loss: 1.99746895134449\n",
            "validation Loss: 2.2675965446978807\n",
            "Training Loss: 1.9724413067102433\n",
            "validation Loss: 2.3287983536720276\n",
            "Training Loss: 1.9768311709165574\n",
            "validation Loss: 2.289585047867149\n",
            "Training Loss: 1.8975236356258391\n",
            "validation Loss: 2.1513301404193044\n",
            "Training Loss: 1.9755352467298508\n",
            "validation Loss: 2.2574046817608178\n",
            "Training Loss: 1.8315739899873733\n",
            "validation Loss: 2.3329363986849785\n",
            "Training Loss: 1.8513600647449493\n",
            "validation Loss: 2.3135608956217766\n",
            "Training Loss: 2.096558001637459\n",
            "validation Loss: 1.6678847651928663\n",
            "Training Loss: 1.8801564484834672\n",
            "validation Loss: 2.372734964825213\n",
            "Training Loss: 1.891633892059326\n",
            "validation Loss: 2.18112456984818\n",
            "Training Loss: 1.7100888460874557\n",
            "validation Loss: 2.256440443918109\n",
            "Training Loss: 2.045445519685745\n",
            "validation Loss: 2.4129099622368813\n",
            "Training Loss: 1.9646835207939148\n",
            "validation Loss: 2.3757424000650644\n",
            "Training Loss: 1.9485897541046142\n",
            "validation Loss: 2.31700271461159\n",
            "Training Loss: 1.9093167051672935\n",
            "validation Loss: 2.394652733579278\n",
            "Training Loss: 2.008108814060688\n",
            "validation Loss: 2.2866843324154615\n",
            "Training Loss: 1.754040938615799\n",
            "validation Loss: 2.2434969427995384\n",
            "Training Loss: 1.6403022766113282\n",
            "validation Loss: 2.4787692707031965\n",
            "Training Loss: 1.8506630420684815\n",
            "validation Loss: 2.119097277056426\n",
            "Training Loss: 1.6941878855228425\n",
            "validation Loss: 2.1868802923709154\n",
            "Training Loss: 1.8555204212665557\n",
            "validation Loss: 2.024079669266939\n",
            "Training Loss: 1.6649684712290764\n",
            "validation Loss: 2.308726880699396\n",
            "Training Loss: 1.5960066244006157\n",
            "validation Loss: 2.0798388114199042\n",
            "Training Loss: 1.8560537666082382\n",
            "validation Loss: 2.0944017246365547\n",
            "Training Loss: 1.6121460244059562\n",
            "validation Loss: 2.1354148872196674\n",
            "Training Loss: 1.4994727075099945\n",
            "validation Loss: 2.2505677975714207\n",
            "Training Loss: 1.9278434365987778\n",
            "validation Loss: 2.2844603713601828\n",
            "Training Loss: 1.6254219949245452\n",
            "validation Loss: 2.06472528539598\n",
            "Training Loss: 2.1116506695747375\n",
            "validation Loss: 2.336359940469265\n",
            "Training Loss: 1.8188373863697052\n",
            "validation Loss: 2.157940173521638\n",
            "Training Loss: 1.8031864136457443\n",
            "validation Loss: 2.315267238765955\n",
            "Training Loss: 1.7100725531578065\n",
            "validation Loss: 2.119202135130763\n",
            "Training Loss: 1.904422441124916\n",
            "validation Loss: 2.225031979382038\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}