{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder_Decoder_Batched.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0933764bcd744487b3451cba86f33410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f3a5bbed4c6b4d3c8474d88cd01ba5f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e6b002c4d314e6c844a4ffa7040cb1f",
              "IPY_MODEL_ae7dc475d1f844559b60ff3356a09080"
            ]
          }
        },
        "f3a5bbed4c6b4d3c8474d88cd01ba5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e6b002c4d314e6c844a4ffa7040cb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d9ae7f071b0e416daf63be58b71f596f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a218ac1d21b46bda83e0419deeaac22"
          }
        },
        "ae7dc475d1f844559b60ff3356a09080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_68ef22db0d0f4a1bb3b78747d8c14d87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [18:09&lt;00:00, 10.89s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e031ad32ec34146bc678710bccef97c"
          }
        },
        "d9ae7f071b0e416daf63be58b71f596f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a218ac1d21b46bda83e0419deeaac22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68ef22db0d0f4a1bb3b78747d8c14d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e031ad32ec34146bc678710bccef97c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "18EWGbGeXEp7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score , mean_squared_error\n",
        "import matplotlib.colors\n",
        "import math\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import time\n",
        "sns.set()\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "#importing essential libraries"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCbI5RHXdE8b"
      },
      "source": [
        "## English and Hindi Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-ylbPvNcoNM",
        "outputId": "1493212a-a5e5-404b-979e-c94b1e2b0655"
      },
      "source": [
        "char_eng = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_eng = '<pad>'\n",
        "eng_dict = {}\n",
        "eng_dict[pad_eng] = 0\n",
        "\n",
        "for i,letter in enumerate(char_eng):\n",
        "  eng_dict[letter] = i+1\n",
        "\n",
        "print(eng_dict)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3O-Agp_crS8",
        "outputId": "e090b79a-cbc9-41d0-f3ce-64df4698bddf"
      },
      "source": [
        "hindi_dict = {}\n",
        "hindi_dict['<pad>'] = 0\n",
        "\n",
        "char_hindi = ''\n",
        "for i in range(2304, 2432):\n",
        "  char_hindi += chr(i)\n",
        "\n",
        "for i, letter in enumerate(char_hindi):\n",
        "  hindi_dict[letter] = i+1\n",
        "\n",
        "print(hindi_dict)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irmgBQ_LdJbF"
      },
      "source": [
        "## Text Preprocessing helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsnyBKKscssl"
      },
      "source": [
        "def split_hindi_words(hindi_list):\n",
        "  new_hindi_list = []\n",
        "  for hind_name in hindi_list:\n",
        "    hind_name = hind_name.replace(',',' ').replace('_',' ').replace('.',' ').replace(\"'\",' ').replace('-',' ').replace('/',' ').replace('\\u200d',' ').replace('(',' ').replace(')',' ').replace('?',' ')\n",
        "    hind_name = hind_name.split()\n",
        "    new_hindi_list.append(hind_name)\n",
        "  return new_hindi_list\n",
        "\n",
        "def split_english_words(english_list):\n",
        "  new_english_list = []\n",
        "  regex = re.compile('[^a-zA-Z]')\n",
        "  for eng_name in english_list:\n",
        "    eng_name = eng_name.upper()\n",
        "    eng_name = eng_name.replace(\"'\",'').replace('/',' ')\n",
        "    eng_name = regex.sub(' ', eng_name)\n",
        "    eng_name = eng_name.split()\n",
        "    new_english_list.append(eng_name)\n",
        "  return new_english_list\n",
        "\n",
        "def clean_english_list(eng_list):\n",
        "  regex = re.compile('[^a-zA-Z]')\n",
        "  new_english_names = []\n",
        "  for word in eng_list:\n",
        "    new_english_names.append(regex.sub('', word))\n",
        "  return new_english_names\n",
        "\n",
        "def clean_hindi_list(hindi_list):\n",
        "  new_hindi_names = []\n",
        "  for word in hindi_list:\n",
        "    word = word.replace(',','').replace('_','').replace('.','').replace(\"'\",'')\n",
        "    new_hindi_names.append(word)\n",
        "  return new_hindi_names"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pebQTs9dNqQ"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgkLPqGicuO2"
      },
      "source": [
        "def convert_eng_to_encoded(X_train):\n",
        "  list_to_return = []\n",
        "  for word in X_train:\n",
        "    onehotstart = torch.zeros([len(word)+1,1])\n",
        "    onehotstart[len(word)][0] = 1\n",
        "    for i,letter in enumerate(word):\n",
        "      index = eng_dict[letter]\n",
        "      onehotstart[i][0] = index\n",
        "    list_to_return.append(onehotstart)\n",
        "  \n",
        "  return list_to_return \n",
        "\n",
        "def convert_hindi_to_encoded(Y_train):\n",
        "  list_to_return = []\n",
        "  for word in Y_train:\n",
        "    onehotstart = torch.zeros([len(word)+1,1])\n",
        "    onehotstart[len(word)][0] = 0\n",
        "    for i,letter in enumerate(word):\n",
        "      index = hindi_dict[letter]\n",
        "      onehotstart[i][0] = index\n",
        "    list_to_return.append(onehotstart)\n",
        "  \n",
        "  return list_to_return"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg3t-IO_dP7R"
      },
      "source": [
        "## Class to process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTWLoBEMcv4n"
      },
      "source": [
        "class EncoderDecoderData():\n",
        "\n",
        "  def __init__(self,filename):\n",
        "    self.final_eng_list, self.final_hindi_list = self.create_data_from_XML(filename)\n",
        "\n",
        " \n",
        "  def create_data_from_XML(self,filename):\n",
        "    tree = ET.parse(filename)\n",
        "    root = tree.getroot()\n",
        "    english_names = []\n",
        "    hindi_names = []\n",
        "    for elem in root.iter():\n",
        "      if elem.tag == 'SourceName':\n",
        "        english_names.append(elem.text)\n",
        "      if elem.tag == 'TargetName' and elem.attrib['ID'] == '1':\n",
        "        hindi_names.append(elem.text)\n",
        "    \n",
        "    new_english_names = split_english_words(english_names)\n",
        "    new_hindi_names = split_hindi_words(hindi_names)\n",
        "\n",
        "    final_hindi_data = []\n",
        "    final_english_data = []\n",
        "    \n",
        "    for eng_word, hindi_word in zip(new_english_names, new_hindi_names):\n",
        "      if len(eng_word) != len(hindi_word):\n",
        "        print('Skipping:', eng_word, '-', hindi_word)\n",
        "      else:\n",
        "        for eng_word_part, hindi_word_part in zip(eng_word, hindi_word):\n",
        "          final_hindi_data.append(hindi_word_part)\n",
        "          final_english_data.append(eng_word_part)\n",
        "    \n",
        "    final_hindi_data = clean_hindi_list(final_hindi_data)\n",
        "    final_english_data = clean_english_list(final_english_data)\n",
        "    self.final_eng_list = final_english_data\n",
        "    self.final_hindi_list = final_hindi_data\n",
        "    return self.final_eng_list, self.final_hindi_list\n",
        "\n",
        "\n",
        "  def generate_random_sample(self):\n",
        "    index = np.random.randint(len(self.final_eng_list))\n",
        "    return self.final_eng_list[index], self.final_hindi_list[index]\n",
        "  \n",
        "  def generate_random_batch(self,batch_size):\n",
        "    index = np.random.randint(len(self.final_eng_list))\n",
        "    batch_list_english = []\n",
        "    batch_list_hindi = []\n",
        "    for i in range(index,index+batch_size,1):\n",
        "      if i >= len(self.final_eng_list):\n",
        "        batch_list_english.append(self.final_eng_list[i-len(self.final_eng_list)])\n",
        "        batch_list_hindi.append(self.final_hindi_list[i-len(self.final_eng_list)])\n",
        "      else:\n",
        "        batch_list_english.append(self.final_eng_list[i])\n",
        "        batch_list_hindi.append(self.final_hindi_list[i])\n",
        "    return batch_list_english, batch_list_hindi\n",
        "  \n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioRVAMgFdSLL"
      },
      "source": [
        "## Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-RoeyRscylY"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size,emb_size,hidden_size):\n",
        "    super().__init__()  \n",
        "    self.input_size  = input_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.input_size,self.emb_size)\n",
        "\n",
        "    self.rnn = nn.LSTM(self.emb_size,self.hidden_size)\n",
        "  \n",
        "  def forward(self,input_word):\n",
        "    input_to_rnn = self.embedding(input_word.long())\n",
        "\n",
        "    output, (h_state,c_state) = self.rnn(input_to_rnn)\n",
        "\n",
        "    return h_state,c_state"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28qUTxozzjz"
      },
      "source": [
        "class Encoder_pre_attention(nn.Module):\n",
        "  def __init__(self, input_size, emb_size, hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size,emb_size)\n",
        "    self.rnn = nn.GRU(emb_size,hidden_size)\n",
        "  \n",
        "  def forward(self,input_word):\n",
        "    input_to_rnn = self.embedding(input_word.long())\n",
        "    output, h_state = self.rnn(input_to_rnn)\n",
        "\n",
        "    return h_state"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psib9-GQdURc"
      },
      "source": [
        "## Decoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cmQeDSDczsC"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,output_size,emb_size,hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size,emb_size)\n",
        "    self.rnn = nn.LSTM(emb_size, hidden_size)\n",
        "    self.fc = nn.Linear(hidden_size,output_size)\n",
        "  \n",
        "  def forward(self,input,h_state,c_state):\n",
        "\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    input_to_rnn = self.embedding(input.long())\n",
        "\n",
        "    output,(h_state,c_state) = self.rnn(input_to_rnn,(h_state,c_state))\n",
        "\n",
        "    prediction = self.fc(output.squeeze(0))\n",
        "\n",
        "    return prediction, h_state, c_state"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vud27dE03Oi"
      },
      "source": [
        "class Decoder_pre_attention(nn.Module):\n",
        "  def __init__(self,output_size,emb_size,hidden_size):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.output_size = output_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size,emb_size)\n",
        "    self.rnn = nn.GRU(hidden_size + emb_size,hidden_size)\n",
        "    self.fc = nn.Linear(emb_size + hidden_size*2,output_size)\n",
        "  \n",
        "  def forward(self,input,h_state, context):\n",
        "    \n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    input_to_rnn = self.embedding(input.long())\n",
        "\n",
        "    input_to_rnn_joined = torch.cat((input_to_rnn,context),2)\n",
        "\n",
        "    output, h_state = self.rnn(input_to_rnn_joined,h_state)\n",
        "\n",
        "    output = torch.cat((input_to_rnn,output,context),2)\n",
        "\n",
        "    prediction = self.fc(output.squeeze(0))\n",
        "\n",
        "    return prediction, h_state"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhxbO2xjdWH9"
      },
      "source": [
        "## Seq-2-Seq Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L640tfLkc04k"
      },
      "source": [
        "class seq2seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self,input, target,teacher_forcing_ratio = 0.5):\n",
        "\n",
        "    target_length = target.shape[0]\n",
        "    batch_size = target.shape[1]\n",
        "    output_size = self.decoder.output_size\n",
        "\n",
        "    outputs_to_return = torch.zeros(target_length,batch_size,output_size)\n",
        "\n",
        "    h_state,c_state = self.encoder(input)\n",
        "\n",
        "    input = torch.zeros(batch_size)\n",
        "\n",
        "    for i in range(target_length):\n",
        "      output,h_state,c_state = self.decoder(input,h_state,c_state)\n",
        "\n",
        "      outputs_to_return[i] = output\n",
        "\n",
        "      teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "\n",
        "      top1 = output.argmax(1)\n",
        "\n",
        "      input = target[i] if teacher_force else top1\n",
        "    \n",
        "    return outputs_to_return\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdvxwgrZ3F7D"
      },
      "source": [
        "class seq2seq_pre_attention(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self,input,target,teacher_forcing_ratio = 0.5):\n",
        "    \n",
        "    batch_size = target.shape[1]\n",
        "    target_length = target.shape[0]\n",
        "    output_size = self.decoder.output_size\n",
        "\n",
        "    outputs_to_return = torch.zeros(target_length,batch_size,output_size)\n",
        "\n",
        "    context = self.encoder(input)\n",
        "\n",
        "    h_state = context\n",
        "\n",
        "    input = torch.zeros(batch_size)\n",
        "    \n",
        "    for i in range(target_length):\n",
        "      output,h_state = self.decoder(input,h_state,context)\n",
        "\n",
        "      outputs_to_return[i] = output\n",
        "\n",
        "      teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "\n",
        "      top1 = output.argmax(1)\n",
        "\n",
        "      input = target[i] if teacher_force else top1\n",
        "    \n",
        "    return outputs_to_return\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-6uBh0HdbvA"
      },
      "source": [
        "## Setting up the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z279CrCTc2JL"
      },
      "source": [
        "input_size = 27\n",
        "output_size = 129\n",
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = Encoder_pre_attention(input_size,embedding_size,hidden_size)\n",
        "decoder = Decoder_pre_attention(output_size,embedding_size,hidden_size)\n",
        "model = seq2seq_pre_attention(encoder,decoder)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G36-oiCxdfXa"
      },
      "source": [
        "### Weight initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N39z0Svc3nN",
        "outputId": "e7956369-8a85-4bf6-d50f-074a8fd84ea4"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq_pre_attention(\n",
              "  (encoder): Encoder_pre_attention(\n",
              "    (embedding): Embedding(27, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "  )\n",
              "  (decoder): Decoder_pre_attention(\n",
              "    (embedding): Embedding(129, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc): Linear(in_features=1280, out_features=129, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDWoLCUXdhfc"
      },
      "source": [
        "### Loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrRsMVEdc42s"
      },
      "source": [
        "opt = optim.Adam(model.parameters(),lr=0.001)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP-yZtiYc61t"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diagyyLUdjil"
      },
      "source": [
        "### Training Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYk8PIq9xSm3"
      },
      "source": [
        "def generate_batch(data_loader,batch_size=16):\n",
        "  X_train,Y_train = data_loader.generate_random_batch(5000)\n",
        "  X_train, Y_train = convert_eng_to_encoded(X_train), convert_hindi_to_encoded(Y_train)\n",
        "\n",
        "  ## Forming X_train and Y_train with given preprocessing class and functions above\n",
        "\n",
        "  X_batch = []\n",
        "  Y_batch = []\n",
        "  sequence_length = np.random.randint(3,10)\n",
        "  for xpoint, ypoint in zip(X_train,Y_train):\n",
        "    if len(X_batch) == batch_size:\n",
        "      break\n",
        "    else:\n",
        "      if ypoint.shape[0] == sequence_length:\n",
        "        X_batch.append(xpoint)\n",
        "        Y_batch.append(ypoint)\n",
        "  \n",
        "  # We first select a random batch length and then look for all hindi words having the same length\n",
        "  # A list of X_batch and Y_batch is then formed whose length is batch_size\n",
        "\n",
        "  max_input_length = 0\n",
        "  for xpoint in X_batch:\n",
        "    if xpoint.shape[0] > max_input_length:\n",
        "      max_input_length = xpoint.shape[0]\n",
        "  \n",
        "  # We find the maximum length of the words that are stored in X_batch\n",
        "  \n",
        "  X_final_batch = torch.zeros(max_input_length,batch_size)\n",
        "  Y_final_batch = torch.zeros(sequence_length,batch_size)\n",
        "  for i in range(batch_size):\n",
        "    actual = X_batch[i].shape[0]\n",
        "    X_final_batch[:actual,i] = X_batch[i].view(-1)\n",
        "  for i in range(batch_size):\n",
        "    Y_final_batch[:,i] = Y_batch[i].view(-1)\n",
        "  \n",
        "  # Finally we form arrays, one X_final_batch having shape (max_length_X, batch_size) and one Y_final_batch having shape (batch_length, batch_size)\n",
        "  # Values from X_batch and Y_batch are appropriately placed here\n",
        "  \n",
        "  return X_final_batch,Y_final_batch\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0jzGDiHc76u"
      },
      "source": [
        "def train(model, data_loader, loss_func, optimizer,no_of_batches):\n",
        "  \n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  for i in range(no_of_batches):\n",
        "    X_final_batch, Y_final_batch = generate_batch(data_loader,16)\n",
        "    \n",
        "       \n",
        "    input = X_final_batch\n",
        "    target = Y_final_batch\n",
        "    \n",
        "    \n",
        "\n",
        "    output = model(input,target)\n",
        "\n",
        "    output_size = output.shape[-1]\n",
        "\n",
        "    loss = loss_func(output.view(-1,output_size),target.view(-1).long())\n",
        "    total_loss += loss.item()\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  \n",
        "\n",
        "  return total_loss/no_of_batches\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ZKzP99dni_"
      },
      "source": [
        "### Evaluation helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg_oaLQEc9Iv"
      },
      "source": [
        "def evaluate(model, test_data_loader, loss_func):\n",
        "  \n",
        "  model.eval()\n",
        "  X_test,Y_test = test_data_loader.generate_random_batch(32)\n",
        "  X_test_new = convert_eng_to_encoded(X_test)\n",
        "  Y_test_new = convert_hindi_to_encoded(Y_test)\n",
        "  \n",
        "  total_loss = 0\n",
        "\n",
        "  for i in range(len(X_test_new)):\n",
        "    input = X_test_new[i]\n",
        "    target = Y_test_new[i]\n",
        "\n",
        "\n",
        "    output = model(input,target)\n",
        "\n",
        "    output_size = output.shape[-1]\n",
        "\n",
        "    loss = loss_func(output.view(-1,output_size),target.view(-1).long())\n",
        "\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss/32\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkeMkZ3Ndp1Q"
      },
      "source": [
        "## Starting the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dRhBx7LhkcZ",
        "outputId": "2419bb7f-61b5-46b7-ad31-0c9a9f9fd6e2"
      },
      "source": [
        "data_store = EncoderDecoderData('training.xml')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping: ['MAHARANI', 'PADMINI'] - ['महारानी', 'पद्', 'मिनी']\n",
            "Skipping: ['STATE', 'MUSEUM', 'OF', 'THE', 'VERMONT', 'HISTORICAL', 'SOCIETY'] - ['स्टेट', 'म्युज़ियम', 'ऑफ', 'द', 'वरमाउंट', 'हिस्टॉरिकल', 'सोसायट', 'ी']\n",
            "Skipping: ['I', 'DUKAANT'] - ['इंदुकांत']\n",
            "Skipping: ['EFFIE', 'AWARDS'] - ['एफी', 'अवार्ड्', 'स']\n",
            "Skipping: ['LAURENCE', 'OLIVIER', 'AWARDS'] - ['लॉरेंस', 'ओलिवर', 'अवार्ड्', 'स']\n",
            "Skipping: ['ETTA'] - ['एट्', 'टा']\n",
            "Skipping: ['COLLEGE', 'FOOTBALL', 'AWARDS'] - ['कॉलेज', 'फुटबॉल', 'अवार्ड्', 'स']\n",
            "Skipping: ['STEVE', 'RHODES'] - ['स्टीव', 'रोड्', 'स']\n",
            "Skipping: ['WINDHAM', 'COUNTY', 'HISTORICAL', 'MUSEUM'] - ['व', 'िंडहैम', 'काउंट', 'ी', 'ह', 'िस्टॉर', 'िकल', 'म्युज़ियम']\n",
            "Skipping: ['PLAZA'] - ['प्लाज़ा', '66']\n",
            "Skipping: ['ADVAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['BHAALACHAN', 'DR'] - ['भालचन्द्र']\n",
            "Skipping: ['BARHARWA', 'JUNCTION'] - ['बरहरवा']\n",
            "Skipping: ['STATE', 'BNK', 'TR'] - ['स्टेट', 'बैंक', 'ऑफ', 'त्रावणकोर']\n",
            "Skipping: ['IN', 'DRAJEET'] - ['इन्द्रजीत']\n",
            "Skipping: ['WOODSTOCK', 'HISTORICAL', 'SOCIETY'] - ['वुडस्टॉक', 'ह', 'िस्टॉर', 'िकल', 'सोसायटी']\n",
            "Skipping: ['BROOKLYN', 'MUSEUM'] - ['ब्रुकल', 'िन', 'म्युज़ियम']\n",
            "Skipping: ['SAINT', 'FRANCIS', 'DASSISI', 'HIGH', 'SCHOOL'] - ['सेंट', 'फ्रांसिस', 'ड', 'िअस', 'ीसी', 'हाई', 'स्कूल']\n",
            "Skipping: ['PADMA', 'VIBHUSHAN'] - ['पद्', 'म', 'विभूषण']\n",
            "Skipping: ['GROUPE', 'CAISSE', 'D', 'PARGME'] - ['ग्रुप', 'कैस', 'डेपॉर्मे']\n",
            "Skipping: ['BRITISH', 'BOOK', 'AWARDS'] - ['ब्रिटिश', 'बुक', 'अवार्ड्', 'स']\n",
            "Skipping: ['SOUTH', 'ARLINGTON', 'CHURCH', 'OF', 'CHRIST'] - ['साउथ', 'अर्लिंग्टन']\n",
            "Skipping: ['JACK', 'RICHARDS'] - ['जैक', 'रिचर्ड्', 'स']\n",
            "Skipping: ['I', 'DIYAA'] - ['इंडिया']\n",
            "Skipping: ['DAINIK', 'JUGASANKHA'] - ['दैन', 'िक', 'जुगसंखा']\n",
            "Skipping: ['EISNER', 'AWARDS'] - ['ईस्नर', 'अवार्ड्', 'स']\n",
            "Skipping: ['THE', 'PIONEER', 'DAILY'] - ['द', 'पायोन', 'ियर', 'डेली']\n",
            "Skipping: ['SIYASAT'] - ['स', 'ियासत']\n",
            "Skipping: ['SCOTTISH', 'CHURCH', 'COLLEGE', 'CALCUTTA'] - ['स्कॉट', 'िश', 'चर्च', 'कॉलेज', 'कैलकटा']\n",
            "Skipping: ['ISMAIL'] - ['इस्मा', 'ईल']\n",
            "Skipping: ['DESHONNATI'] - ['देशोन्नत', 'ि']\n",
            "Skipping: ['PEARSE', 'MUSEUM'] - ['प', 'ियर्स', 'म्युज़ियम']\n",
            "Skipping: ['TAMBURITZA'] - ['तम्बुरिट्', 'ज़ा']\n",
            "Skipping: ['ANAIKUTTAM', 'RESERVOIR'] - ['अनइकुट्', 'टम', 'रिज़रवायर']\n",
            "Skipping: ['CENTRAL', 'CHRONICLE'] - ['सेंट्रल', 'क्रॉन', 'िकल']\n",
            "Skipping: ['SOCI', 'T', 'G', 'N', 'RALE'] - ['सोसायटी', 'जनरल']\n",
            "Skipping: ['KING', 'EDWARD', 'VII'] - ['किंग', 'एडवर्ड']\n",
            "Skipping: ['ORDER', 'OF', 'SPORTS', 'MERIT'] - ['ऑडर', 'ऑफ', 'स्पोर्ट्', 'स', 'मेरिट']\n",
            "Skipping: ['SRI', 'RAMAKRISHNA', 'VIDYASHALA', 'MYSORE'] - ['श्री', 'रामकृष्ण', 'व', 'िद्याशाला', 'मैसूर']\n",
            "Skipping: ['BRAJEN', 'DR'] - ['ब्रजेन्द्र']\n",
            "Skipping: ['DIBANG', 'VALLEY'] - ['दिबंगवैली']\n",
            "Skipping: ['THATTHA'] - ['ठट्', 'ठा']\n",
            "Skipping: ['LULU'] - ['लु', 'लु']\n",
            "Skipping: ['WILHEM', 'BUSCH', 'MUSEUM'] - ['व', 'िल्हेम', 'बुश', 'म्युज़ियम']\n",
            "Skipping: ['CHITTAR', 'RESERVOIR'] - ['चित्तर', 'रिज़रवायर', '2']\n",
            "Skipping: ['FIELDS', 'MEDAL'] - ['फील्ड्', 'स', 'मेडल']\n",
            "Skipping: ['SQUADS'] - ['सक्वैड्', 'स']\n",
            "Skipping: ['CARMEL', 'CONVENT', 'SCHOOL', 'CHANAKYA', 'PURI'] - ['कारमेल', 'कॉन्', 'वेंट', 'स्कूल', 'चाणक्य', 'पुरी']\n",
            "Skipping: ['VITTHAL'] - ['विट्', 'ठल']\n",
            "Skipping: ['LOGIE', 'AWARD'] - ['लॉग', 'अवार्ड्', 'स']\n",
            "Skipping: ['LOLA', 'AWARDS'] - ['लोला', 'अवार्ड्', 'स']\n",
            "Skipping: ['NOTTINGHAM', 'EAST', 'MIDLANDS'] - ['नॉटिंगहॅम', 'ईस्ट', 'मिडलैंड्', 'स']\n",
            "Skipping: ['MAHINDRA', 'UNITED', 'WORLD', 'COLLEGE', 'OF', 'INDIA'] - ['मह', 'िंद्रा', 'यूनाइटेड', 'वर्ल्ड', 'कॉलेज', 'ऑफ', 'इंडिया']\n",
            "Skipping: ['ADWAAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['ADWAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['THE', 'ECONOMIC', 'TIMES'] - ['द', 'इकनॉम', 'िक', 'टाइम्स']\n",
            "Skipping: ['I', 'JEENIYAR'] - ['इंजीनियर']\n",
            "Skipping: ['JITEN', 'DR'] - ['जितेन्द्र']\n",
            "Skipping: ['PRECISION', 'VALLEY', 'CORVETTE', 'MUSEUM'] - ['प्रिस', 'िज़न', 'वैली', 'कॉरवेट', 'म्युज़ियम']\n",
            "Skipping: ['SCHOCK', 'PRIZE', 'IN', 'VISUAL', 'ARTS'] - ['श्कॉक', 'प्राइज़', 'इन', 'विज़्युल', 'आर्ट्', 'स']\n",
            "Skipping: ['NATUN', 'DIN'] - ['नतुन', 'द', 'िन']\n",
            "Skipping: ['LYCHNOSTATIS', 'OPEN', 'AIR', 'MUSEUM'] - ['ल', 'िक्नोस्टेट', 'िस', 'ओपन', 'एयर', 'म्युज़ियम']\n",
            "Skipping: ['ADVAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['THE', 'TYNE', 'TURRETS'] - ['द', 'टायने', 'टरेट्', 'स']\n",
            "Skipping: ['VERIA', 'FOLKLORE', 'MUSEUM'] - ['वेर', 'िया', 'फोकलोर', 'म्युज़ियम']\n",
            "Skipping: ['DUBLIN', 'WRITERS', 'MUSEUM'] - ['डब्ल', 'िन', 'राइटर्स', 'म्युज़ियम']\n",
            "Skipping: ['NAIDUNIYA'] - ['नईदुन', 'िया']\n",
            "Skipping: ['FIDEL', 'EDWARDS'] - ['फिडल', 'एडवर्ड्', 'स']\n",
            "Skipping: ['ROSTER'] - ['रोस्', 'टर']\n",
            "Skipping: ['ORDER', 'OF', 'VASA'] - ['ऑडर', 'ऑफ़', 'द', 'वासा']\n",
            "Skipping: ['RUBIN', 'MUSEUM', 'OF', 'ART'] - ['रुब', 'िन', 'म्युज़ियम', 'ऑफ', 'आर्ट']\n",
            "Skipping: ['DINAKARAN'] - ['द', 'िनाकरण']\n",
            "Skipping: ['HARVEY', 'AWARDS'] - ['हार्वे', 'अवार्ड्', 'स']\n",
            "Skipping: ['ARGENTINA', 'MUSEUM', 'OF', 'NATURAL', 'SCIENCES'] - ['अर्जेंट', 'ीना', 'म्युज़ियम', 'ऑफ', 'नैचरल', 'साइंसेस']\n",
            "Skipping: ['BIG', 'BROTHER', 'AWARDS'] - ['बिग', 'ब्रदर', 'अवार्ड्', 'स']\n",
            "Skipping: ['ANDROS', 'MARITIME', 'MUSEUM'] - ['एण्ड्रॉस', 'मैर', 'िटाइम', 'म्युज़ियम']\n",
            "Skipping: ['CALCIUM'] - ['कै', 'ल्शियम']\n",
            "Skipping: ['TAKHAT', 'SHRI', 'SACHKHAND', 'SAHIB', 'NANDED', 'MAHARASHTRA'] - ['तखत', 'श्री', 'सचखंड', 'साहिब', 'नांदेड़', 'महाराष्', 'ट्र']\n",
            "Skipping: ['AZAMNAGAR', 'ROAD'] - ['आज़मनगर']\n",
            "Skipping: ['COMMIE', 'AWARDS'] - ['कॉमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['VIDUTHALAI'] - ['व', 'िदुथलई']\n",
            "Skipping: ['AKTIESELSKABET', 'DAMPSKIBSSELSKABET', 'TORM'] - ['एक्टीसल्:कॅबेट', 'डॅम्प्सकीबेसेल्सकॅबेट', 'टॉर्म', '4']\n",
            "Skipping: ['HISTORICAL', 'MUSEUM', 'OF', 'CRETE'] - ['ह', 'िस्टॉर', 'िकल', 'म्युज़ियम', 'ऑफ', 'क्रेट']\n",
            "Skipping: ['FRANCE', 'T', 'L', 'COM'] - ['फ़्रांस', 'टेलीकॉम']\n",
            "Skipping: ['MEDZHYBIZH'] - ['मेझ्ही', 'बिज़']\n",
            "Skipping: ['BRIT', 'AWARDS'] - ['ब्रिट', 'अवार्ड्', 'स']\n",
            "Skipping: ['ASOMIYA', 'PRATIDIN'] - ['असोम', 'िया', 'प्रतिद', 'िन']\n",
            "Skipping: ['RUTGERS'] - ['रट्', 'जर्स']\n",
            "Skipping: ['DAINIK', 'JANAMBHUMI'] - ['दैन', 'िक', 'जन्मभूमि']\n",
            "Skipping: ['CAPE', 'TOWN'] - ['केपटाउन']\n",
            "Skipping: ['PETRONAS', 'TOWER'] - ['प्रेट्रोनॉस', 'टॉवर', '2']\n",
            "Skipping: ['CHATTAN', 'SINGH'] - ['चट्', 'टान', 'सिंह']\n",
            "Skipping: ['AUSTRALIAN', 'WAR', 'MEMORIAL'] - ['ऑस्ट्रेल', 'ियन', 'वार', 'मेमोर', 'ियल']\n",
            "Skipping: ['EADS'] - ['इएड्', 'स']\n",
            "Skipping: ['NEW', 'ZEALAND'] - ['न्यूज़ीलैंड']\n",
            "Skipping: ['MUSEUM', 'AND', 'STUDY', 'CENTRE', 'OF', 'THE', 'GREEK', 'THEATRE'] - ['म्युज़ियम', 'एण्ड', 'स्टडी', 'सेंटर', 'ऑफ', 'द', 'ग्रीक', 'थ', 'िएटर']\n",
            "Skipping: ['HISTORICAL', 'AND', 'FOLKLORE', 'MUSEUM', 'OF', 'CORINTH'] - ['ह', 'िस्टॉर', 'िकल', 'एण्ड', 'फोकलोर', 'म्युज़ियम', 'ऑफ', 'कॉर', 'िन्थ']\n",
            "Skipping: ['RAMCOIND'] - ['राम्को', 'इंड']\n",
            "Skipping: ['NAWAB', 'SIRAZUDDAULA'] - ['नवाब', 'सिराज़ुद्', 'दौला']\n",
            "Skipping: ['PAUL', 'VRELLIS', 'GREEK', 'HISTORY', 'MUSEUM'] - ['पॉल', 'व्रेलिस', 'ग्रीक', 'ह', 'िस्ट्री', 'म्युज़ियम']\n",
            "Skipping: ['MANEE', 'DR'] - ['मणींद्र']\n",
            "Skipping: ['BILLIARDS'] - ['बिलियर्ड्', 'स']\n",
            "Skipping: ['PETROBR', 'S'] - ['पेट्रोब्रस']\n",
            "Skipping: ['DHARITRI'] - ['धर', 'ित्री']\n",
            "Skipping: ['VISHVEN', 'DR'] - ['विश्वेन्द्र']\n",
            "Skipping: ['NANJING', 'MUSEUM'] - ['नंज', 'िंग', 'म्युज़ियम']\n",
            "Skipping: ['MUSEUM', 'OF', 'DIONYSIOS', 'SOLOMOS', 'AND', 'EMINENT', 'ZAKYNTHIANS'] - ['म्युज़ियम', 'ऑफ', 'डीयॉन', 'िस', 'ियॉस', 'सोलोमॉस', 'एण्ड', 'एम', 'िनेंट', 'झॅक', 'िन्थ', 'ियन्स']\n",
            "Skipping: ['NAHANNI'] - ['ना', 'हान्नी']\n",
            "Skipping: ['AUSTRALIAN', 'NATIONAL', 'UNIVERSITY'] - ['ऑस्ट्रेलियननेशनल', 'यूनिवर्सिटी']\n",
            "Skipping: ['GOKHALE', 'MEMORIAL', 'GIRLS', 'SCHOOL', 'CALCUTTA'] - ['गोखले', 'मेमोर', 'ियल', 'गर्ल्स', 'स्कूल', 'कैलकटा']\n",
            "Skipping: ['JAHAN', 'AARA'] - ['जहाँआरा']\n",
            "Skipping: ['NAVABHARAT', 'FERRO', 'ALLOYS'] - ['नव', 'भारत', 'फ़ैरो', 'अलॉय']\n",
            "Skipping: ['UNIVERSITY', 'OF', 'LEEDS'] - ['यूनिवर्सिटी', 'ऑफ', 'लीड्', 'स']\n",
            "Skipping: ['ADBUL', 'QAWI'] - ['अद्', 'बुल', 'कावी']\n",
            "Skipping: ['GONCI', 'RE', 'EURIS'] - ['गॉन्सियर', 'यूरिस']\n",
            "Skipping: ['JEEVAN', 'MRITYU'] - ['जीवन', 'मृ', 'त्यु']\n",
            "Skipping: ['DIVYA', 'BHASKAR'] - ['द', 'िव्य', 'भास्कर']\n",
            "Skipping: ['RAMA', 'LINGESHWARA'] - ['रामालिंगेश्वर']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'RHODES'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'रोड्स']\n",
            "Skipping: ['FAKHRUN', 'NISA'] - ['फखरुन्निसा']\n",
            "Skipping: ['DEVEN', 'DR'] - ['देवेन्द्र']\n",
            "Skipping: ['FORT', 'LAFAYETTE'] - ['फोर्ट', 'लैफायेट्', 'ट']\n",
            "Skipping: ['MAJOR', 'LEAGUE', 'LACROSSE', 'SPORTSMAN', 'OF', 'THE', 'YEAR', 'AWARD'] - ['मेजर', 'लीग', 'लैक्रोस', 'स्पोर्ट्', 'समैन', 'ऑफ', 'द', 'ईयर', 'अवार्ड']\n",
            "Skipping: ['DAINIK', 'BHASKAR'] - ['दैन', 'िक', 'भास्कर']\n",
            "Skipping: ['FOUR', 'CONTINENTS', 'FIGURE', 'SKATING', 'CHAMPIONS'] - ['फोर', 'कॉनटिनेंट्', 'स', 'फिगर', 'स्केटिंग', 'चैंपियन्स']\n",
            "Skipping: ['GALLANTS', 'BOWER'] - ['गैलंट्', 'स', 'बॉवर']\n",
            "Skipping: ['GUINESS', 'WORLD', 'OF', 'RECORD', 'MUSEUM'] - ['ग', 'िनीज', 'वर्ल्ड', 'ऑफ', 'रेकॉर्ड्स', 'म्युज़ियम']\n",
            "Skipping: ['REDIFF', 'COM', 'INDIA', 'LIMITED'] - ['रेडिफ़', 'डॉट', 'कॉम', 'इंडिया', 'लिमिटेड']\n",
            "Skipping: ['ORDER', 'OF', 'THE', 'NATIONAL', 'COAT', 'OF', 'ARMS'] - ['ऑडर', 'ऑफ', 'द', 'नेशनल', 'कोट', 'ऑफ', 'आर्म्', 'स']\n",
            "Skipping: ['NATIONAL', 'MUSEUM', 'OF', 'AMERICAN', 'INDIAN'] - ['नेशनल', 'म्युज़ियम', 'ऑफ', 'अमेर', 'िकन', 'इंड', 'ियन']\n",
            "Skipping: ['THE', 'HUMBER', 'FORTS'] - ['द', 'हंबर', 'फोर्ट्', 'स']\n",
            "Skipping: ['OMKARNATH', 'THAKUR'] - ['ओंकार', 'नाथ', 'ठाकुर']\n",
            "Skipping: ['NAOMI', 'AWARDS'] - ['नाओमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['AMERICAN', 'NUMISMATIC', 'SOCIETY', 'MUSEUM'] - ['अमेर', 'िकन', 'न्यूम', 'िस्मेट', 'िक', 'सोसायटी', 'म्युज़ियम']\n",
            "Skipping: ['RASTRIYA', 'SAHARA'] - ['राष्', 'ट्रीय', 'सहारा']\n",
            "Skipping: ['GARY', 'ROBERTSON'] - ['गेरी', 'रॉबर्ट्', 'सन']\n",
            "Skipping: ['ANDREW', 'SYMONDS'] - ['एण्ड्रयू', 'सिमंड्', 'स']\n",
            "Skipping: ['NEW', 'YORK', 'HISTORICAL', 'SOCIETY'] - ['न्यू', 'यॉर्क', 'ह', 'िस्टॉर', 'िकल', 'सोसायटी']\n",
            "Skipping: ['IDJWI'] - ['इड्', 'ज्वी']\n",
            "Skipping: ['OPENTV'] - ['ओपन', 'टीवी']\n",
            "Skipping: ['BRITISH', 'COMEDY', 'AWARDS'] - ['ब्रिटिश', 'कॉमेडी', 'अवार्ड्', 'स']\n",
            "Skipping: ['SHAILEN', 'DR'] - ['शैलेन्द्र']\n",
            "Skipping: ['FORT', 'RONDUIT'] - ['फोर्ट', 'रॉनड्', 'यूट']\n",
            "Skipping: ['GERMAN', 'HISTORICAL', 'MUSEUM'] - ['जर्मन', 'ह', 'िस्टॉर', 'िकल', 'म्युज़ियम']\n",
            "Skipping: ['STUDIO', 'MUSEUM', 'IN', 'HARLEN'] - ['स्टुड', 'ियो', 'म्युज़ियम', 'इन', 'हार्लेन']\n",
            "Skipping: ['UN', 'MANAA'] - ['उन्मना']\n",
            "Skipping: ['ENVOY', 'COMMUNICATIONS', 'GROUP'] - ['एन्वॉय', 'कम्युनिकेशंस']\n",
            "Skipping: ['ARISTOTLE', 'ONASSIS'] - ['एरीस्टोटल', 'ओनासि', 'स']\n",
            "Skipping: ['DAINIK', 'AGRADOOT'] - ['दैन', 'िक', 'अग्रदूत']\n",
            "Skipping: ['WAR', 'OF', 'THE', 'HOLY', 'LEAGUE'] - ['वार', 'ऑफ', 'होली', 'लीग']\n",
            "Skipping: ['PHIL', 'EDMONDS'] - ['फिल', 'एडमंड्', 'स']\n",
            "Skipping: ['UNITED', 'STATES', 'MILITARY', 'ACADEMY'] - ['यूनाइटेड', 'स्टेट्', 'स', 'मिलिट्री', 'अकेडमी']\n",
            "Skipping: ['HINDI', 'MILAP'] - ['ह', 'िन्दी', 'म', 'िलाप']\n",
            "Skipping: ['THE', 'GOULANDRIS', 'MUSEUM', 'OF', 'NATURAL', 'HISTORY'] - ['द', 'गॉलैंड्रीस', 'म्युज़ियम', 'ऑफ', 'नैचरल', 'ह', 'िस्टरी']\n",
            "Skipping: ['PRICKETTS', 'FORT'] - ['प्रिकेट्', 'स', 'फोर्ट']\n",
            "Skipping: ['ROBERT', 'BRIDGES'] - ['रॉबर्ट', 'ब्रिजि', 'ज']\n",
            "Skipping: ['MUSEUM', 'OF', 'ENGRAVINGS', 'AND', 'GRAPHIC', 'ARTS'] - ['म्युज़ियम', 'ऑफ', 'एन्ग्रेव', 'िंग्स', 'एण्ड', 'ग्राफ', 'िक', 'आर्ट्स']\n",
            "Skipping: ['WHITNEY', 'MUSEUM', 'OF', 'AMERICAN', 'ART'] - ['व्ह', 'िटनी', 'म्युज़ियम', 'ऑफ', 'अमेर', 'िकन', 'आर्ट']\n",
            "Skipping: ['SAPTAHIK', 'HINDUSTAN'] - ['साप्ताह', 'िक', 'ह', 'िन्दुस्तान']\n",
            "Skipping: ['VIJAYA', 'KARNATAKA'] - ['व', 'िजय', 'कर्नाटका']\n",
            "Skipping: ['CORNISH', 'COLONY', 'MUSEUM'] - ['कॉर्न', 'िश', 'कॉलोनी', 'म्युज़ियम']\n",
            "Skipping: ['MUSEUM', 'OF', 'ISLAMIC', 'CERAMICS'] - ['म्युज़ियम', 'ऑफ', 'इस्लाम', 'िक', 'स', 'िरेम', 'िक्स']\n",
            "Skipping: ['SUREN', 'DAR'] - ['सुरेन्दर']\n",
            "Skipping: ['SAINT', 'JOHNS', 'HIGH', 'SCHOOL', 'CHANDIGARH'] - ['सेंट', 'जॉन्', 'स', 'हाई', 'स्कूल', 'चंडीगढ़']\n",
            "Skipping: ['DENIRO'] - ['डी', 'निरो']\n",
            "Skipping: ['LOKAMAAN', 'Y'] - ['लोकमान्य']\n",
            "Skipping: ['VAPARAISO', 'CHURCH', 'OF', 'CHRIST'] - ['व्हापरासिओ']\n",
            "Skipping: ['AJI', 'ASSAMESE', 'DAILY'] - ['अज', 'ि', 'असमीज', 'डेल्ही']\n",
            "Skipping: ['PHILATELIC', 'MUSEUM'] - ['फ', 'िलेटेलिक', 'म्युज़ियम']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'PIRAEUS'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'पायरस']\n",
            "Skipping: ['SAUJAN', 'Y'] - ['सौजन्य']\n",
            "Skipping: ['PARIS', 'CHARLES', 'DE', 'GAULLE'] - ['पेरिस', 'रॉसे', 'चार्ल्स', 'डे', 'ग्यूले']\n",
            "Skipping: ['PARKWAY', 'APOSTOLIC'] - ['पार्क', 'वे', 'अपोस्टोलिक']\n",
            "Skipping: ['PRATIDIN'] - ['प्रत', 'िद', 'िन']\n",
            "Skipping: ['RADIO', 'ACADEMY', 'AWARDS'] - ['रेडियो', 'अकेडमी', 'अवार्ड्', 'स']\n",
            "Skipping: ['KAPEE', 'DR'] - ['कपींद्र']\n",
            "Skipping: ['NETHERLANDS'] - ['नीदरलैंड्', 'स']\n",
            "Skipping: ['CHAITANYA', 'ENGLISH', 'MEDIUM', 'HIGH', 'SCHOOL', 'TIRUPATHI'] - ['चैतन्य', 'इंग्ल', 'िश', 'म', 'ीड', 'ियम', 'हाई', 'स्कलू', 'त', 'िरुपती']\n",
            "Skipping: ['SORRENTO'] - ['सॉरेन्टो', '1']\n",
            "Skipping: ['PRIX', 'DES', 'DEUX', 'MAGOTS'] - ['प्रिक्स', 'डेस', 'ड्', 'यूक्स', 'मैगट्', 'स']\n",
            "Skipping: ['MAUNA', 'LOA'] - ['मौनालोआ']\n",
            "Skipping: ['HANSRAJ', 'MORARJI', 'PUBLIC', 'SCHOOL'] - ['हंसराज', 'मोरारजी', 'पब्ल', 'िक', 'स्कूल']\n",
            "Skipping: ['BUDNASEEB'] - ['बद', 'नसीब']\n",
            "Skipping: ['SQUIDDY', 'AWARDS'] - ['स्क्विडी', 'अवार्ड्', 'स']\n",
            "Skipping: ['MASS', 'MUTUAL', 'LIFE'] - ['मास', 'म्युच्युअल', 'लाइफ़', 'इंश्योरेंस']\n",
            "Skipping: ['STATS', 'CHIPPAC'] - ['स्टेट्सचिपपैक']\n",
            "Skipping: ['KAREN', 'BLIXEN', 'MUSEUM'] - ['कैरेन', 'ब्ल', 'िक्सन', 'म्युज़ियम']\n",
            "Skipping: ['YESHIVA', 'UNIVERSITY', 'MUSEUM'] - ['येश', 'िवा', 'युन', 'िवर्स', 'िटी', 'म्युज़ियम']\n",
            "Skipping: ['KAVIGNAR', 'INKULAB'] - ['कव', 'िग्नर', 'इंकलाब']\n",
            "Skipping: ['DRAFTS'] - ['ड्राफ्ट्', 'स']\n",
            "Skipping: ['SATISH', 'GUJRAL'] - ['स', 'तीश', 'गुजराल']\n",
            "Skipping: ['BHATTU'] - ['भट्', 'टु']\n",
            "Skipping: ['DAVIS', 'MUSEUM', 'AND', 'CULTURAL', 'ART'] - ['डेव', 'िस', 'म्युज़ियम', 'एण्ड', 'कल्चरल', 'आर्ट']\n",
            "Skipping: ['AMERICAN', 'ACADEMY', 'OF', 'ARTS', 'AND', 'LETTERS', 'FICTION', 'AWARD'] - ['अमेरिकन', 'अकेडमी', 'ऑफ', 'आर्ट्', 'स', 'एण्ड', 'लेटर्स', 'फिक्शन', 'अवार्ड']\n",
            "Skipping: ['NEWFOUNDLAND'] - ['न्यू', 'फाउंडलैंड']\n",
            "Skipping: ['FLORINA', 'MUSEUM', 'OF', 'MODERN', 'ART'] - ['फ्लोर', 'िना', 'म्युज़ियम', 'ऑफ', 'मॉडर्न', 'आर्ट']\n",
            "Skipping: ['LONDONHEATHROW'] - ['लंदन', 'हीथ्रो']\n",
            "Skipping: ['RYAN', 'HINDS'] - ['रियान', 'हिन्ड्', 'स']\n",
            "Skipping: ['MUSEUM', 'OF', 'THE', 'CITY', 'OF', 'NEW', 'YORK'] - ['म्युज़ियम', 'ऑफ', 'द', 'स', 'िट', 'ी', 'ऑफ', 'न्यू', 'यॉर्क']\n",
            "Skipping: ['GAJEN', 'DR'] - ['गजेन्द्र']\n",
            "Skipping: ['NAREN', 'DAR'] - ['नरेन्दर']\n",
            "Skipping: ['KOSTAS', 'FRONTZOS', 'MUSEUM', 'OF', 'EPIRUS', 'FOLK', 'ART'] - ['कोस्टस', 'फ्रंटज़ॉस', 'म्युज़ियम', 'ऑफ', 'एप', 'िरस', 'फोक', 'आर्ट']\n",
            "Skipping: ['THE', 'INTERNATIONAL', 'ANDY', 'AWARDS'] - ['द', 'इंटरनेशनल', 'एण्डी', 'अवार्ड्', 'स']\n",
            "Skipping: ['DELHI', 'PUBLIC', 'SCHOOL', 'INDIRAPURAM'] - ['डेल्ही', 'पब्ल', 'िक', 'स्कूल', 'इंदिरापुरम']\n",
            "Skipping: ['HUKUMACHAN', 'D'] - ['हुकुमचन्द']\n",
            "Skipping: ['ADWAAKI'] - ['अद्', 'वाकी']\n",
            "Skipping: ['DAYTON', 'ART', 'INSTITUTE'] - ['डेटन', 'आर्ट', 'इंस्टीट्', 'यूट']\n",
            "Skipping: ['NATIONAL', 'TELEVISION', 'AWARDS'] - ['नेशनल', 'टेलीविज़न', 'अवार्ड्', 'स']\n",
            "Skipping: ['VANAKAN', 'YAA'] - ['वनकन्या']\n",
            "Skipping: ['COBH', 'HERITAGE', 'CENTER'] - ['कोब', 'हेर', 'िटेज', 'सेंटर']\n",
            "Skipping: ['RETALIX'] - ['रेटालिक्स', 'लि']\n",
            "Skipping: ['CARMEL', 'CONVENT', 'SCHOOL', 'CHANDIGARH'] - ['कारमेल', 'कॉन्', 'वेंट', 'स्कूल', 'चंडीगढ़']\n",
            "Skipping: ['MUSEUM', 'OF', 'WORKS', 'BY', 'THEOPHILOS'] - ['म्युज़ियम', 'ऑफ', 'वर्क्स', 'बाय', 'थ', 'ियोफ', 'िलस']\n",
            "Skipping: ['CHENGALPATTU'] - ['चेंगलपट्', 'टु']\n",
            "Skipping: ['VIV', 'RICHARDS'] - ['विव', 'रिचर्ड्', 'स']\n",
            "Skipping: ['LAUREUS', 'WORLD', 'SPORTS', 'AWARDS'] - ['लॉरेस', 'वर्ल्ड', 'स्पोर्ट्', 'स', 'अवार्ड्', 'स']\n",
            "Skipping: ['WILSON', 'CASTLE'] - ['व', 'िल्सन', 'कॅसल']\n",
            "Skipping: ['STATE', 'HERALDIC', 'MUSEUM'] - ['स्टेट', 'हेराल्ड', 'िक', 'म्युज़ियम']\n",
            "Skipping: ['NYIKA', 'PLATEAU'] - ['न्याइका', 'प्लेट्', 'यू']\n",
            "Skipping: ['LAGARD', 'RE', 'GROUPE'] - ['लैगार्डेयर', 'ग्रुप']\n",
            "Skipping: ['PADMA', 'BHUSHAN'] - ['पद्', 'म', 'भूषण']\n",
            "Skipping: ['ROYAL', 'ONTARIO', 'MUSEUM'] - ['रॉयल', 'ऑन्टार', 'ियो', 'म्युज़ियम']\n",
            "Skipping: ['GOVERNMENT', 'MODEL', 'HIGHER', 'SECONDARY', 'SCHOOL', 'THIRUVANANTHAPURAM'] - ['गवर्नमेंट', 'मॉडेल', 'हायर', 'सेकंडरी', 'स्कूल', 'थ', 'िरुवनंतपुरम']\n",
            "Skipping: ['SRISAILAM'] - ['श्री', 'शैलम']\n",
            "Skipping: ['SPATHAREION', 'MUSEUM', 'OF', 'THE', 'SHADOW', 'THEATRE'] - ['स्पाथेर', 'ियन', 'म्युज़ियम', 'ऑफ', 'द', 'शैडो', 'थ', 'िएटर']\n",
            "Skipping: ['DAINIK', 'JANASADHARAN'] - ['दैन', 'िक', 'जनसाधारण']\n",
            "Skipping: ['COLVIN', 'TALUQADARS', 'COLLEGE', 'LUCKNOW'] - ['कॉल्व', 'िन', 'तालुकदार्स', 'कॉलेज', 'लखनऊ']\n",
            "Skipping: ['SADGUNA'] - ['सद्', 'गुण']\n",
            "Skipping: ['SUMMIT', 'AWARDS'] - ['सुमित', 'अवार्ड्', 'स']\n",
            "Skipping: ['THE', 'HINDU'] - ['द', 'ह', 'िन्दू']\n",
            "Skipping: ['MUNICIPAL', 'GALLERY', 'OF', 'CORFU'] - ['म्युन', 'िस', 'िपल', 'गैलरी', 'ऑफ', 'कोरफू']\n",
            "Skipping: ['PORTSMOUTH'] - ['पोर्ट्', 'समाउथ']\n",
            "Skipping: ['FORT', 'STEURGAT'] - ['फोर्ट', 'स्ट्', 'यूर्गेट']\n",
            "Skipping: ['ADWAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['KAN', 'HAIYAALAAL'] - ['कन्हैयालाल']\n",
            "Skipping: ['KARA', 'KUM'] - ['काराकुम']\n",
            "Skipping: ['CHASHME', 'BADDOOR'] - ['चश्मे', 'बद्', 'दूर']\n",
            "Skipping: ['RAILWAY', 'MUSEUM', 'OF', 'THE', 'MUNICIPALITY', 'OF', 'KALAMATA'] - ['रेल्वे', 'म्युज़ियम', 'ऑफ', 'द', 'म्यून', 'िस', 'िपाल्टी', 'ऑफ', 'कलमाटा']\n",
            "Skipping: ['ADVAAKEE'] - ['अद्', 'वाकी']\n",
            "Skipping: ['MUSEUM', 'OF', 'ROMANI', 'CULTURE'] - ['म्युज़ियम', 'ऑफ', 'रोमान', 'ी', 'कल्चर']\n",
            "Skipping: ['SHAKE', 'HANDS'] - ['शेक', 'हैंड्', 'स']\n",
            "Skipping: ['WIND', 'RIVER'] - ['विंडरिवर']\n",
            "Skipping: ['HAMILTON', 'MASAKADZA'] - ['हैमिल्टन', 'मसाकद्', 'ज़ा']\n",
            "Skipping: ['CHARLES', 'ROBERTS', 'AWARD'] - ['चार्ल्स', 'रॉबर्ट्', 'स', 'अवार्ड']\n",
            "Skipping: ['PATHANAMTHITTA'] - ['पथानामथीट्', 'टा']\n",
            "Skipping: ['NETAJI', 'SUBHASH', 'CHANDRA', 'BOSE'] - ['नेताजी', 'सुभाषचंद्र', 'बोस']\n",
            "Skipping: ['ROCKBROOK', 'UNITED'] - ['रॉकब्रुक', 'यूनाइटेड', 'मेथोडिस्ट']\n",
            "Skipping: ['MUQADDAS'] - ['मुक़द्', 'दस']\n",
            "Skipping: ['WALTER', 'SCOTT'] - ['वॉल्टरस्कॉट']\n",
            "Skipping: ['COLOURPLUS', 'FASHIONS'] - ['कलर', 'प्लस', 'फ़ैशन्स']\n",
            "Skipping: ['ATTUR'] - ['अट्', 'टूर']\n",
            "Skipping: ['GEOLOGICAL', 'MUSEUM', 'OF', 'CHINA'] - ['ज', 'िओलॉजकिल', 'म्युज़ियम', 'ऑफ', 'चाइना']\n",
            "Skipping: ['BAL', 'KRISHNA'] - ['बालकृष्णा']\n",
            "Skipping: ['THE', 'MORNING', 'QUICK'] - ['द', 'मॉर्निंग', 'क्व', 'िक']\n",
            "Skipping: ['SHAILE', 'DR'] - ['शैलेंद्र']\n",
            "Skipping: ['RAAJEN', 'DR'] - ['राजेन्द्र']\n",
            "Skipping: ['PORTSDOWN', 'HILL'] - ['पोर्ट्', 'सडाउन', 'हिल']\n",
            "Skipping: ['WEST', 'SPITSBERGEN'] - ['वेस्ट', 'स्पीट्', 'सबर्गन']\n",
            "Skipping: ['STIRLING', 'SMITH', 'MUSEUM', 'AND', 'ART', 'GALLERY'] - ['स्टर्ल', 'िंग', 'स्म', 'िथ', 'म्युज़ियम', 'एण्ड', 'आर्ट', 'गैलरी']\n",
            "Skipping: ['ARMY', 'PUBLIC', 'SCHOOL', 'NEW', 'DELHI'] - ['आर्मी', 'पब्', 'लिक', 'स्', 'कूल', 'न्यू', 'डेल्ही']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMDw8DgVc-hg",
        "outputId": "35c6d876-b8f8-4859-ea35-50cba03766b5"
      },
      "source": [
        "data_tester = EncoderDecoderData('testing.xml')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping: ['W', 'TTEMBERG'] - ['यूटमबर्ग']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0933764bcd744487b3451cba86f33410",
            "f3a5bbed4c6b4d3c8474d88cd01ba5f1",
            "2e6b002c4d314e6c844a4ffa7040cb1f",
            "ae7dc475d1f844559b60ff3356a09080",
            "d9ae7f071b0e416daf63be58b71f596f",
            "4a218ac1d21b46bda83e0419deeaac22",
            "68ef22db0d0f4a1bb3b78747d8c14d87",
            "7e031ad32ec34146bc678710bccef97c"
          ]
        },
        "id": "4lGDs08Jc_sA",
        "outputId": "a046109a-9546-4bf5-cc0a-609368129903"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "for i in tqdm_notebook(range(epochs)):\n",
        "  print('Training Loss:',train(model,data_store,loss_func,opt,20))\n",
        "  print('validation Loss:',evaluate(model,data_store,loss_func))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0933764bcd744487b3451cba86f33410",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 4.85427622795105\n",
            "validation Loss: 4.618210569024086\n",
            "Training Loss: 4.5607970476150514\n",
            "validation Loss: 4.323454365134239\n",
            "Training Loss: 4.241516661643982\n",
            "validation Loss: 3.987589880824089\n",
            "Training Loss: 3.8402589678764345\n",
            "validation Loss: 3.576748311519623\n",
            "Training Loss: 3.7195451855659485\n",
            "validation Loss: 3.5410033464431763\n",
            "Training Loss: 3.6192405581474305\n",
            "validation Loss: 3.7381883561611176\n",
            "Training Loss: 3.6278109431266783\n",
            "validation Loss: 3.4831403121352196\n",
            "Training Loss: 3.6286044836044313\n",
            "validation Loss: 3.6207800954580307\n",
            "Training Loss: 3.4905601859092714\n",
            "validation Loss: 3.5406046956777573\n",
            "Training Loss: 3.482581579685211\n",
            "validation Loss: 3.422429397702217\n",
            "Training Loss: 3.4594224095344543\n",
            "validation Loss: 3.542326919734478\n",
            "Training Loss: 3.4694266080856324\n",
            "validation Loss: 3.4094421938061714\n",
            "Training Loss: 3.519970381259918\n",
            "validation Loss: 3.4220425188541412\n",
            "Training Loss: 3.410076451301575\n",
            "validation Loss: 3.422138050198555\n",
            "Training Loss: 3.3707704305648805\n",
            "validation Loss: 3.499423034489155\n",
            "Training Loss: 3.3716578841209413\n",
            "validation Loss: 3.349540002644062\n",
            "Training Loss: 3.360866963863373\n",
            "validation Loss: 3.3463660329580307\n",
            "Training Loss: 3.3301021099090575\n",
            "validation Loss: 3.3065887466073036\n",
            "Training Loss: 3.353300654888153\n",
            "validation Loss: 3.336729258298874\n",
            "Training Loss: 3.319940984249115\n",
            "validation Loss: 3.2407532185316086\n",
            "Training Loss: 3.2780029296875\n",
            "validation Loss: 3.3391150534152985\n",
            "Training Loss: 3.219220149517059\n",
            "validation Loss: 3.3602274879813194\n",
            "Training Loss: 3.257420516014099\n",
            "validation Loss: 3.4057981073856354\n",
            "Training Loss: 3.2283919215202332\n",
            "validation Loss: 3.201254814863205\n",
            "Training Loss: 3.1841941714286803\n",
            "validation Loss: 3.399959161877632\n",
            "Training Loss: 3.09684237241745\n",
            "validation Loss: 3.3033988177776337\n",
            "Training Loss: 3.083474540710449\n",
            "validation Loss: 3.2349178045988083\n",
            "Training Loss: 3.1884588599205017\n",
            "validation Loss: 3.3211257606744766\n",
            "Training Loss: 3.0470955848693846\n",
            "validation Loss: 3.2191772162914276\n",
            "Training Loss: 3.0158660650253295\n",
            "validation Loss: 3.095237784087658\n",
            "Training Loss: 3.0493070006370546\n",
            "validation Loss: 3.231748975813389\n",
            "Training Loss: 2.968218505382538\n",
            "validation Loss: 3.1564119309186935\n",
            "Training Loss: 2.9824797451496123\n",
            "validation Loss: 3.268269918859005\n",
            "Training Loss: 2.8982738137245176\n",
            "validation Loss: 3.234406102448702\n",
            "Training Loss: 2.8486037254333496\n",
            "validation Loss: 3.230297163128853\n",
            "Training Loss: 2.8465976774692536\n",
            "validation Loss: 2.9665850214660168\n",
            "Training Loss: 2.84028799533844\n",
            "validation Loss: 2.8914266228675842\n",
            "Training Loss: 2.765755182504654\n",
            "validation Loss: 3.111753150820732\n",
            "Training Loss: 2.8292263776063917\n",
            "validation Loss: 3.134427160024643\n",
            "Training Loss: 2.826588052511215\n",
            "validation Loss: 2.9265173971652985\n",
            "Training Loss: 2.52077357172966\n",
            "validation Loss: 2.976715609431267\n",
            "Training Loss: 2.7373606324195863\n",
            "validation Loss: 3.015009231865406\n",
            "Training Loss: 2.7074044585227965\n",
            "validation Loss: 2.6663773432374\n",
            "Training Loss: 2.626047444343567\n",
            "validation Loss: 2.7155190464109182\n",
            "Training Loss: 2.770859920978546\n",
            "validation Loss: 2.8078964203596115\n",
            "Training Loss: 2.449859857559204\n",
            "validation Loss: 2.8390270732343197\n",
            "Training Loss: 2.547762131690979\n",
            "validation Loss: 3.015060655772686\n",
            "Training Loss: 2.5937579393386843\n",
            "validation Loss: 2.704928182065487\n",
            "Training Loss: 2.5755797386169434\n",
            "validation Loss: 2.915556386113167\n",
            "Training Loss: 2.558941978216171\n",
            "validation Loss: 2.815883081406355\n",
            "Training Loss: 2.5308493793010713\n",
            "validation Loss: 2.535458903759718\n",
            "Training Loss: 2.5317059993743896\n",
            "validation Loss: 2.6107047013938427\n",
            "Training Loss: 2.261249229311943\n",
            "validation Loss: 2.5388211123645306\n",
            "Training Loss: 2.5805826127529143\n",
            "validation Loss: 2.65936953574419\n",
            "Training Loss: 2.2666137337684633\n",
            "validation Loss: 2.80823528021574\n",
            "Training Loss: 2.3273940324783324\n",
            "validation Loss: 2.57906705327332\n",
            "Training Loss: 2.348085442185402\n",
            "validation Loss: 2.648147512227297\n",
            "Training Loss: 2.4131758213043213\n",
            "validation Loss: 2.7075752913951874\n",
            "Training Loss: 2.1873388051986695\n",
            "validation Loss: 2.637593511492014\n",
            "Training Loss: 2.2958237528800964\n",
            "validation Loss: 2.4472971288487315\n",
            "Training Loss: 2.3885608851909637\n",
            "validation Loss: 2.451068449765444\n",
            "Training Loss: 2.1098517686128617\n",
            "validation Loss: 2.6941309347748756\n",
            "Training Loss: 2.2827146649360657\n",
            "validation Loss: 2.6975795067846775\n",
            "Training Loss: 2.0878819942474367\n",
            "validation Loss: 2.6774371787905693\n",
            "Training Loss: 2.207903093099594\n",
            "validation Loss: 2.3611006271094084\n",
            "Training Loss: 2.200940081477165\n",
            "validation Loss: 2.5370994992554188\n",
            "Training Loss: 2.07817790210247\n",
            "validation Loss: 2.4726064559072256\n",
            "Training Loss: 1.9404492318630218\n",
            "validation Loss: 2.519424192607403\n",
            "Training Loss: 2.0186994820833206\n",
            "validation Loss: 2.3878795243799686\n",
            "Training Loss: 1.9419701546430588\n",
            "validation Loss: 2.294325416907668\n",
            "Training Loss: 2.316554236412048\n",
            "validation Loss: 2.3249827418476343\n",
            "Training Loss: 2.136707326769829\n",
            "validation Loss: 2.3284691385924816\n",
            "Training Loss: 2.152944341301918\n",
            "validation Loss: 2.302349254488945\n",
            "Training Loss: 2.178269240260124\n",
            "validation Loss: 2.6298702992498875\n",
            "Training Loss: 1.9652364134788514\n",
            "validation Loss: 2.3928286572918296\n",
            "Training Loss: 1.8307051181793212\n",
            "validation Loss: 2.3451892556622624\n",
            "Training Loss: 1.868239364027977\n",
            "validation Loss: 2.5492181163281202\n",
            "Training Loss: 1.8835677981376648\n",
            "validation Loss: 2.358388226479292\n",
            "Training Loss: 1.8983103424310683\n",
            "validation Loss: 2.503606602549553\n",
            "Training Loss: 1.9389779090881347\n",
            "validation Loss: 2.476383537054062\n",
            "Training Loss: 2.06934754550457\n",
            "validation Loss: 2.3076189570128918\n",
            "Training Loss: 1.6985980093479156\n",
            "validation Loss: 2.4688867926597595\n",
            "Training Loss: 1.6088701367378235\n",
            "validation Loss: 2.127218469977379\n",
            "Training Loss: 1.7362169161438943\n",
            "validation Loss: 2.2743913801386952\n",
            "Training Loss: 1.8502230927348138\n",
            "validation Loss: 2.3606600374914706\n",
            "Training Loss: 1.789269357919693\n",
            "validation Loss: 2.449647732079029\n",
            "Training Loss: 1.9239583157002926\n",
            "validation Loss: 2.318482725531794\n",
            "Training Loss: 2.0588860273361207\n",
            "validation Loss: 2.090596101246774\n",
            "Training Loss: 1.8915105789899826\n",
            "validation Loss: 2.341902058571577\n",
            "Training Loss: 1.787473875284195\n",
            "validation Loss: 2.152348268777132\n",
            "Training Loss: 1.6273447379469872\n",
            "validation Loss: 1.9834528621286154\n",
            "Training Loss: 1.5301614373922348\n",
            "validation Loss: 2.221974428743124\n",
            "Training Loss: 1.6813322693109511\n",
            "validation Loss: 2.152955605648458\n",
            "Training Loss: 1.755094975233078\n",
            "validation Loss: 1.9429264727514237\n",
            "Training Loss: 1.6874019548296928\n",
            "validation Loss: 2.104458581889048\n",
            "Training Loss: 1.4731921508908272\n",
            "validation Loss: 2.2089150506071746\n",
            "Training Loss: 1.7988520413637161\n",
            "validation Loss: 2.083863761741668\n",
            "Training Loss: 1.583529880642891\n",
            "validation Loss: 2.217359086498618\n",
            "Training Loss: 1.6643393218517304\n",
            "validation Loss: 2.211845541605726\n",
            "Training Loss: 1.5284410670399666\n",
            "validation Loss: 2.104870497714728\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NiXuTj0n46b"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "for i in tqdm_notebook(range(epochs)):\n",
        "  print('Training Loss:',train(model,data_store,loss_func,opt,20))\n",
        "  print('validation Loss:',evaluate(model,data_store,loss_func))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlTY0wfbqGa5"
      },
      "source": [
        "# torch.save(model.state_dict(),'Trained_model_2.pth')\n",
        "# torch.save(opt.state_dict(),'Trained_Optimizer_2')\n",
        "# # state_dict = torch.load('Trained_model.pth')\n",
        "# # net.load_state_dict(state_dict)\n",
        "# # accuracy_on_model(net,len(X_test_new),1,X_test_new,Y_test_new,X_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}